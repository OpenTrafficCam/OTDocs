{"config":{"lang":["de"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"live/","title":"Live","text":""},{"location":"live/#opentrafficcam-live-hoyerswerda","title":"OpenTrafficCam LIVE Hoyerswerda","text":"<p>OpenTrafficCam wird in zahlreichen Projekten in Forschung und Praxis erfolgreich eingesetzt. Dabei wird der Stra\u00dfenverkehr tempor\u00e4r erfasst und anschlie\u00dfend automatisiert analysiert.</p> <p>Im Rahmen einer mFund-F\u00f6rderung wurde die Basis f\u00fcr die Live-Prozessierung der erfassten Verkehrsvideos gelegt. Aufbauend auf diesen Arbeiten erfolgt nun eine Implementierung vor Ort in Hoyerswerda. OpenTrafficCam LIVE wird dabei um die spezifischen Anforderungen der TU Dresden und der Stadt Hoyerswerda erweitert. Diese umfassen sowohl technischen Anforderungen im Hintergrund als auch neue Auswertungen und Darstellungen der Verkehrsdaten.</p> <p>Im Ergebnis werden mit OpenTrafficCam LIVE der Stra\u00dfenverkehr in Hoyerswerda permanent erfasst und die Videostreams in Echtzeit verkehrlich analysiert. Dadurch wird es m\u00f6glich, Verkehrsdaten nicht nur retrospektiv auszuwerten, sondern unmittelbar f\u00fcr Verkehrsmanagement, \u00d6ffentlichkeitsinformation oder vernetzte Systeme nutzbar zu machen.</p> <p>Kontaktieren Sie uns</p>"},{"location":"live/#ziele","title":"Ziele","text":"<p>Mit OpenTrafficCam LIVE Hoyerswerda entwickeln wir ein Reallabor f\u00fcr den Stra\u00dfenverkehr. Damit sollen Verkehrsbewegungen automatisiert, permanent und in Echtzeit erfasst und analysiert werden. Dabei steht nicht nur die reine Z\u00e4hlung Verkehrsteilnehmender im Fokus \u2013 sondern ein tiefes Verst\u00e4ndnis f\u00fcr Verhalten, Nutzungsmuster und Sicherheitsaspekte im Stra\u00dfenverkehr.</p> <p>So soll OpenTrafficCam LIVE Hoyerswerda Forschung und Entwicklung in folgenden Bereichen erm\u00f6glichen:</p> <ul> <li> <p>Automatisierte Verkehrserfassung,</p> </li> <li> <p>Verhaltens- und Sicherheitsanalysen im Stra\u00dfenraum,</p> </li> <li> <p>smartes, situationsabh\u00e4ngiges Verkehrsmanagement sowie,</p> </li> <li> <p>automatisiertes und vernetztes Fahren.</p> </li> </ul> <p>Gleichzeitig soll das System wertvolle Informationen f\u00fcr die Verkehrsplanung und -lenkung in Hoyerswerda liefern, unter anderem in den Bereichen Verkehrsmodellierung und Verkehrssicherheit.</p> <p>Auch f\u00fcr andere St\u00e4dte und Verwaltungen wird OpenTrafficCam LIVE als Open Source-Alternative zu propriet\u00e4ren Anbietern verf\u00fcgbar sein.</p> <p>OpenTrafficCam LIVE soll als offenes System Transparenz schaffen, digitale Souver\u00e4nit\u00e4t st\u00e4rken und f\u00fcr Forschung und Praxis gleicherma\u00dfen nutzbar sein.</p>"},{"location":"live/#umsetzung","title":"Umsetzung","text":""},{"location":"live/#technische-umsetzung","title":"Technische Umsetzung","text":"<p>Der Stra\u00dfenverkehr wird mit OTCamera aufgezeichnet und die Videostreams werden per Mobilfunk (LTE/5G) an das Rechenzentrum der TU Dresden \u00fcbertragen. Dort erfolgen in Echtzeit mit OTVision KI-basierte Detektion und Tracking sowie mit OTAnalytics die Bestimmung von Verkehrskennwerten. Die ermittelten Informationen werden per Web-Dashboard visualisiert und stehen f\u00fcr Forschung, Entwicklung und lokale Verkehrsplanung zur Verf\u00fcgung.</p> <p>Perspektivisch sollen die Live-Informationen f\u00fcr Forschung und Entwicklung im Bereich intelligentes Verkehrsmanagement verwendet werden (zum Beispiel dynamische LSA-Steuerung).</p> <p>Das System im \u00dcberblick</p> <p></p>"},{"location":"live/#untersuchungsbereiche","title":"Untersuchungsbereiche","text":"<p>Das Reallabor umfasst zwei Untersuchungsbereiche in der Neustadt Hoyerswerdas mit 8 Messstellen und 19 Kamerasystemen.</p> <p>Untersuchungsbereiche und Messstellen</p> UntersuchungsbereicheMessstellen Neustadt OstMessstellen Neustadt West <p></p> <p></p> <p>Messstellen:</p> <ul> <li>\ud83d\udea6 Lichtsignalanlage Knotenpunkt Claus-vonStauffenberg-Stra\u00dfe / Maria-Grollmu\u00df-Stra\u00dfe / Erich-Weinert-Stra\u00dfe / K\u00e4the-Niederkirchner-Stra\u00dfe</li> <li>\ud83d\udee3\ufe0f Zufahrt zu dieser Lichtsignalanlage aus Richtung Osten (Maria-Grollmu\u00df-Stra\u00dfe)</li> <li>\ud83c\udd7f\ufe0f Ein-/Ausfahrt des Parkplatzes zum Einkaufscenter \"Treff 8\"</li> <li>\ud83d\ude8c Bushaltestelle \"Klinikum\"</li> <li>\ud83d\udeb6 Fu\u00dfg\u00e4nger-Lichtsignalanlage zwischen \"Lausitzer Seenland Klinikum\" und Einkaufscenter \"Treff 8\"</li> </ul> <p></p> <p>Messstellen:</p> <ul> <li>\ud83d\udea6 Lichtsignalanlage Knotenpunkt B97 Elsterstra\u00dfe / Albert-Einstein-Stra\u00dfe / Alte Berliner Stra\u00dfe</li> <li>\ud83d\ude8c Bushaltestelle \"Albert-Einstein-Stra\u00dfe\"</li> <li>\ud83d\udeb6 Verkehrsinsel \"Albert-Einstein-Stra\u00dfe\"</li> </ul>"},{"location":"live/#projektfortschritt","title":"Projektfortschritt","text":"04/2025: Projektstart <p>Im April 2025 begann das Projekt zur Weiterentwicklung von OpenTrafficCam zu einem Live-System.</p> 06/2025: Kickoff und Testmessung <p>Im Juni 2025 fanden das Kickoff mit der Stadt Hoyerswerda und eine tempor\u00e4re Testmessung im Untersuchungsbereich \"Hoyerswerda Neustadt Ost\" mit 21 Kamerasystemen statt.</p> <p>Impressionen</p> <ul> <li> <p> Anbringung OTCameras </p> </li> <li> <p> Montierte OTCameras </p> </li> <li> <p> Kickoff 1/2 (Quelle: Wochenkurier/Peter Aswendt) </p> </li> <li> <p> Kickoff 2/2 (Quelle: SZ/Juliane Mietzsch) </p> </li> </ul> <p>Presseberichte</p> <p>Radio Lausitz: \"Hoyerswerda wird zum Labor f\u00fcr Verkehrsforscher\"</p> <p>Wochenkurier: \"Smarter Verkehr in Hoyerswerda\"</p> <p>S\u00e4chsische Zeitung: \"Verkehrsforscher \u00fcberwachen die Klinikums-Kreuzung in Hoyerswerda\"</p> <p>Lausitzwelle: \"Kameras beobachten Verkehrslage\"</p> <p> </p> <p>09/2026: Go-Live</p>"},{"location":"live/#beteiligte","title":"Beteiligte","text":"<ul> <li> <p>F\u00f6rdergeber</p> <p>Bundesministerium f\u00fcr Wirtschaft und Energie</p> <p>Land Sachsen</p> <p></p> </li> <li> <p>F\u00f6rdernehmer und Auftraggeber</p> <p>TU Dresden</p> <p>Professur f\u00fcr Mobilit\u00e4tssystemplanung</p> <p></p> </li> <li> <p>Auftragnehmer</p> <p>platomo GmbH</p> <p></p> </li> <li> <p>Kommunaler Partner</p> <p>Stadt Hoyerswerda</p> <p></p> </li> </ul>"},{"location":"team/","title":"Team","text":""},{"location":"team/#entwicklung","title":"Entwicklung","text":"<p>Das Team rund um OpenTrafficCam besteht aus Softwareentwicklern und Ingenieur:innen aus den Bereichen Verkehr und Fotogrammetrie.</p> <p>Die Grundidee zu OpenTrafficCam entstand 2017 w\u00e4hrend der freiberuflichen T\u00e4tigkeiten der platomo Gr\u00fcnder. Der Markt der Verkehrserfassungsger\u00e4te war gepr\u00e4gt von eher geschlossenen und teuren L\u00f6sungen, die den Anforderungen der wissenschaftlichen und planerischen Projekte nicht gen\u00fcgte.</p> <p>Mit dem Raspberry Pi war schon l\u00e4nger eine offene und gut dokumentierte Hardwareplattform verf\u00fcgbar, auf der wir unsere ersten Kameraprototypen aufbauten.</p> <p>Gleichzeitig entwickelte sich die KI-basierte Objekterkennung \u00e4u\u00dferst dynamisch und es entstanden mehrere Open Source Modelle. Diese Modelle haben wir in unsere ersten Softwareprototypen integriert und die entstanden Detektionen mit Trackingalgorithmen kombiniert. F\u00fcr unsere damaligen Projekte haben wir eine Analysesoftware geschrieben, um die passenden Verkehrsdaten zu generieren, aufzubereiten und zu visualisieren.</p> <p>Mit den ersten Prototypen haben wir gemeinsam mit der TU Dresden im mFUND eine F\u00f6rderung beim Bundesministerium f\u00fcr Digitales und Verkehr beantragt und erhalten.</p> <p>Seit 2021 entwickeln wir im Rahmen einer gr\u00f6\u00dferen F\u00f6rderung (auch im mFUND) die OpenTrafficCam zu einem Open Source Verkehrserfassungssystem weiter, welches einerseits frei nutzbar sein soll und andererseits durch zus\u00e4tzliche Dienste das Gesch\u00e4ftsmodell der platomo GmbH bildet.</p> <p>Durch die Kombination von Praxispartner und Forschungseinrichtung k\u00f6nnen wir die Fragestellungen im Laufe der Entwicklung aus unterschiedlichen Sichtweisen beleuchten und l\u00f6sen.</p> <p>OpenTrafficCam wird von der platomo GmbH aus Karlsruhe und der TU Dresden entwickelt.</p> <ul> <li> <p></p> </li> <li> <p></p> </li> </ul>"},{"location":"team/#forderung","title":"F\u00f6rderung","text":"<p>Das Projekt OpenTrafficCam_live wird in der F\u00f6rderlinie mFUND mit insgesamt 973.140 Euro durch das Bundesministerium f\u00fcr Digitales und Verkehr gef\u00f6rdert.</p> <p>\u00dcber den mFUND des BMDV:</p> <p>Im Rahmen der Innovationsinitiative mFUND f\u00f6rdert das BMDV seit 2016 datenbasierte Forschungs- und Entwicklungsprojekte f\u00fcr die digitale und vernetzte Mobilit\u00e4t der Zukunft. Die Projektf\u00f6rderung wird erg\u00e4nzt durch eine aktive fachliche Vernetzung zwischen Akteuren aus Politik, Wirtschaft, Verwaltung und Forschung und durch die Bereitstellung von offenen Daten auf der Mobilithek. Weitere Informationen finden Sie unter www.mfund.de</p> <ul> <li> <p></p> </li> <li> <p></p> </li> </ul> <p>Zudem werden Teile der Weiterentwicklung von OpenTrafficCam von der Deutschen Forschungsgemeinschaft (DFG) gef\u00f6rdert. Konkret erfolgt im Rahmen des Projekts NFDI4ing die Entwicklung von OTGroundTruther,  einem Tool zur manuellen Erzeugung von Referenzdatens\u00e4tzen. Mit diesen Referenzdatens\u00e4tzen k\u00f6nnen die mit OpenTrafficCam automatisiert abgeleiteten Verkehrskennwerte validiert werden.</p> <ul> <li> <p></p> </li> <li> <p></p> </li> </ul>"},{"location":"OTAnalytics/","title":"Features","text":"<p>The brain of OpenTrafficCam.</p> <p>OTAnalytics is a set of self-developed algorithms to gather traffic measures based on trajectories of road users. It provides a user-friendly graphical interface for configuring analyses, extracting valuable insights from your data, and exporting the results efficiently.</p>"},{"location":"OTAnalytics/#key-features","title":"Key features","text":"<ul> <li>Configuration of traffic analyses using sections and flows</li> <li>Generation of traffic counts and flow matrices</li> <li>Export of event lists for detailed analysis</li> <li>Visualization of tracks, events, and traffic data</li> <li>Various visualization layers and filters</li> <li>Runs on Windows, macOS, and Linux</li> </ul>"},{"location":"OTAnalytics/#content-of-documentation","title":"Content of documentation","text":"<ul> <li>Requirements: Prerequisites for running OTAnalytics</li> <li>Installation: How to install OTAnalytics on different operating systems</li> <li>First Use: Quick walkthrough of your first analysis</li> <li>Usage GUI: Detailed guide on how to use the graphical user interface</li> <li>Usage CLI: Command-line interface reference</li> </ul> <p>Most of the code is developed by the OpenTrafficCam contributors (based on Python libraries like NumPy, pandas, tkinter and PyAV).</p> <p>Current status</p> <p>OTAnalytics is actively maintained and developed. The latest version offers a comprehensive set of features for traffic analysis.</p> <p>It is tested on Windows, macOS, and Linux.</p> <p>For the latest updates and releases, visit the GitHub repository.</p>"},{"location":"OTAnalytics/firstuse/","title":"First Use","text":"<p>This page guides you through your first steps with OTAnalytics using the Graphical User Interface (GUI). If you haven't installed OTAnalytics yet, start with the Requirements and Installation pages.</p>"},{"location":"OTAnalytics/firstuse/#run-otanalytics","title":"Run OTAnalytics","text":"<p>Run the start script as follows:</p> WindowsmacOS / Linux <pre><code>.\\start_gui.cmd\n</code></pre> <pre><code>sh ./start_gui.sh\n</code></pre>"},{"location":"OTAnalytics/firstuse/#prepare-your-data","title":"Prepare your data","text":"<p>Make sure you have the required input data ready: <code>.ottrk</code> files from OTVision and, optionally, corresponding video files for visual validation.</p>"},{"location":"OTAnalytics/firstuse/#workflow-overview","title":"Workflow overview","text":""},{"location":"OTAnalytics/firstuse/#1-set-up-the-project","title":"1. Set up the project","text":"<p>Enter a project name and start date, then load your track files. See Setup the project for detailed instructions.</p>"},{"location":"OTAnalytics/firstuse/#2-define-sections-and-flows","title":"2. Define sections and flows","text":"<p>Draw sections on the video image and create flows to define traffic relationships. See Configure traffic analysis for step-by-step guidance.</p>"},{"location":"OTAnalytics/firstuse/#3-validate-with-visualization","title":"3. Validate with visualization","text":"<p>Use the visualization layers and filters to check your configuration before exporting.</p>"},{"location":"OTAnalytics/firstuse/#4-export-results","title":"4. Export results","text":"<p>Export traffic counts, event lists, or road user assignments. See Analysis exports for all export options.</p> <p>For detailed explanations of all features, continue with Usage (GUI) and Usage (CLI).</p>"},{"location":"OTAnalytics/installation/","title":"Installation","text":"<p>In this section, we provide instructions how to install OTAnalytics on the most common Operating Systems.</p> <p>Before installing OTAnalytics, make sure your system meets all requirements.</p>"},{"location":"OTAnalytics/installation/#install-python-312","title":"Install Python 3.12","text":"<p>If not done yet, visit www.python.org/downloads/ and install Python 3.12.x for your platform.</p> <p>Please make sure to include at least the following boxes:</p> WindowsmacOS / Linux <ul> <li>Add Python to PATH</li> <li>pip</li> <li>tcl/tk</li> <li>Add python to environment variables</li> <li>Precompile standard library</li> </ul> <ul> <li>GUI applications</li> <li>UNIX command-line tools</li> <li>Shell profile updater</li> <li>Install or upgrade pip</li> </ul>"},{"location":"OTAnalytics/installation/#install-otanalytics","title":"Install OTAnalytics","text":"<p>Visit OTAnalytics latest releases on GitHub, scroll down to the list of assets and download the correct \"otanalytics\" <code>.zip</code> file for your platform.</p> <p>Unzip the file to a location of your choice (preferably local).</p> <p>Open a terminal window (or command prompt), navigate to the unzipped folder and run the install script as follows:</p> WindowsmacOS / Linux <pre><code>.\\install.cmd\n</code></pre> <pre><code>sh ./install.sh\n</code></pre> <p>Wait until the installation of dependencies is complete - this could take some time.</p>"},{"location":"OTAnalytics/installation/#what-else","title":"What else?","text":"<p>If you encounter problems, have questions or like to request features, please open an issue on GitHub.</p> <p>We welcome code contributions (e.g. fixing bugs or adding features) by the community. If you consider contributing, please check the contribute section of this documentation first.</p> <p>If you like to receive professional support, please have a look at our services or contact us for more information.</p>"},{"location":"OTAnalytics/otconfig/","title":"OTConfig","text":"<p>OTConfig is a program for configuring video-based traffic analysis (e.g. traffic counts, speed measurements) with the OpenTrafficCam pipeline (OTVision and OTAnalytics). It has a subset of the functionality OTAnalytics has. The configuration consists of three steps:</p> <ol> <li>Project setup</li> <li>Definition of the traffic flows to be analyzed</li> <li>Export of the configuration file</li> </ol> <p>The results of the OpenTrafficCam pipeline are raw data on the individual traffic movements as well as count values for the individual traffic flows. Further evaluations, such as speed or time requirement analyses, are carried out in downstream processes upon customer request. The relevant information is transmitted individually and any desired aggregations of vehicle classes are agreed individually and subsequently implemented by platomo.</p> <p>Traffic flows are defined by using detectors (sections). Each flow consists of exactly one start and one end section (from-to relationship). Sections can be defined in the form of lines with any number of support points as well as areas.</p> <p>Info</p> <p>A separate configuration must be created for each camera view. Due to the different positions of the sections and flows, videos with different views cannot be configured at the same time.</p>"},{"location":"OTAnalytics/otconfig/#why-otconfig","title":"Why OTConfig?","text":"<p>If the configuration and execution of the OpenTrafficCam pipeline (processing or analysis) are not to be carried out in the same step or in the same institution, it must be ensured that the configured pipeline is also executed later as desired. For this purpose, we have developed the OTConfig tool for our customers to make the external processing of videos through our infrastructure as efficient and smooth as possible.</p> <p>With OTConfig, the customer can specify all the information required for execution and preconfigure the OpenTrafficCam pipeline accordingly. This information is then saved in an otconfig file. This ensures that all relevant information is transferred to us in a structured manner and applied correctly in the OpenTrafficCam pipeline.</p> <p>Info</p> <p>The transmitted information is validated again by trained personnel before each execution of the pipeline and, if necessary, checked for plausibility. In particular, the position of the sections is optimized to ensure the best possible result.</p>"},{"location":"OTAnalytics/requirements/","title":"Requirements","text":"<p>OTAnalytics is designed to run on most modern operating systems (Windows, macOS, Linux). Before installing OTAnalytics, please make sure your system meets the following prerequisites.</p>"},{"location":"OTAnalytics/requirements/#system-requirements","title":"System requirements","text":"<ul> <li>CPU: Modern 64-bit processor</li> <li>RAM: 8 GB or more recommended (more RAM allows more tracks to be loaded simultaneously)</li> <li>Storage: At least 2 GB free disk space for installation and temporary files</li> <li>Display: At least Full-HD recommended for comfortable UI usage</li> </ul>"},{"location":"OTAnalytics/requirements/#operating-system","title":"Operating system","text":"<ul> <li>Windows 10/11 (64-bit)</li> <li>macOS (Apple Silicon and Intel are supported)</li> <li>Linux (recent distributions; tested with Ubuntu/Debian-based systems)</li> </ul>"},{"location":"OTAnalytics/requirements/#software","title":"Software","text":"<ul> <li>Python 3.12.x</li> <li>Download from python.org/downloads</li> <li>Make sure to include pip and Tk/GUI components during installation</li> <li>For step-by-step installation instructions, see the Installation page</li> </ul>"},{"location":"OTAnalytics/requirements/#input-data","title":"Input data","text":"<p>OTAnalytics analyzes trajectories (tracks) exported by OTVision:</p> <ul> <li>Track files: .ottrk files created by OTVision Track</li> <li>Optional: Video files corresponding to the track files (for visual validation)</li> </ul> <p>If you plan to use OTAnalytics without videos, you can still configure and run analyses based on .ottrk files.</p>"},{"location":"OTAnalytics/usage/usage-cli/","title":"Usage CLI","text":"<p>After processing videos with OTVision\u00b4s tracking module (OTVision track), you can use OTAnalytics for analysing traffic data. The analysis must be configured in the OTAnalytics Graphical User Interface (GUI). Afterwards, the analysis can be processed using the GUI or CLI interface.</p>"},{"location":"OTAnalytics/usage/usage-cli/#synopsis","title":"Synopsis","text":"<pre><code>python -m OTAnalytics [-h]\n                [--cli]\n                [--cli-mode {bulk,stream}]\n                [--show-svz]\n                [--config CONFIG]\n                [--ottrks OTTRKS [OTTRKS ...]]\n                [--otflow OTFLOW]\n                [--save-dir SAVE_DIR]\n                [--save-name SAVE_NAME]\n                [--save-suffix SAVE_SUFFIX]\n                [--debug]\n                [--event-formats EVENT_FORMATS [EVENT_FORMATS ...]]\n                [--count-intervals COUNT_INTERVALS [COUNT_INTERVALS ...]]\n                [--track-export]\n                [--track-statistics-export]\n                [--num-processes NUM_PROCESSES]\n                [--logfile LOGFILE]\n                [--logfile_overwrite]\n                [--include-classes INCLUDE_CLASSES [INCLUDE_CLASSES ...]]\n                [--exclude-classes EXCLUDE_CLASSES [EXCLUDE_CLASSES ...]]\n</code></pre>"},{"location":"OTAnalytics/usage/usage-cli/#description","title":"Description","text":"<p>If you have run OTVision track to extract movements of road users, you are ready to analyze the movements in each file. To do so, you first need to configure the analysis within the OTAnalytics GUI. Afterwards, you can analyze the data using the GUI or CLI. The CLI allows you to process a configured analysis on a high performance workstation.</p>"},{"location":"OTAnalytics/usage/usage-cli/#parameters","title":"Parameters","text":""},{"location":"OTAnalytics/usage/usage-cli/#help","title":"help","text":"<p><code>-h</code>, <code>--help</code></p> <p>Show the help message and exit.</p>"},{"location":"OTAnalytics/usage/usage-cli/#cli","title":"cli","text":"<p><code>--cli</code></p> <p>Start OTAnalytics in CLI mode. If omitted, OTAnalytics will run in GUI mode.</p>"},{"location":"OTAnalytics/usage/usage-cli/#cli-mode","title":"cli-mode","text":"<p><code>--cli-mode {bulk,stream}</code></p> <p>Specify the execution mode for the CLI. Possible values are:</p> <ul> <li><code>bulk</code>: Read all tracks into memory and process all in one.</li> <li><code>stream</code>: Read data in chunks. Thus, reading <code>cli-chunk-size</code> tracks into memory and process them. Afterwards, read the next ones</li> </ul> <p>This parameter is optional. Defaults to <code>bulk</code>.</p>"},{"location":"OTAnalytics/usage/usage-cli/#show-svz","title":"show-svz","text":"<p><code>--show-svz</code></p> <p>Show SVZ-Tab in OTAnalytics GUI. If omitted the tab will be hidden.</p>"},{"location":"OTAnalytics/usage/usage-cli/#config","title":"config","text":"<p><code>--config &lt;path/to/config file&gt;</code></p> <p>Path to otconfig file. The otconfig file contains the configuration of the analysis. How to create an otconfig file, see Usage GUI</p>"},{"location":"OTAnalytics/usage/usage-cli/#ottrks","title":"ottrks","text":"<p><code>--ottrks &lt;paths to .ottrk files&gt;</code></p> <p>Specify one or more paths directories or to <code>.ottrk</code> files containing track data to be processed.</p>"},{"location":"OTAnalytics/usage/usage-cli/#otflow","title":"otflow","text":"<p><code>--otflow &lt;path/to/file&gt;</code></p> <p>Path to an <code>.otflow</code> file containing section information needed for analysis. How to create an otflow file, see Usage GUI</p>"},{"location":"OTAnalytics/usage/usage-cli/#save-dir","title":"save-dir","text":"<p><code>--save-dir &lt;path/to/directory&gt;</code></p> <p>Specify the directory where output files will be saved.</p> <p>This parameter is optional. It defaults to the directory where the otconfig or otflow file is stored.</p>"},{"location":"OTAnalytics/usage/usage-cli/#save-name","title":"save-name","text":"<p><code>--save-name &lt;filename&gt;</code></p> <p>Specify the base name of the output file (e.g., event data). A proper suffix or file type will be appended automatically.</p> <p>This parameter is optional. Default is an empty string.</p>"},{"location":"OTAnalytics/usage/usage-cli/#save-suffix","title":"save-suffix","text":"<p><code>--save-suffix &lt;suffix&gt;</code></p> <p>Append a custom suffix to the filenames of all output files. For example, if the <code>save-name</code> is <code>results</code> and the suffix is <code>_test</code>, the resulting file may be <code>results_test.csv</code>.</p> <p>This is optional. Default is an empty string.</p>"},{"location":"OTAnalytics/usage/usage-cli/#event-formats","title":"event-formats","text":"<p><code>--event-formats &lt;formats&gt;</code></p> <p>Specify one or multiple output formats for the event list. Supported formats:</p> <ul> <li><code>otevents</code> (default)</li> <li><code>csv</code></li> <li><code>xlsx</code></li> </ul> <p>This parameter is optional and defaults to <code>otevents</code>.</p>"},{"location":"OTAnalytics/usage/usage-cli/#count-intervals","title":"count-intervals","text":"<p><code>--count-intervals &lt;intervals in minutes&gt;</code></p> <p>Specify the time interval(s) in minutes for event counting.</p> <p>Example: <code>--count-intervals 5 10 15</code> will export event counts for 5-minute, 10-minute, and 15-minute intervals.</p>"},{"location":"OTAnalytics/usage/usage-cli/#track-export","title":"track-export","text":"<p><code>--track-export</code></p> <p>Enable the export of track data as <code>.csv</code>.</p>"},{"location":"OTAnalytics/usage/usage-cli/#track-statistics-export","title":"track-statistics-export","text":"<p><code>--track-statistics-export</code></p> <p>Enable the export of track statistics as <code>.csv</code>.</p>"},{"location":"OTAnalytics/usage/usage-cli/#num-processes","title":"num-processes","text":"<p><code>--num-processes &lt;integer&gt;</code></p> <p>Specify the number of processes to be used for multiprocessing. The CLI will divide the workload among these many processes to improve performance.</p>"},{"location":"OTAnalytics/usage/usage-cli/#include-classes","title":"include-classes","text":"<p><code>--include-classes &lt;list of classes&gt;</code></p> <p>Whitelist filter to include tracks with given classes. Classes specified in <code>--include-classes</code> are always kept even if they appear in <code>--exclude-classes</code>.</p>"},{"location":"OTAnalytics/usage/usage-cli/#exclude-classes","title":"exclude-classes","text":"<p><code>--exclude-classes &lt;list of classes&gt;</code></p> <p>Blacklist filter to exclude tracks with given classes.</p>"},{"location":"OTAnalytics/usage/usage-cli/#debug","title":"debug","text":"<p><code>--debug</code></p> <p>Set log level to <code>DEBUG</code> mode for detailed information about the process.</p>"},{"location":"OTAnalytics/usage/usage-cli/#logfile","title":"logfile","text":"<p><code>--logfile &lt;path/to/file&gt;</code></p> <p>Specify the directory or file location where logs should be saved.</p>"},{"location":"OTAnalytics/usage/usage-cli/#logfile_overwrite","title":"logfile_overwrite","text":"<p><code>--logfile_overwrite</code></p> <p>Overwrite the log file if it already exists. If omitted, logs will be appended to the existing file.</p>"},{"location":"OTAnalytics/usage/usage-cli/#notes","title":"Notes","text":"<ul> <li>Parameters specified via the CLI will override configuration file parameters</li> <li>Ensure all file paths are provided in the correct format and are accessible</li> </ul>"},{"location":"OTAnalytics/usage/usage-ui/","title":"Usage GUI","text":"<p>After processing videos with OTVision\u00b4s tracking module (OTVision track), you can use OTAnalytics for analyzing traffic data. The OTAnalytics Graphical User Interface (GUI) enables you to configure analyses, extract valuable insights from your data, and export the results efficiently.</p>"},{"location":"OTAnalytics/usage/usage-ui/#terminology","title":"Terminology","text":"<p>Vehicles and pedestrians are detected in the individual frames of the video using detect in OTVision. In a single frame of a video, each detected object (Detection) is represented by a Bounding Box, which is the rectangular area surrounding the detected vehicle or pedestrian together with its classification (e.g., car, bike, pedestrian).</p> <p>The tracker in OTVision links consecutive detections, or bounding boxes, to form the Track of a Road User.</p> <p>A track is therefore composed of a series of detections, each with its own bounding box and classification. Ideally, all detections within a track share the same classification. However, in practice, a track may include detections with varying classifications (e.g., truck and truck_with_trailer). This typically occurs when a road user is poorly visible in some of the frames (far away, partially obscured). OTAnalytics assigns a single Track Classification to each track, as a road user can have only one classification in the real world (e.g., a vehicle is either a car or a bus).</p> <p>A track's trajectory is represented by a representative point from each bounding box associated with the track. This point is called the Track Point. The trajectory is formed by connecting the track points collected across successive frames.</p> <p>Offset examples</p> 0.1, 0.10.8, 0.3 <p></p> <p></p> <p>The position of the track point within the bounding box can be configured using the * *Offset** attribute. This offset is defined separately for the x- and y-axes, with values ranging from <code>0</code> to <code>1</code>. Origin <code>(0, 0)</code> is the top left-hand corner of the bounding box. These values determine the relative position of the track point within the bounding box, providing flexibility in track point placement.</p> <p>To analyze the tracks, OTAnalytics provides Sections and Flows. Imagine sections as the digital equivalent of physical detectors used for traffic measurement, sometimes, also called virtual inductive loops (not to be confused with the detection of bounding boxes in OTVision). Each time a track's trajectory intersects a section, an Event is generated. Each event contains detailed information, including the track's identifier, its position in the frame, and the timestamp of the intersection.</p> <p>Flows define traffic flows to be analyzed. As explained in Flows, a flow is defined by two sections: a starting section and an ending section. Tracks that intersect both sections of a flow in the defined order can be assigned to that flow. To achieve this, the events belonging to a track are sorted chronologically based on their time of occurrence. The assignment of a track to a flow is called Track Assignment.</p> Assignment of trajectories to flows <p>A trajectory can be assigned to at most one flow. To be assigned to a flow, a trajectory must cross the start section and the end section of the flow in the correct order (one Section-Enter event for the start section and one for the end section).</p>"},{"location":"OTAnalytics/usage/usage-ui/#cases","title":"Cases","text":"<ol> <li>A trajectory crosses exactly one start section and one end section (2 Section-Enter events)<ul> <li>The trajectory is assigned to this flow</li> </ul> </li> <li>A trajectory crosses more than two start and end sections (3 or more Section-Enter events)<ul> <li>The trajectory is assigned to the flow whose sections were traversed for the longest duration (the Section-Enter events are furthest apart in time)</li> </ul> </li> </ol>"},{"location":"OTAnalytics/usage/usage-ui/#example","title":"Example","text":"<p>Sections: S1, S2, E1, E2</p> <p>Flows:</p> <ul> <li>Flow A: Start = S1, End = E1</li> <li>Flow B: Start = S2, End = E2</li> </ul>"},{"location":"OTAnalytics/usage/usage-ui/#case-1-exactly-one-start-and-one-end-intersection","title":"Case 1: Exactly one start and one end intersection","text":"<p>Trajectory T100 generates the following events:</p> Time Event 10:00:05 Enter S1 (Start of Flow A) 10:02:20 Enter E1 (End of Flow A) <p>T100 crossed S1 \u2192 E1 in the correct order. There are exactly two relevant Section-Enter events (start + end). T100 is assigned to Flow A.</p>"},{"location":"OTAnalytics/usage/usage-ui/#case-2-more-than-two-relevant-startend-events","title":"Case 2: More than two relevant start/end events","text":"Time Event 11:00:10 Enter S1 (Start of Flow A) 11:00:30 Enter S2 (Start of Flow B) 11:01:00 Enter E1 (End of Flow A) 11:06:00 Enter E2 (End of Flow B) <p>Candidate flows (start \u2192 end in correct order)</p> <ul> <li>Flow A: S1 (11:00:10) \u2192 E1 (11:01:00) \u21d2 Duration = 50 s</li> <li>Flow B: S2 (11:00:30) \u2192 E2 (11:06:00) \u21d2 Duration = 5 min 30 s</li> </ul> <p>Assign the trajectory to the flow whose start and end Section-Enter events are furthest apart in time. T200 is assigned to Flow B, because 5:30 &gt; 0:50.</p>"},{"location":"OTAnalytics/usage/usage-ui/#user-interface","title":"User Interface","text":"<p>The user interface is divided into four main areas: the Configuration Bar, Workspace, Visualization Layers, and Visualization Filters.</p> <p>The Configuration Bar consists of four parts:</p> <ol> <li>Project: Manage project metadata and save or load <code>.otconfig</code> or <code>.otflow</code> files.</li> <li>Tracks/Video: Add input files, where only track files can be added, while video    files can be both added and removed. An overview of loaded video files is also    displayed.</li> <li>Sections/Flows: Create, edit, and delete sections and flows. The flow names are    shown with numbers indicating the number of assigned tracks per flow.</li> <li>Analysis: Export of analyzed traffic data.</li> </ol> <p>The Workspace consists of the Canvas and the Track Statistics. The canvas displays the tracks and video frames. It is also where the geometry of sections can be created or edited. Below the workspace, the Track Statistics provides key statistics about the displayed tracks.</p> <p>By choosing the Visualization Layers, that are located right to the workspace, one can customize how tracks are displayed.</p> <p>With the Visualization Filters located below the workspace, one can adjust which tracks are displayed.</p> <p></p>"},{"location":"OTAnalytics/usage/usage-ui/#setup-the-project","title":"Setup the project","text":"<p>To make the handling and reproducibility of analyses easier, all related data is organized within a project. Setting up a project ensures that all necessary information, such as videos, tracks, and configurations, is stored in one centralized place. This simplifies workflows and maintains consistency across analyses.</p>"},{"location":"OTAnalytics/usage/usage-ui/#enter-project-information","title":"Enter project information","text":"<p>The project must first be named. The project name is entered in the corresponding Name field.</p> <p>Info</p> <p>It is recommended to choose a unique name (e.g. name of the measuring point, camera number; or a combination of several unique details).</p> <p>The start time (date and time) of the first video must then be entered in the corresponding Start date fields. It is possible to enter in ISO 8601 format (YYYY-MM-DD) or German date format ( DD.MM.YYYY).</p> <p>Info</p> <p>OTAnalytics assumes that all selected videos are contiguous in time. If the videos to be processed have a time gap (e.g., videos from three days from 6:00 a.m. to 10:00 p.m.), several projects with chronologically contiguous videos must be created.</p>"},{"location":"OTAnalytics/usage/usage-ui/#load-track-files","title":"Load track files","text":"<p>To analyze tracks in OTAnalytics, you must load the track files (.ottrk). This can be done using the Add tracks... button, which opens a file browser to select the respective files. Multiple track files can be added at once. If a corresponding video file exists, OTAnalytics will automatically load it alongside the track file. If no associated video is found, the track file cannot be loaded.</p> <p>Alternatively, you can add videos only. This is particularly useful for configuring an analysis handled by the platomo team. To add videos, use the Add... button, and to remove them from the configuration, use the Remove button.</p> <p>Tip</p> <p>Added videos are displayed in alphabetical order in the \u201cVideos\u201d overview panel and are later processed in this order. The videos should therefore be named in such a way that the chronological order matches the alphabetical order. Additional zero-padding (e.g., <code>01</code>, <code>02</code>) is helpful if you want to ensure correct alphabetical sorting.</p> <p>When you click a video in the configuration bar, the first image of the video is displayed as the background image. This allows you to check whether all videos have been selected correctly.</p>"},{"location":"OTAnalytics/usage/usage-ui/#save-and-open-the-project","title":"Save and open the project","text":"<p>Once you added all required tracks and videos and created all sections and flows, the project can be saved.</p> <ol> <li>Click on the Save as... button in the Project section of the configuration bar.</li> <li>Select a suitable filename in the file browser and save it in the same folder as the videos.</li> </ol> <p>You can save the project in two ways:</p> <p>An <code>.otconfig</code> file stores all information necessary to reload the project in OTAnalytics. This is the default option.</p> <p>An <code>.otflow</code> file stores only information about sections and flows. Hence, it can be reused / imported into another project with different videos of the same measurement site.</p> <p>Tip</p> <p>We recommend regularly saving the progress of the project while it is still being processed. This prevents possible loss of data. The Save button automatically saves the file to the last selected location. If it is colored in orange, changes have been made.</p> <p>We also recomment to store all project related files (videos, <code>.otdet</code>, <code>.ottrk</code> and <code>.otconfig</code>) in the same folder.</p> <p>When you return later, you can reopen the project using the Open ... button.</p>"},{"location":"OTAnalytics/usage/usage-ui/#configure-traffic-analysis","title":"Configure traffic analysis","text":"<p>To perform traffic analyses with OTAnalytics, sections and flows are needed. A traffic flow depicts the directional travel relationship between two sections. Each flow always consists of two sections: A start section and an end section. Before defining flows, you must create the sections. The flows can then be assigned to the sections.</p>"},{"location":"OTAnalytics/usage/usage-ui/#sections","title":"Sections","text":"<p>Sections can consist of two or more support points (shown as a circle). They are drawn directly on the background image in OTAnalytics. To do this, the Sections tab must first be selected in the Sections/Flows section.</p> <p>Tip</p> <p>Both line sections and area sections can be created. If no occupancy durations (e.g., of parking areas) are analyzed, we recommend using line sections.</p> <p>The following example is limited to line sections. However, the procedure described can also be applied to area sections. The only difference is that the polygon is automatically closed when the add mode is exited.</p>"},{"location":"OTAnalytics/usage/usage-ui/#add-sections","title":"Add sections","text":"<p>A new line section is added in the following steps:</p> <ol> <li>Left-click on the Add line button, which starts the add mode.</li> <li>Set the first point by left-clicking at the correct position in the video image. The point is now fixed. Further points can be added to the section by moving the mouse and left-clicking again.    A section must consist of at least two points.</li> <li>Once the desired length and shape of a section has been reached, right-click or press the Enter key to exit Add mode.    (Pressing the Esc key cancels add mode without saving the previously created section.)</li> <li>A pop-up window opens. Enter the name for the section in this window and confirm. The name is reused in the analysis.</li> <li>The created section appears with the assigned name in the sections part in the configuration bar.</li> <li>Repeat the process to add further sections.</li> </ol> <p>Warning</p> <p>A name can only be assigned once, duplicate names of several sections are not possible. We recommend including the approximate compass direction of the geographical location (e.g. north, north-east) in the name of the section.</p> <p></p>"},{"location":"OTAnalytics/usage/usage-ui/#select-sections","title":"Select sections","text":"<p>To select a specific section, click on its name in the list of sections. The selected section is now highlighted both in the list and on the canvas.</p> <p>You can also select multiple sections with Ctrl+Left Button on Windows or Cmd+Left Button on macOS. Deselect a single with Ctrl+Right Button on Windows or Cmd+Right Button on macOS or deselect all sections with Right Button into a empty place in the list.</p>"},{"location":"OTAnalytics/usage/usage-ui/#change-the-geometry-of-a-section","title":"Change the geometry of a section","text":"<p>The geometry of sections that have already been created can be changed using the following procedure:</p> <ol> <li>First select the section to be changed in the list.</li> <li>Then start the change mode by left-clicking on the Edit button. The supporting points are now visible.</li> <li>Support points can now be selected by left-clicking and reset as in add mode.</li> <li>The + button can be used to add another support point.</li> </ol> <p>The selected point \u201csticks\u201d to the mouse pointer until the left mouse button is pressed. The old geometry is displayed as a dashed reference.</p> <ol> <li>Right-click to exit change mode and save the changes.</li> <li>Pressing Esc exits the change mode without saving the changes to the    geometry.</li> </ol>"},{"location":"OTAnalytics/usage/usage-ui/#change-attributes-of-a-section","title":"Change attributes of a section","text":"<p>The attributes (name and offset) of a section can be changed by clicking on the Properties button. To do this, the desired section must first be selected from the list.</p> <p>The bounding box offset defines, which reference point of the bounding boxes of the track is used to trigger an event.</p> <p>Tip</p> <p>Since the bounding box offset can be individually configured for each section, tracks intersecting different sections can be finely tuned. This helps to increase the recording accuracy of the section or to fulfil special use cases (e.g. if the front of a road user is to be recorded).</p>"},{"location":"OTAnalytics/usage/usage-ui/#remove-sections","title":"Remove sections","text":"<p>A section can be removed by clicking on the Remove button. To do this, the desired section must be removed from any flows and it must be selected in the list. You can use multiselect to remove multiple sections at once.</p>"},{"location":"OTAnalytics/usage/usage-ui/#flows","title":"Flows","text":"<p>A flow always consists of exactly two sections: a start section and an end section. To edit flows, the Flows tab must first be selected in the Sections/Flows section.</p>"},{"location":"OTAnalytics/usage/usage-ui/#add-flows","title":"Add flows","text":"<p>A new flow is added in the following steps:</p> <ol> <li>Left-click on the Add button. A pop-up window opens. The appropriate sections can be selected from the drop-down menu.</li> <li>The start point of the flow is defined as the first section.</li> <li>The endpoint of the flow is defined as the second section.</li> <li>The name is automatically set in the Name field after selecting the two sections. This can be changed as required. The name is reused in the analysis.</li> <li>If you plan to perform speed measurements, you can enter the distance between the sections (which must be measured independently of OTAnalytics)</li> </ol> <p>Warning</p> <p>A name can only be assigned once, duplicate names of several flows are not possible.</p> <p></p> <p>Tip</p> <p>Entering flows for many sections can be tedious. Fortunately, OTAnalytics can automatically generate flows for all possible combinations of start and end sections. Just hit the Generate button. The naming convention for the flows is <code>{Name of start section} --&gt; {Name of end section}</code>.</p>"},{"location":"OTAnalytics/usage/usage-ui/#select-flows","title":"Select flows","text":"<p>To select a specific flow, click on its name in the list of flows. The selected flow is now highlighted both in the list and on the canvas (by an arrow indicating its direction).</p> <p>You can also select multiple flows with Ctrl+Left Button on Windows or Cmd+Left Button on macOS. Deselect a single with Ctrl+Right Button on Windows or Cmd+Right Button on macOS or deselect all sections with Right Button into a empty place in the list.</p>"},{"location":"OTAnalytics/usage/usage-ui/#edit-a-flow","title":"Edit a flow","text":"<p>To edit a flow, it must first be selected in the list. Then left-click on the Properties button to open the same pop-up window as when creating a flow. The desired changes can now be made.</p>"},{"location":"OTAnalytics/usage/usage-ui/#remove-flows","title":"Remove flows","text":"<p>To remove a flow, it must first be selected in the list. Left-click on the Remove button to remove the selected flow. You can use multiselect to remove multiple flows at once.</p>"},{"location":"OTAnalytics/usage/usage-ui/#visualization-layers","title":"Visualization layers","text":"<p>Utilizing the different visualization layers helps you to explore the tracks and to refine traffic analysis by offering specific views of the data. Tracks are consistently displayed in the same colors, which are determined by their classification. The visualization layers are organized into groups.</p> <p>Warning</p> <p>Some visualization layers are not updated automatically, when you change the configuration of the project or the sections/flows.</p> <p>If the visualization is outdated, just hit the Update flow highlighting button - calculations will be run and visulalization will be updated.</p>"},{"location":"OTAnalytics/usage/usage-ui/#background","title":"Background","text":"<p>This layer shows a frame of the video as a background image. The currently shown frame can be configured using the filter. If a date/time filter is set, the frame at the end of the range is shown. Otherwise, the first frame of the selected video is shown.</p>"},{"location":"OTAnalytics/usage/usage-ui/#show-tracks","title":"Show tracks","text":"<p>This group of layers offers options to control which tracks are displayed based on their intersection with sections or their assignment to flows.</p> <p>They enable you to check the position of the sections and the definition of the flows and to optimise them iteratively. Checking, if the sections \"catch\" the correct tracks is a crucial step to ensure a good quality of each traffic analysis.</p> AllIntersecting sections <p>All tracks are shown.</p> <p></p> <p>Tracks intersecting at least one of the selected sections are shown.</p> <pre><code>![Show tracks intersecting the selected sections](usage-ui/show-tracks-intersecting-sections.png)\n</code></pre> Not intersecting sections <p>Tracks intersecting none of the selected sections are shown.</p> <pre><code>![Show tracks not intersection the selected sections](usage-ui/show-tracks-not-intersecting-sections.png)\n</code></pre> Assigned to flows <p>Tracks assigned to at least one selected flow are shown.</p> <pre><code>![Show tracks assigned to flows](usage-ui/show-tracks-assigned-to-flows.png)\n</code></pre> Not assigned to flows <p>Tracks assigned to none of the selected flows are shown.</p> <pre><code>![Show tracks not assigned to flows](usage-ui/show-tracks-not-assigned-to-flows.png)\n</code></pre> <p>Tip</p> <p>When checking and optimising sections and flows, also have a look at the track statistics shown below the canvas.</p>"},{"location":"OTAnalytics/usage/usage-ui/#show-start-and-endpoints","title":"Show start- and endpoints","text":"<p>In this group of layers, you can decide if the start- and endpoints of tracks are highlighted. Like in Show tracks, you can display the start- and endpoints based on their intersection with sections or their assignment to flows.</p> <p>They enable you, for example, to check whether some tracks are interrupted and where this may be clustered.</p> AllIntersecting sectionsNot intersecting sectionsAssigned to flowsNot assigned to flows <p>Start- and endpoints of all tracks are highlighted.</p> <p></p> <p>Start- and endpoints of tracks intersecting at least one of the selected sections are highlighted.</p> <p></p> <p>Start- and endpoints of tracks intersecting none of the selected sections are highlighted.</p> <p></p> <p>Start- and endpoints of tracks assigned to at least one selected flow are highlighted.</p> <p></p> <p>Start- and endpoints of tracks assigned to none of the selected flows are highlighted.</p> <p></p>"},{"location":"OTAnalytics/usage/usage-ui/#show-detections-of-current-frame","title":"Show detections of current frame","text":"<p>These layers visualize all detections of a single frame either using a bounding box or using the track point (with the current offset).</p> <p>It can be helpful for an in-depth assessment of the quality of samples of detections and tracks.</p> Bounding boxTrack point <p>The bounding boxes of all detections in the currently visible frame are shown.</p> <p></p> <p>The track points of all detections in the currently visible frame are shown.</p> <p></p>"},{"location":"OTAnalytics/usage/usage-ui/#show-events","title":"Show events","text":"<p>This group of layers highlights the locations of events triggered by the intersection of tracks with sections in a given period or at a given moment.</p> <p>Like the other layers, they provide in-depth data validation. Additionally, the event locations themselves can serve as an interesting focus for traffic analysis.</p> <p>Info</p> <p>The locations of the events are determined based on the bounding box offset configured locally for each individual section.</p> <p>In contrast, a global offset can be set for visualising the tracks.</p> <p>As a result, the event locations may not align with the corresponding tracks.</p> <p>To align the tracks, select a specific section and hit the Update with section offset button.</p> Current filterCurrent frame <p>The locations of all events within the date/time filter range are shown.</p> <p></p> <p>All events present at the moment of the current frame are shown.</p> <p></p>"},{"location":"OTAnalytics/usage/usage-ui/#visualization-filters","title":"Visualization filters","text":"<p>As the number of analyzed video hours grows, distinguishing individual tracks in the visualization becomes increasingly difficult. Filters enable you to reduce the number of displayed track and the part of the track which is shown, making the visualization more focused and easier to manage.</p> <p>Info</p> <p>Filters are applied only to the visualization in the workspace and to the calculation of track statistics displayed below. Analyses and data exports always include all loaded tracks, regardless of any applied filters.</p>"},{"location":"OTAnalytics/usage/usage-ui/#filter-by-time","title":"Filter by Time","text":"<p>Filter tracks based on time and date. Only track points that are within the date/time range will be displayed.</p> <p>You can toggle the filter by checking/unchecking the box left to the Filter by date button. By clicking the Filter by date button, a popup appears where you can enter a custom date/time range.</p> <p>The &lt; and &gt; buttons to the right of the Filter by date button allow you to shift the filter forward or backward by its current duration. For more precise adjustments, use the &lt; and &gt; buttons below to fine-tune the filter's position (Seconds, Frames, Events).</p> <p></p>"},{"location":"OTAnalytics/usage/usage-ui/#filter-by-classification","title":"Filter by Classification","text":"<p>Filter tracks based on their classification (e.g., car, bicyclist).</p> <p>You can toggle the filter by checking/unchecking the box left to the Filter by classification button. By clicking the Filter by classification button, a popup appears where you can select the track classes to be displayed.</p>"},{"location":"OTAnalytics/usage/usage-ui/#analysis-exports","title":"Analysis exports","text":"<p>After all sections and flows have been configured, you can choose an analysis for export.</p> <p>Tip</p> <p>We highly recommend using the different visualization layers and the track statisics below the canvas before exporting analyses. This is how you gain detailed insights into the data and can optimize the configuration. It is important to ensure a good quality of your exports.</p>"},{"location":"OTAnalytics/usage/usage-ui/#traffic-counts","title":"Traffic counts","text":"<p>This is the most commonly used metric for analysing traffic, supporting various use cases such as planning, simulation, and safety analysis.</p> <p>The traffic counts are derived from the assignment of tracks to flows. They can be exported as a <code>.csv</code> file, which can be easily imported into your preferred software. You can specify the start and end date and time, as well as the interval for aggregating traffic counts.</p> <p></p> <p>The <code>.csv</code> file includes the following values:</p> Value Datatype Description Example start time string Timestamp indicating the start of the interval 2023-05-24 11:45:00 start occurrence date string Date at the start of the interval 2023-05-24 start occurrence time string Time at the start of the interval 11:45:00 end time string Timestamp indicating the end of the interval 2023-05-24 12:00:00 end occurrence date string Date at the end of the interval 2023-05-24 end occurrence time string Time at the end of the interval 12:00:00 classification string The classification of the track / road user (e.g., car, bicyclist) bicyclist flow string Direction of traffic flow, specifying origin and destination sections south --&gt; east from section string Name of the section where the track originated south to section string Name of the section where the track was directed east count int Number of tracks / road users recorded during the interval 1"},{"location":"OTAnalytics/usage/usage-ui/#event-list","title":"Event list","text":"<p>Traffic events provide a more granular data source for in-depth use cases, such as speed measurements, occupancy durations, or time gap analysis.</p> <p>The events (tracks intersecting sections) can be exported into a single file in one of the following formats:</p> <ul> <li><code>.csv</code> contains the event data in a format that can be read by many software   products.</li> <li><code>.xslx</code> includes the event data along with additional information about the   sections used to generate the events for further analysis in MS Excel.</li> <li><code>.otevents</code> is a bzip2-compressed JSON file that contains the event data,   additional information about the sections used to generate the events, and metadata   collected during processing.</li> </ul> <p>The export includes the following values for each event:</p> Value Datatype Description Example road_user_id string Unique identifier for the track / road user 5f8cd584-f490-4fec-afd0-b55ebf39ab4e#0#102341 road_user_type string Track classification of the road user (e.g., car, pedestrian) car hostname string Name of the camera or device capturing the data OTCamera19 occurrence string Timestamp of the event in date and time format 2023-05-24 11:45:00.000000 frame_number int Frame number of the video corresponding to the event 1 section_id string (optional) Identifier for the section. Not available for the event types enter-scene and leave-scene 1 event_type string Type of event (enter-scene, leave-scene, enter-section, leave-section) enter-section video_name string Name of the video file where the event was recorded OTCamera19_FR20_2023-05-24_11-45-00.mp4 occurrence_sec timestamp Time of the event in seconds since the epoch 1684928700.0 coordinate_px_x float X-coordinate of the road user in pixels 142.60128784179688 coordinate_px_y float Y-coordinate of the road user in pixels 199.48948669433594 vector_px_x float X-component of the road user's movement vector -0.2536773681640625 vector_px_y float Y-component of the road user's movement vector 0.075958251953125 section_name string (optional) Name of the section where the event occurred. Not available for the event types enter-scene and leave-scene north occurrence_day string Day of the event in date format 2023-05-24 occurrence_time string Time of the event in time format 11:45:00.000000"},{"location":"OTAnalytics/usage/usage-ui/#road-user-assignments","title":"Road User Assignments","text":"<p>Use this export in case no aggregation of tracks to traffic counts is wanted, but the assignment strategy of OTAnalytics should be used.</p> <p>All tracks assigned to flows can be exported using the Export road user assignments... Button. The tracks assigned to flows are exported into a <code>.csv</code> file with the following format:</p> Value Datatype Description Example flow_id int Unique identifier for the flow 16 flow_name string Name of the flow north --&gt; east road_user_id string Unique identifier for the track / road user 5f8cd584-f490-4fec-afd0-b55ebf39ab4e#0#102708 max_confidence float Maximum confidence score for the road user's track classification 0.9151925444602966 start_occurrence timestamp Timestamp indicating the starting time of the track 2023-05-24 11:45:07.600000 start_occurrence_day string Date indicating the start of the track 2023-05-24 start_occurrence_time string Time indicating the start of the track 11:45:07.600000 end_occurrence timestamp Timestamp indicating the ending time of the track 2023-05-24 11:45:09.600000 end_occurrence_day string Date indicating the end of the track 2023-05-24 end_occurrence_time string Time indicating the end of the track 11:45:09.600000 start_frame_number int Frame number corresponding to the start of the track 153 end_frame_number int Frame number corresponding to the end of the track 193 start_video_name string Name of the video file where the start of the track was recorded OTCamera19_FR20_2023-05-24_11-45-00.mp4 end_video_name string Name of the video file where the end of the track was recorded OTCamera19_FR20_2023-05-24_11-45-00.mp4 start_section_id int Identifier for the section where the track originated 3 end_section_id int Identifier for the section where the track ended 2 start_section_name string Name of the section where the track started north end_section_name string Name of the section where the track ended east start_coordinate_px_x float X-coordinate of the road user at the start of the track, in pixels 718.1949096679688 start_coordinate_px_y float Y-coordinate of the road user at the start of the track, in pixels 301.2111328125 end_coordinate_px_x float X-coordinate of the road user at the end of the track, in pixels 775.1578369140625 end_coordinate_px_y float Y-coordinate of the road user at the end of the track, in pixels 326.5100158691406 start_vector_px_x float X-component of the road user's movement vector at the start of the track -0.54376220703125 start_vector_px_y float Y-component of the road user's movement vector at the start of the track 1.6538543701171875 end_vector_px_x float X-component of the road user's movement vector at the end of the track 0.179168701171875 end_vector_px_y float Y-component of the road user's movement vector at the end of the track 2.1141113281249773 hostname string Name of the camera or device capturing the road user OTCamera19"},{"location":"OTAnalytics/usage/usage-ui/#track-statistics","title":"Track Statistics","text":"<p>This is a CSV export of the track statistics shown in the workspace below the camera image.</p>"},{"location":"OTCamera/","title":"Features","text":"<p>The eye of OpenTrafficCam.</p> <p>Major revision in progress</p> <p>We are currently developing a completely new PCB featuring a more robust power supply, GPS, LTE connectivity and intrusion detection. The software is also being refactored to support the new PCB and modules.</p> <p>The current documentation is based on the legacy Raspberry Pi Zero W, Raspberry Pi OS and camera stack. We are moving to the Raspberry Pi Zero 2W, Raspberry Pi OS Trixie and the new camera stack.</p> <p>OTCamera is a mobile camera system for capturing traffic videos.</p> <p>It is intended for DIY enthusiasts. You don't want to build OTCamera yourself or are looking for someone to conduct the entire traffic survey for you? Just drop us a message.</p> <p></p>"},{"location":"OTCamera/#key-features","title":"Key features","text":"<ul> <li>Based on Raspberry Pi Zero W</li> <li>Waterproof case</li> <li>Operation with buttons and/or web ui</li> <li>Continuous recording for up to one week (depending on the external battery)</li> <li>Privacy compliant recording</li> <li>Low hardware costs per camera system (DIY)</li> </ul>"},{"location":"OTCamera/gettingstarted/firstuse/","title":"First Use","text":"<p>This page gives a quick overview of the most important steps for using OTCamera in the field. For full details, refer to the individual usage pages linked below.</p>"},{"location":"OTCamera/gettingstarted/firstuse/#quick-start-checklist","title":"Quick-start checklist","text":"<ol> <li>Charge OTCamera and the external USB battery.</li> <li>Connect OTCamera to your local Wi-Fi to sync the hardware clock.</li> <li>On site: open the outer case, connect the USB battery and turn on all switches.</li> <li>Check the LEDs \u2014 green blinks 2x (external power), yellow blinks (Wi-Fi on), red blinks (recording).</li> <li>Connect your phone or tablet to OTCamera's Wi-Fi access point and check the preview at 10.10.50.1.</li> <li>Set the \"24/7\" switch as needed, turn off the Wi-Fi switch and close the case.</li> <li>Mount and align OTCamera using the ball head and bracket.</li> <li>After the measurement: dismount, switch off all switches and charge the batteries.</li> <li>Download the videos via Wi-Fi/SSH or USB flash drive.</li> </ol>"},{"location":"OTCamera/gettingstarted/firstuse/#detailed-instructions","title":"Detailed instructions","text":"<ul> <li>Preparation &amp; Safety \u2014 charging, Wi-Fi sync, on-site checks</li> <li>Buttons and LEDs \u2014 switches and LED blink patterns</li> <li>Mounting \u2014 mounting, aligning and dismounting OTCamera</li> <li>Get the Videos \u2014 downloading videos via Wi-Fi/SSH or USB</li> </ul>"},{"location":"OTCamera/gettingstarted/installation/","title":"Overview","text":"<p>No matter which version you want to install, you will always use a Raspberry Pi as a base.</p> <p>You will need:</p> <ul> <li>Raspberry Pi 2B / 3B(+) / 4 / Zero W / Zero 2W and power supply</li> <li>We recommend a Zero W or Zero 2W because they draw substantially less power.</li> <li>Micro SD card (a High Endurance version is recommended)</li> <li>SD Card Reader</li> <li>Raspberry Pi Imager.</li> </ul> <p>Warning</p> <p>Even though we have completed the guide ourselves, every PC is different and Raspberry OS can change as well. There are steps described with which you can break your Windows, macOS or Linux on your PC. You should know what you are doing. We are not responsible for any damage that may occur.</p>"},{"location":"OTCamera/gettingstarted/installation/#quick-installer","title":"Quick Installer","text":"<p>We provide a simple quick installer which will do most of the work for you. We're assuming that you'll use an DS3132 RTC module. If you are using a different or no RTC module you should adjust the quick installer.</p> <p>Info</p> <p>If you like, you can also setup the Raspberry Pi and install OTCamera manually. The necessary steps are described on the following pages.</p>"},{"location":"OTCamera/gettingstarted/installation/#prepare-the-sd-card","title":"Prepare the SD-Card","text":"<p>To use the quick installer you'll need to prepare a SD-Card first. If done correctly you should now be able to connect to your Raspberry Pi via SSH.</p>"},{"location":"OTCamera/gettingstarted/installation/#update-and-upgrade-the-system","title":"Update and Upgrade the System","text":"<p>After preparing the SD-Card it is important to update and upgrade the Raspberry Pi before using the quick installer.</p> <p>To do this execute the following command:</p> <pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y &amp;&amp; sudo reboot\n</code></pre>"},{"location":"OTCamera/gettingstarted/installation/#download-and-run-quick-installer","title":"Download and Run Quick Installer","text":"<p>Connect to your Raspberry Pi via SSH again after reboot and download and run the quick installer:</p> <pre><code>wget https://raw.githubusercontent.com/OpenTrafficCam/OTCamera/master/raspi-files/install_otcamera.sh\nsudo bash install_otcamera.sh\n</code></pre> <p>You will be asked for some config values.</p> <p>After the installer complete, simply reboot the Raspberry Pi and the OTCamera software should be up and running.</p> <p>If not, please report your issues using the GitHub issue tracker.</p>"},{"location":"OTCamera/gettingstarted/requirements/","title":"Requirements","text":"<p>You will need some special hardware to build your own OTCamera to record videos.</p> <p>OTCamera is based on a Raspberry Pi Zero W and the official Rapberry Pi Camera Module V2 (we are considering V3 modules as soon as picamera2 is stable).</p> <p>A specially designed PCB (printed circuit board) is needed to connect all additional parts to the Raspberry Pi. The single parts for the PCB as well as basic soldering skills are required to assemble a OTCamera.</p> <p></p> <p>As case a common waterproof explorer case is used in addition to a 3D printed inlay which carries all OTCamera parts.</p> <p>To power everthing up, its best to use an external USB-battery.</p> <p>All in all you need:</p> <ul> <li>Raspberry Pi Zero W</li> <li>Raspberry Pi Camera Module V2</li> <li>Ribbon cable to connect Camera to Raspberry</li> <li>Micro SD card</li> <li>Explorer Cases 2209 (or another waterproof case)</li> <li>OTCamera PCB (+ components) (released soon)</li> <li>OTCamera 3D printed parts (released soon)</li> <li>Adafruit PowerBoost 1000C</li> <li>Adafruit PiRTC - Precise DS3231 Real Time Clock</li> <li>1 cell LiPo battery</li> <li>Four switches</li> <li>Three LEDs (for example Barthelme 6V DC, 9.5mm)</li> <li>Some cables and cable shoes</li> </ul> <p>To assemble everything, you will need a drill, a soldering iron and some basics tools as well.</p>"},{"location":"OTCamera/gettingstarted/installation/hwclock/","title":"Hardware Clock","text":"<p>The Raspberry Pi itself doesn't bring a hardware clock. It's assumed that a Pi always has a internet connection to get the current time. Since we want to use OTCamera in the field and thus may not have a Wi-Fi connection to access internet, we'll need a dedicated hardware clock (i.e. real time clock or RTC). It contains a backup battery (a coin cell) to keep track of time. The Pi will use the hardware clock time to set the system time.</p> <p>There are several RTC chips out there. There are even several RTC modules that are specifically designed to use with a Pi. One of the most precise RTC chips is the DS3231. We are using a Adafruit PiRTC - Precise DS3231 Real Time Clock for Raspberry Pi.</p> <p>Adafruit also provides a good (but little bit outdated) tutorial on how to use a RTC on the Pi. The follwing description is based on that tutorial.</p> <p>First, we'll need to enable I2C to communicate with the RTC. Either use <code>sudo raspi-config</code> and navigate to:</p> <ul> <li>Interface Options \u2192 I2C \u2192 Yes</li> </ul> <p>Or just use the command-line version of raspi-config:</p> <pre><code>sudo raspi-config nonint do_i2c 0\n</code></pre> <p>Let's install and use <code>i2c-tools</code> to see if the RTC works:</p> <pre><code>sudo apt install i2c-tools -y\nsudo i2cdetect -y 1\n</code></pre> <p>You should see an output of several lines containing <code>68</code> in one of them.</p> <p>We now need to add a device tree overlay in <code>/boot/config.txt</code> by adding the highlighted line at end of the file:</p> /boot/config.txt (end of file)<pre><code>...\n# (e.g. for USB device mode) or if USB support is not required.\notg_mode=1\n\n[all]\n\n[pi4]\ndtoverlay=vc4-fkms-v3d\n# Run as fast as firmware / board allows\narm_boost=1\n\n[all]\ngpu_mem=128\n\n# OTCamera\ndtoverlay=disable-bt\ndisable_camera_led=1\ndtparam=act_led_trigger=none\ndtparam=act_led_activelow=on\ndtoverlay=i2c-rtc,ds3231\n</code></pre> I'm not using the DS3231 <p>If you are using a different RTC than the DS3231, check out the Adafruit tutorial.</p> <p>In a nutshell: Use <code>dtoverlay=i2c-rtc,ds1307</code> or <code>dtoverlay=i2c-rtc,pcf8523</code> instead.</p> <p>After rebooting the Pi (<code>sudo reboot</code>) you can run <code>sudo i2cdetect -y 1</code> again. Instead of <code>68</code>there should be a <code>UU</code> instead.</p> <p>Now we'll disable the fake hardware clock:</p> <pre><code>sudo apt remove fake-hwclock -y\nsudo update-rc.d -f fake-hwclock remove\nsudo systemctl disable fake-hwclock\n</code></pre> <p>Additionally, we'll need to comment out some lines in <code>/lib/udev/hwclock-set</code>. So run <code>sudo nano /lib/udev/hwclock-set</code> and add <code>#</code> at the beginning of the lines:</p> /lib/udev/hwclock-set<pre><code>#!/bin/sh\n# Reset the System Clock to UTC if the hardware clock from which it\n# was copied by the kernel was in localtime.\n\ndev=$1\n\n#if [ -e /run/systemd/system ] ; then\n#    exit 0\n#fi\n\n#/sbin/hwclock --rtc=$dev --systz\n/sbin/hwclock --rtc=$dev --hctosys\n</code></pre> <p>The Pi should now be able to communicate with the RTC. Let's try it:</p> <pre><code>sudo hwclock -r\n</code></pre> <p>The Pi should now have automatically synced the time. If not, check if the Pi knows the correct time and sync it once:</p> <pre><code>date\nsudo hwclock -w\n</code></pre> <p>Done :)</p> <p>Your OTCamera should now be able to keep track of time without any USB power connected and without access to the internet. We recommend booting your OTCamera once within Wi-Fi range before each recording to update the time, although the DS3231 is fairly accurate.</p>"},{"location":"OTCamera/gettingstarted/installation/otcamera/","title":"Install OTCamera","text":"<p>Now we are ready to install last missing dependencies and setup OTCamera.</p>"},{"location":"OTCamera/gettingstarted/installation/otcamera/#setup-python-and-dependencies","title":"Setup Python and Dependencies","text":"<p>By default, Raspberry OS light doesn't come with PIP and git installed. We will need it to install required packages.</p> <pre><code>sudo apt install python3-pip git -y\n</code></pre> If you are using RaspberryPi OS legacy <p>Since the latest RaspberryPi OS python3 is the default python version. If you are using (not recommended) an older version of RaspberryPi OS, you need to make python3 your default version.</p> <p>Raspberry OS legacy ships with python 2 and python 3. By default python 2 is used. We want to change that to python 3 by adding two single lines to <code>.bashrc</code>.</p> <pre><code>echo \"alias python='/usr/bin/python3'\" &gt;&gt; ~/.bashrc\necho \"alias pip=pip3\" &gt;&gt; ~/.bashrc\n\nsource ~/.bashrc\n\npython --version\npip --version\n</code></pre> <p>Both commands should state, that they are (using) python 3.(x).</p>"},{"location":"OTCamera/gettingstarted/installation/otcamera/#clone-otcamera","title":"Clone OTCamera","text":"<p>We'll need to download OTCamera using git to get all the code we'll need to run OTCamera.</p> <pre><code>git clone --depth 1 https://github.com/OpenTrafficCam/OTCamera.git\n</code></pre> <p>OTCamera requires additional python packages, which need to be installed.</p> <pre><code>cd OTCamera\npip install -r requirements.txt --upgrade\n</code></pre>"},{"location":"OTCamera/gettingstarted/installation/otcamera/#setup-webserver-for-preview","title":"Setup Webserver for Preview","text":"<p>We are currently using nginx (a small webserver) to serve a small HTML file including a preview of the camera view.</p> <p>To install nginx:</p> <pre><code>sudo apt install nginx -y\n</code></pre> <p>We need to configure nginx to serve the OTCamera GUI. Open the nginx config file <code>/etc/nginx/sites-available/default</code> and edit the webserver root.</p> <pre><code>sudo nano /etc/nginx/sites-available/default\n</code></pre> <p>You should see something like this (there may be some comment lines starting with # which you can ignore).</p> /etc/nginx/sites-available/default<pre><code>server {\n        listen 80 default_server;\n        listen [::]:80 default_server;\n\n        root /var/www/html;\n\n        index index.html index.htm index.nginx-debian.html;\n\n        server_name _;\n\n        location / {\n                try_files $uri $uri/ =404;\n        }\n\n}\n</code></pre> <p>The important line we need to change is highlighted. Replace <code>/var/www/html</code> with the full path to the <code>OTCamera/gui/webfiles</code> folder. If your username is <code>pi</code> it should be <code>/home/pi/OTCamera/webfiles</code>.</p> <p>Restart nginx afterwards to let it know about the new directory:</p> <pre><code>sudo systemctl restart nginx.service\n</code></pre>"},{"location":"OTCamera/gettingstarted/installation/otcamera/#start-otcamera-on-startup","title":"Start OTCamera on startup","text":"<p>Now we want to start OTCamera every time the Raspberry starts. To do so, we will setup and enable a service. Edit <code>otcamera.service</code> inside the OTCamera repository according to your username and path.</p> ./raspi-files/otcamera.service<pre><code>[Unit]\nDescription=This service starts OTCamera and keeps it running\n\n[Service]\nUser=pi\nWorkingDirectory=/home/pi/OTCamera\nRestart=always\nRestartSec=3\nExecStart=/usr/bin/python3 run.py\n\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Now copy the file to <code>/lib/systemd/system</code> and enable it:</p> <pre><code>sudo cp ./raspi-files/otcamera.service /lib/systemd/system\nsudo systemctl daemon-reload\nsystemctl enable otcamera.service\n</code></pre> <p>After rebooting the Raspberry you should be able to access it's Wi-Fi ap and to open the OTCamera status site using the Raspberry's ip address: http://10.10.50.1</p>"},{"location":"OTCamera/gettingstarted/installation/prepare-sd-card/","title":"Prepare the SD Card","text":"<p>Download and install the Raspberry Pi Imager, insert the SD-Card and start the Imager.</p> <p></p> <p>It's recommended to use the Raspberry Pi Imager Advanced Options to do some basic setup. Just press Ctrl+Shift+X on startscreen to access advanced options. In newer Raspberry Pi Imager versions you just need to press the gear symbol.</p> <p>Setup a hostname, enable SSH, either password based or using public-key authentication (recommended!), configure your Wi-Fi credentials and choose the correct locale for Wi-Fi, timezone and keyboard layout. You can also skip the first-run wizard.</p> <p>If you set the default username to anything different than \"pi\" (which is recommended) you'll need to replace \"pi\" in the following documentation with your username. For example: instead of connecting to <code>ssh pi@hostname</code> you'll need <code>ssh username@hostname</code>.</p> How to generate a public key <p>Generate SSH-Keys for password-less connection. On your desktop computer open a command-line terminal (CMD or Powershell on Windows or a bash on Linux) and run</p> <pre><code>ssh-keygen\n</code></pre> <p>to generate a public private key combination. Add the private key to your ssh-agent (you may need to update OpenSSH on Windows).</p> <p></p> <p>Now insert the SD card into your PC. Select Raspberry Pi OS Lite (Legacy, 32-bit) as operating system under \"Raspberry Pi (Other)\". Then select the SD card on which the operating system will be installed.</p> <p>Warning</p> <p>It is possible to select not only the SD card, but also the hard drive or SSD where your operating system or other PC data is stored. Make sure that you really select the right SD card. Most of the time, you can tell by the size.</p> <p>Press the Write button and wait until the Raspberry Pi Imager has completely written and verified your SD card.</p> <p>Tip</p> <p>Sometimes the verification may fail. Try another USB port on your PC or another card reader. If this does not help, maybe the SD card is broken.</p> Setup without Raspberry Pi Imager <ol> <li> <p>Add an empty file named <code>ssh</code> to the boot partition to enable ssh on first boot.</p> </li> <li> <p>Add your Wi-Fi credentials as described in the Raspberry Pi Documentation</p> </li> <li> <p>Connect to the pi using ssh (<code>ssh pi@raspberry</code>)to the Pi using password authentication.</p> <p>Create and edit the needed <code>authorized_keys</code> file.</p> <pre><code>mkdir -p ~/.ssh\nnano ~/.ssh/authorized_keys\n</code></pre> <p>Copy your public key on the host and paste it on the pi, save&amp;close using Ctrl+X - Enter - Y.</p> </li> </ol>"},{"location":"OTCamera/gettingstarted/installation/prepare-sd-card/#next-steps","title":"Next steps","text":"<p>You're now ready to boot your Raspberry Pi to install OTCamera. You can either do it by using our Quick Installer or follwing all steps on the next sites.</p>"},{"location":"OTCamera/gettingstarted/installation/raspberry/","title":"Setup the Raspberry","text":"<p>Now, take the SD-Card and insert it in the Raspberry. You can now boot the Pi on LAN or within your Wi-Fi by connecting it to the power supply. Try to connect to the Pi using a command-line or PowerShell:</p> <pre><code>ssh pi@otcamera01\n</code></pre> <p>If everything is setup correctly, you will be asked to add the host key to the list of known hosts (answer: yes) and you should be connected to your Raspberry Pi.</p> warning: agent returned different signature type ssh-rsa (expected rsa-sha2-512) <p>If you are on Windows you may need to update OpenSSH if you ar getting this error:</p> <pre><code>warning: agent returned different signature type ssh-rsa (expected rsa-sha2-512)\n</code></pre> <p>Warning</p> <p>This guide should run on Windows 10 but you are modifying your system files. Please do not do anything you do not understand! Read the source for more information. We are not liable for any damage you may cause to your system. But we followed the steps ourselves and everything went well.</p> <ol> <li> <p>Download the latest OpenSSH for Windows release here. Depending on your Windows version it is probably <code>OpenSSH-Win64.zip</code>.</p> </li> <li> <p>Open Windows Explorer and navigate to your Download folder. You should see the <code>OpenSSH-Win64.zip</code>. Open an elevated PowerShell via File \u2192 Open Windows PowerShell \u2192 Open Windows PowerShell as administrator.</p> </li> <li> <p>If you copy and paste the following code, you will unzip OpenSSH and overwrite the system's default OpenSSH version.</p> <pre><code># Overwrite windows installed bins\n$openSshBins = (Get-ChildItem 'C:\\WINDOWS\\System32\\OpenSSH\\').Name\nExpand-Archive -Path .\\OpenSSH-Win64.zip -DestinationPath .\ntakeown.exe /a /r /f C:\\Windows\\System32\\OpenSSH\\\nicacls.exe 'C:\\Windows\\System32\\OpenSSH' /grant 'BUILTIN\\Administrators:(OI)(CI)F'\nicacls.exe 'C:\\Windows\\System32\\OpenSSH' /grant 'BUILTIN\\Administrators:F' /t\nStop-Service ssh-agent\n$openSshBins | %{ Copy-Item -Path .\\OpenSSH-Win64\\$_ -Destination C:\\Windows\\System32\\OpenSSH\\ }\nStart-Service ssh-agent\n</code></pre> </li> </ol> <p></p> <p>If you have successfully logged in now, we can configure the Raspberry Pi for OTCamera.</p>"},{"location":"OTCamera/gettingstarted/installation/raspberry/#update","title":"Update","text":"<p>Update the pi by running apt and reboot.</p> <pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y &amp;&amp; sudo reboot\n</code></pre> A new version of configuration file is available <p></p> <p>If you get this message, don't worry. Keep the local version currently installed, since we changed the ssh server configuration using the RPi Imager.</p>"},{"location":"OTCamera/gettingstarted/installation/raspberry/#raspi-config","title":"Raspi Config","text":"<p>Reconnect to your pi (open PowerShell and run <code>ssh pi@otcamera01</code>) and run the raspberry configuration tool.</p> <pre><code>sudo raspi-config\n</code></pre> <p>Change the following settings to appropriate values:</p> <ul> <li>System Options \u2192 Password (if not already done with Raspi Imager choose a new password for security reasons)</li> <li>Interface Options \u2192 I1 Legacy Camera \u2192 yes (since the new camera API is not supported by picamerax)</li> <li>Advanced options \u2192 GL driver \u2192 G1 Legacy (This may take a while, but saves a lot of energy.)</li> </ul> Setup without Raspberry Pi Imager <p>If you did not use the Raspberry Pi Imager, you will need to setup a few more things.</p> <ul> <li>System Options<ul> <li>Hostname</li> </ul> </li> <li>Localization Options<ul> <li>Timezone (Europe/Berlin)</li> <li>WLAN Country (DE)</li> </ul> </li> </ul> <p>Exit the raspi-config selecting \"Finish\" and reboot the Pi afterwards.</p>"},{"location":"OTCamera/gettingstarted/installation/raspberry/#power-saving-options","title":"Power Saving Options","text":"<p>After a reboot we also want to disable the HDMI output for additional power saving. CNX Software made some great power measurements for some Raspberry Pis. We'll need to add a specific line to the file <code>/etc/rc.local</code> in order to deactivate HDMI on every boot.</p> <pre><code>sudo nano /etc/rc.local\n</code></pre> <p>This opens the texteditor nano. We need to insert <code>/usr/bin/tvservice -o</code> in this file as highlighted below. Additionally we'll insert <code>sbin/iw dev wlan0 set power_save off</code> to disable automatic Wi-Fi power saving since we'll deactivate it anyways as soon as we don't need Wi-Fi.</p> /etc/rc.local<pre><code>#!/bin/sh -e\n#\n# rc.local\n#\n# This script is executed at the end of each multiuser runlevel.\n# Make sure that the script will \"exit 0\" on success or any other\n# value on error.\n#\n# In order to enable or disable this script just change the execution\n# bits.\n#\n# By default this script does nothing.\n\n# Print the IP address\n_IP=$(hostname -I) || true\nif [ \"$_IP\" ]; then\n  printf \"My IP address is %s\\n\" \"$_IP\"\nfi\n\n/usr/bin/tvservice -o\n/sbin/iw dev wlan0 set power_save off\n\nexit 0\n</code></pre> <p>Press Ctrl+X and Y and Enter to save the file and exit nano.</p> <p>Additionally we will disable bluetooth and the camera and onboard LED's. Edit <code>/boot/config.txt</code>to do so.</p> <pre><code>sudo nano /boot/config.txt\n</code></pre> <p>The config is quite long. We will add some lines (highlighted) at the end of the file:</p> /boot/config.txt (end of file)<pre><code>...\n# (e.g. for USB device mode) or if USB support is not required.\notg_mode=1\n\n[all]\n\n[pi4]\ndtoverlay=vc4-fkms-v3d\n# Run as fast as firmware / board allows\narm_boost=1\n\n[all]\ngpu_mem=128\n\n# OTCamera\ndtoverlay=disable-bt\ndisable_camera_led=1\ndtparam=act_led_trigger=none\ndtparam=act_led_activelow=on\ndtparam=audio=off\ndisplay_auto_detect=0\ngpio=6,16,18,19=ip\ngpio=16,18,19=pu\ngpio=6=pd\ngpio=5,12,13=op\ngpio=5,12=dl\ngpio=13=dh\n</code></pre> <p>Lines 22 to 27 define the default state of the GPIO pins. If you are using the OTCamera PCB, you want to add those lines. If you are not using it, you may want to adjust settings to your specific setup.</p> <p>Rebooting the Pi activates the new settings.</p> <pre><code>sudo reboot\n</code></pre>"},{"location":"OTCamera/gettingstarted/installation/wifi-ap/","title":"Wi-Fi Accesspoint","text":"<p>In order to access the OTCamera Raspberry Pi in field, we will let the Raspberry create it's own Wi-Fi. First, we'll need to install some packages:</p> <pre><code>sudo apt install hostapd dnsmasq dhcpcd -y\n</code></pre> <p>Now we'll need to configure all three of the newly installed packages. Let's start with hostap.</p> <pre><code>sudo nano /etc/default/hostapd\n</code></pre> <p>We need to modify line 13 to specify a valid <code>hostapd.conf</code>. To do so, insert <code>\"/etc/hostapd/hostapd.conf\"</code> right after <code>DAEMON_CONF=</code> in line 13:</p> /etc/default/hostapd<pre><code># Defaults for hostapd initscript\n#\n# WARNING: The DAEMON_CONF setting has been deprecated and will be removed\n#          in future package releases.\n#\n# See /usr/share/doc/hostapd/README.Debian for information about alternative\n# methods of managing hostapd.\n#\n# Uncomment and set DAEMON_CONF to the absolute path of a hostapd configuration\n# file and hostapd will be started during system boot. An example configuration\n# file can be found at /usr/share/doc/hostapd/examples/hostapd.conf.gz\n#\nDAEMON_CONF=\"/etc/hostapd/hostapd.conf\"\n\n# Additional daemon options to be appended to hostapd command:-\n#       -d   show more debug messages (-dd for even more)\n#       -K   include key data in debug messages\n#       -t   include timestamps in some debug messages\n#\n# Note that -B (daemon mode) and -P (pidfile) options are automatically\n# configured by the init.d script and must not be added to DAEMON_OPTS.\n#\n#DAEMON_OPTS=\"\"\n</code></pre> <p>Now let's edit this hostapd.conf file to configure our access point:</p> /etc/hostapd/hostapd.conf<pre><code>channel=11\nssid=MyOTCameraWifiNetwork\nwpa_passphrase=reallysafepassword\ninterface=uap0\nhw_mode=g\nmacaddr_acl=0\nauth_algs=1\nwpa=2\nwpa_key_mgmt=WPA-PSK\nwpa_pairwise=TKIP\nrsn_pairwise=CCMP\ncountry_code=DE\n</code></pre> <p>If you are planning to connect OTCamera to your own Wi-Fi (e.g. to transfer files in office or to get internet access to update etc.) you must use the same Wi-Fi channel as your office Wi-Fi network (since the Raspberry has just one antanna). You should edit your office Wi-Fi to always use the same channel to avoid automatic channel selection.</p> <p>The name of the Wi-Fi is specified just after <code>ssid=</code> and the password in line 3.</p> <p>Depending on where you will use OTCamera you should set the according contry code in the last line (for us it's Germany --&gt; DE).</p> <p>Save and exit the file.</p> <p>If you will connect to the OTCamera's Wi-Fi your device will need a valid ip address. dhcpcd and dnsmasq will help us doing by adding some lines (12-14) to the end of <code>/etc/dhcpcd.conf</code>:</p> /etc/dhcpcd.conf<pre><code>...\n# It is possible to fall back to a static IP if DHCP fails:\n# define static profile\n#profile static_eth0\n#static ip_address=192.168.1.23/24\n#static routers=192.168.1.1\n#static domain_name_servers=192.168.1.1\n\n# fallback to static profile on eth0\n#interface eth0\n#fallback static_eth0\ninterface uap0\n        static ip_address=10.10.50.1/24\n        nohook wpa_supplicant\n</code></pre> <p>If your office Wi-Fi uses the same address range you should use another one by, for example, using 51 instead of 50. But you need to remember that address to connect to your OTCamera later on.</p> <p>Finally, let's configure dnsmasq's config (<code>/etc/dnsmasq.conf</code>). It's a quite long file with a lot of explaining comments. We will backup this template and afterwards create a new, empty config:</p> <pre><code>sudo mv /etc/dnsmasq.conf /etc/dnsmasq.conf.backup\nsudo nano /etc/dnsmasq.conf\n</code></pre> <p>Now add the necessary config:</p> /etc/dnsmasq.conf<pre><code>interface=lo,uap0\nno-dhcp-interface=lo,wlan0\nbind-interfaces\nserver=10.10.50.1\ndomain-needed\nbogus-priv\ndhcp-range=10.10.50.10,10.10.50.250,2h\n</code></pre> <p>If you chose a different ip address range in <code>/etc/dhcpcd.conf</code> you'll need to adjust ip addresses here as well.</p> <p>Puh, this was quite a lot configuration edit stuff... But we are almost ready :)</p> <p>We don't want the services to start uncoordinated at bootup. So let's unmask hostapd first and then disable all three services to not start up at boot:</p> <pre><code>systemctl unmask hostapd.service\nsystemctl disable hostapd.service\nsystemctl disable dhcpcd.service\nsystemctl disable dnsmasq.service\n</code></pre> <p>We'll use a script instead to start up services. Make sure you are still inside the OTCamera directory.</p> <pre><code>sudo cp ./raspi-files/usr/local/bin/wifistart /usr/local/bin/wifistart\n</code></pre> <p>Last but not least, let's add this script to <code>/etc/rc.local</code> to start it at boot.</p> /etc/rc.local<pre><code>#!/bin/sh -e\n#\n# rc.local\n#\n# This script is executed at the end of each multiuser runlevel.\n# Make sure that the script will \"exit 0\" on success or any other\n# value on error.\n#\n# In order to enable or disable this script just change the execution\n# bits.\n#\n# By default this script does nothing.\n\n# Print the IP address\n_IP=$(hostname -I) || true\nif [ \"$_IP\" ]; then\n  printf \"My IP address is %s\\n\" \"$_IP\"\nfi\n\n/usr/bin/tvservice -o\n/sbin/iw dev wlan0 set power_save off\n\n/bin/bash /usr/local/bin/wifistart\"\n\nexit 0\n</code></pre> <p>Yeah, we're done! If you carfully followed this steps and we maintained this manual (...) the Raspberry should still connect to your Wi-Fi and will also create it's own Wi-Fi. It could take a minute or two, so don't worry to early. Let's try:</p> <pre><code>sudo reboot\n</code></pre> <p>All good? We hope so!</p>"},{"location":"OTCamera/usage/buttons_leds/","title":"Buttons and LEDs","text":"<p>OTCamera can be controlled using four switches. Its operating status is indicated by 3 LEDs.</p> <p>The switches from left to right are:</p> <ul> <li>Reset</li> <li>Boot</li> <li>Wi-Fi</li> <li>24/7</li> </ul> <p>The three LEDs correspond to the rightmost three switches.</p>"},{"location":"OTCamera/usage/buttons_leds/#reset-switch","title":"Reset Switch","text":"<p>The Reset switch functions as a constant power supply for the Raspberry Pi. By switching it off the power supply to the Raspberry Pi is interrupted. Therefore, it is recommended to keep the Reset switch always turned on unless there is a problem with the Raspberry Pi.</p> <p>Potential Data Loss</p> <p>When the Reset switch is pressed, the Raspberry Pi turns off immediately but is not shut down properly. This can potentially affect the file system, and the SD card may become unreadable.</p> <p>The other three switches are used for normal operation.</p>"},{"location":"OTCamera/usage/buttons_leds/#boot-switch-and-led","title":"Boot Switch and LED","text":"<p>The Boot switch is used to turn the system on and off. When it is turned on, the green LED below the switch remains continuously lit.</p> <p>Once the Raspberry Pi has finished booting and OTCamera has been launched, the green LED starts blinking. Blinking twice means that an external power source is connected via USB. If it blinks only once, the system is running on internal battery power.</p> <p>After OTCamera has finished booting, the other two LEDs also start blinking (if the Wi-Fi and 24/7 switches are turned on).</p>"},{"location":"OTCamera/usage/buttons_leds/#wi-fi-switch-and-led","title":"Wi-Fi Switch and LED","text":"<p>The Wi-Fi switch controls the Wi-Fi module of the Raspberry Pi. This includes both the connection to a Wi-Fi network and the Wi-Fi access point of the OTCamera itself.</p> <p>When the switch is turned off, the Wi-Fi is turned off after a configurable time period (default is 15 minutes). This allows to turn off the switch before mounting OTCamera in the field. Accessing the web interface is still possible for a few minutes. This way, the camera can be safely mounted and aligned.</p> <p>When the Wi-Fi is permanently enabled, the Wi-Fi LED blinks once every 5 seconds. The Wi-Fi LED blinks once per second when the timer is active. Once the Wi-Fi is turned off, the Wi-Fi LED stops blinking and turns off completely.</p> <p>If the Wi-Fi switch is already turned off when OTCamera starts up, the Wi-Fi is immediately turned off after the start (without a timer).</p>"},{"location":"OTCamera/usage/buttons_leds/#247-switch-and-record-led","title":"24/7 Switch and Record LED","text":"<p>The 24/7 switch controls when OTCamera records videos. If the switch is turned on, recording is continuous. If the switch is turned off, recording only occurs between a configured start and end time (default is from 06:00 AM to 10:00 PM).</p> <p>The LED below the switch indicates whether the camera is currently recording. It should blink every 5 seconds. When the LED blinks, a new preview for the web interface is also generated.</p>"},{"location":"OTCamera/usage/buttons_leds/#led-blink-patterns","title":"LED Blink Patterns","text":"LED Blink Pattern Description Boot (green) <code>__________</code> OTCamera powered off. Boot (green) <code>**********</code> OTCamera is booting. Boot (green) <code>*_*_______</code> OTCamera is running and connected to external power (USB). Boot (green) <code>*_________</code> OTCamera is running and not connected to external power (USB). Wi-Fi (yellow) <code>__________</code> Wi-Fi and Access Point (AP) are turned off. Wi-Fi (yellow) <code>*_________</code> Wi-Fi and AP are turned on. Wi-Fi (yellow) <code>*_*_*_*_*_</code> Wi-Fi will turn off after a specific timer period (default is 15 minutes). Record (red) <code>*_________</code> OTCamera is currently recording videos. Record (red) <code>__________</code> OTCamera is not recording videos, probably because the current time is outside the recording timer. <code>_</code>: off <code>*</code>: on"},{"location":"OTCamera/usage/configuration/","title":"Configuration","text":"<p>OTCamera is configured using a YAML configuration file on the Raspberry Pi. The file is located at <code>~/user_config.yaml</code>. Any settings not specified in the file will use sensible defaults.</p> <p>An example configuration file with all available options can be found in the OTCamera repository.</p> <p>The configuration file is organized into the following sections:</p>"},{"location":"OTCamera/usage/configuration/#recording","title":"Recording","text":"<p>Controls when and how long OTCamera records videos.</p> Parameter Default Description <code>start_hour</code> 6 Hour to start recording (0-23) <code>end_hour</code> 22 Hour to stop recording (0-23) <code>interval_length</code> 15 Minutes per video file before splitting <code>num_intervals</code> 0 Number of intervals to record (0 = unlimited) <code>min_free_space</code> 1 Minimum free disk space in GB before old videos are deleted"},{"location":"OTCamera/usage/configuration/#camera","title":"Camera","text":"<p>Settings for the camera sensor.</p> Parameter Default Description <code>fps</code> 20 Frames per second (10-20 recommended) <code>resolution.width</code> 1640 Internal camera resolution width <code>resolution.height</code> 1232 Internal camera resolution height <code>exposure_mode</code> <code>nightpreview</code> Analog/digital gain control mode <code>drc_strength</code> <code>high</code> Dynamic range compression strength <code>rotation</code> 180 Image rotation in degrees <code>awb_mode</code> <code>greyworld</code> Auto white balance mode (use <code>greyworld</code> for NoIR cameras) <code>meter_mode</code> <code>average</code> Metering mode for exposure adjustment"},{"location":"OTCamera/usage/configuration/#video","title":"Video","text":"<p>Settings for the encoded video output files.</p> Parameter Default Description <code>dir</code> <code>~/videos/</code> Directory where videos are saved <code>format</code> <code>h264</code> Video encoding format <code>resolution.width</code> 800 Output video resolution width <code>resolution.height</code> 600 Output video resolution height <code>encoder.profile</code> <code>high</code> H.264 profile <code>encoder.level</code> <code>4</code> H.264 level <code>encoder.bitrate</code> 600000 Bitrate in bits per second <code>encoder.quality</code> 30 Encoding quality parameter"},{"location":"OTCamera/usage/configuration/#preview","title":"Preview","text":"<p>Controls the preview image shown on the web interface.</p> Parameter Default Description <code>interval</code> 5 Seconds between preview image updates <code>format</code> <code>jpeg</code> Preview image format"},{"location":"OTCamera/usage/configuration/#wi-fi","title":"Wi-Fi","text":"Parameter Default Description <code>delay</code> 900 Seconds before Wi-Fi is turned off after the switch is disabled"},{"location":"OTCamera/usage/configuration/#feature-toggles","title":"Feature Toggles","text":"<p>Several optional features can be enabled or disabled:</p> Section Parameter Default Description <code>leds</code> <code>enable</code> false Enable status LEDs <code>buttons</code> <code>enable</code> false Enable hardware buttons <code>debug_mode</code> <code>enable</code> false Enable additional debug logging <code>relay_server</code> <code>enable</code> false Enable SSH relay server connection"},{"location":"OTCamera/usage/configuration/#example","title":"Example","text":"<p>A minimal configuration that adjusts the recording schedule and video resolution:</p> <pre><code>---\nrecording:\n  start_hour: 7\n  end_hour: 20\n\nvideo:\n  resolution:\n    width: 1280\n    height: 720\n</code></pre>"},{"location":"OTCamera/usage/getvideos/","title":"Get the Videos","text":"<p>After recharging the battery of the OTCamera you can download the recorded videos. You have two options:</p> <ol> <li>Via Wi-Fi and SSH and</li> <li>to an USB flash drive</li> </ol>"},{"location":"OTCamera/usage/getvideos/#wi-fi","title":"Wi-Fi","text":"<p>Turn on OTCamera as described in Preparation &amp; Safety. If OTCamera is connected to the Wi-Fi in your office, you can access via SSH using its IP address. This works the same way as during the installation process.</p> <p>Alternatively, you can establish a SSH connection using the access point of OTCamera.</p> <p>By default the recorded videos and logs are stored in the home directory in <code>~/videos</code>. You can download all videos via scp or rsync.</p>"},{"location":"OTCamera/usage/getvideos/#usb","title":"USB","text":"<p>If you have an OTG USB adapter built into OTCamera, you can also have the videos copied to a flash drive.</p> <p>Turn off OTCamera completely. Plug a flash drive in the USB port of OTCamera and turn on OTCamera using the \"Boot\" switch. During boot-up, OTCamera automatically checks whether a flash drive is plugged in. If this is not the case, the recording function starts (as usual).</p> <p>If a flash drive is plugged in, OTCamera starts in USB copy mode. The yellow LED light blinks to indicate that videos are being copied. If the LED lights up continuously, copying is complete. In addition, the green LED lights up continuously when you can remove the flash drive.</p> <p>OTCamera can then be shut down again by switching off the \"Boot\" switch.</p> <p>The flash drive now contains all videos and a csv file. The csv file gives an overview of all videos on OTCamera (not on the flash drive).</p> <p>When you open the csv file, you can specify that certain videos should be deleted. Just change the corresponding column and insert an <code>x</code> or <code>true</code>.</p> <p>When you save the file and plug the flash drive back into OTCamera,the marked videos will be deleted the next time you boot up.</p>"},{"location":"OTCamera/usage/getvideos/#next-steps","title":"Next steps","text":"<p>Now you should have all the videos. In the next step you can process them with OTVision.</p>"},{"location":"OTCamera/usage/mounting/","title":"Mounting &amp; Dismounting","text":""},{"location":"OTCamera/usage/mounting/#mounting","title":"Mounting","text":"<p>This page describes how to mount and dismount OTCamera in the field.</p> <p>Bring a buddy</p> <p>For mounting the OTCamera in the field its best to get some help. We recommend to do the whole mounting process with two persons.</p> <p>First mount the bracket with the ball head. Check that the ball head is firmly in place. The screws on the ball head should be tightened so that it cannot move. On the back of the outer case you can find a mounting plate that you can slide onto the ball head. The mounting plate is clamped to the ball head with a screw so that OTCamera is firmly attached to it.</p> <p>Danger</p> <p>Make sure that nothing falls down, nobody gets hurt and nothing gets damaged. For this purpose, always use and install a drop protection.</p> <p>Now you can loosen the big screw on the ball head a bit and align the OTCamera to your needs. You can check the alignment with your mobile phone. It is best if two people do this. One person aligns the camera and the other checks the preview.</p> <p>Always wait a few seconds before adjusting OTCamera again, as a preview image is only generated every few seconds.</p> How should I align the camera? <p>We always align OTCamera so that no sky is visible. Otherwise, glare can occur and traffic could no longer be seen. In addition, there should be enough space in all directions, so that OTVision can recognize the trajectories of road users sufficiently.</p> <p>Please be sure to follow these instructions, especially if you want to automatically process the videos with OTVision yourself or have us process them for you.</p> <p>If everything is aligned correctly, you can now lock OTCamera.</p> <p>Check all the values again on the preview page and make sure that everything is displayed correctly. Make sure that all screws are tightened and the fall protection is in place.</p>"},{"location":"OTCamera/usage/mounting/#dismounting","title":"Dismounting","text":"<p>When your measurement is finished, you can dismount OTCamera.</p> <p>Basically, just follow the installation steps in reverse order. When dismounting, make sure that nothing can drop (as always!). In particular, opening the screw on the ball head to loosen the mounting plate can cause OTCamera to fall down. Please take special care that this does not happen.</p> <p>When everything is dismounted and you are back on the ground, you can open the outer case and set the three right-hand switches (Boot, Wi-Fi, 24/7) to off.</p> <p>If OTCamera still has power, it will now shut down and indicate this with a rapidly flashing green LED. If the battery has run down in the meantime, OTCamera has already shut down on its own. Nevertheless, set all three switches to \"0\".</p> Turn of all switches! <p>It is sufficient to set the \"Boot\" switch to \"0\". However, we have made the experience that it is better to make sure that every switch is in the off position. Switch on all the switches at the beginning of the measurement and switch them off at the end. Then everything is back in the starting position for the next measurement.</p> <p>When the OTCamera is back in the office, charge the external USB-battery and the OTCamera itself.</p> <p>Once dismounted, proceed with Get the Videos.</p>"},{"location":"OTCamera/usage/preparation_safety/","title":"Preparation &amp; Safety","text":"<p>This page covers safety considerations and everything you need to prepare before heading out to mount OTCamera.</p> <p>Safety and security</p> <p>Please be aware that your OTCamera must not fall down. This applies to the place where it is mounted but also during assembly.</p> <p>Furthermore, check your local data protection regulations to guarantee you are operating according to the law. Depending on the position of the camera, it could be possible that you have to adjust the focus or take further measures.</p> <p>Usually you are not allowed to just install an OTCamera in public space and record videos. Please make sure you contact the responsible authority and verify whether you need a permission.</p> <p>OTCamera contains a Li-Ion cell and runs with an USB-battery. The maximum operating temperature is around 60 degrees Celsius. In warmer regions or on very hot days the ambient temperature in the case may become hotter, especially in direct sunlight.</p>"},{"location":"OTCamera/usage/preparation_safety/#required-parts","title":"Required parts","text":"<p>OTCamera consists of various components:</p> <ol> <li>The inner case with th electronic circuit, the switches and the LEDs,</li> <li>An external USB-battery with a cable connected to the inner case,</li> <li>The outer case for weather and theft protection,</li> <li>The ball head with clamp for mounting the OTCamera on a pole.</li> </ol> <p>You may also need the following during mounting:</p> <ol> <li>Mobile phone or tablet to view the preview of OTCamera,</li> <li>A ladder to mount the OTCamera sufficiently high,</li> <li>Tools (screwdriver, cordless screwdriver, hexagon socket),</li> <li>A drop guard, to secure OTCamera and the bracket from falling down,</li> <li>A lock to protect against theft,</li> <li>High-visibility clothing for you and your colleagues,</li> <li>Traffic cones to warn pedestrians,</li> <li>The permit for operating in the field.</li> </ol> <p>Disclaimer</p> <p>For preparing the measurement you may need to think about more things than we listed. This list is just to remind you that you need to think of everything.</p>"},{"location":"OTCamera/usage/preparation_safety/#preparation","title":"Preparation","text":""},{"location":"OTCamera/usage/preparation_safety/#in-the-office","title":"In the office","text":"<p>Before every measurement you should take care of a few things.</p> <p>Additionally to the external USB-battery, OTCamera contains an integrated battery. Before use, charge the OTCamera and the external USB-battery. With an 26800 mAh external USB-battery and the integrated battery OTCamera can record up to 3.5 or 4 days in a row. For example, if you install the OTCamera on Monday afternoon, you will have recordings of Tuesday, Wednesday and Thursday.</p> <p>Before use, also connect the OTCamera to your local Wi-Fi to update the time of the hardware clock. This is especially necessary if you did not use the OTCamera for a certain time.</p> <p>Practice!</p> <p>We recommend that every person who wants to mount a OTCamera should practice in a save/\"dry\" environment beforehand. This help to prevent anything going wrong out in the field.</p>"},{"location":"OTCamera/usage/preparation_safety/#on-site","title":"On site","text":"<p>Once everything is set up, you can start preparing the actual measurement on site.</p> <p>Open the outer case, connect the external USB-battery with the inner case and put the external USB-battery in one of the brackets on the right side. Turn on OTCamera by bringing all the switches in their \"on\" position.</p> <p>Now the left green LED lights up permanently and indicates that OTCamera is booting up. This takes a few tens of seconds. After booted up, all LEDs turn off shortly and then start flashing.</p> <p>The power supply over USB is working, when the green LED blinks every five seconds for two times. Blinking only ones means that the power supply works using the internal battery. Make sure that the USB cable is plugged in correctly on both sides and that the external USB battery is switched on.</p> <p>The yellow LED should blink once every five seconds. It means that Wi-Fi is switched on permanently. On the one hand OTCamera connects with your Wi-Fi in the office and you can access OTCamera from your network. On the other hand OTCamera creates it's own Wi-Fi access point. The SSID of the access point and the password were set during installation.</p> <p>The red LED should blink once every five seconds. It means that the camera is currently recording. Every time the LED blinks a new preview is created.</p> <p>Everything is working just fine when the green LED blinks twice and the yellow and red LED blink once every five seconds. For a detailed reference of all LED blink patterns, see Buttons and LEDs.</p> <p>You can now connect to the Wi-Fi access point with your mobile phone or tablet. After connecting to the Wi-Fi, you can call up the IP address of OTCamera in a browser, to see the preview. This is usually 10.10.50.1.</p> Why does my mobile phone not connect? <p>Some mobile phones check whether there internet is accessable via Wi-Fi. If this is not the case, the mobile phone is connected but you cannot access the preview. It helps to change the mobile phone settings that you want to use the connection anyway. Or you can switch off your mobile data and thus force the use of Wi-Fi.</p> <p>We therefore use our tablet without mobile data for our measurements.</p> <p>You should now see a site with the preview image. The site automatically reloads every five seconds. Below the preview image is a table with the most important information.</p> Feature: Circular buffer <p>If you have already taken several measurements and the memory card is full, at some point the oldest video will be deleted so that recording can continue.</p> <p>However, we would still recommend deleting the videos after each measurement.</p> <p>At the top of the page you will see a green bar, if everything works fine. If the camera does not record, the external power supply is not connected or the \"24/7\" switch is not switched on, a yellow or red bar will appear at the top with a message.</p> Check the time! <p>The camera name and the current time are displayed at the top of the preview image. Check the date and time each time you use OTCamera.</p> <p>If the time seems incorrect, first check if you are connected to the respective OTCamera and receive the correct preview image. Refresh you browser to check, if this is not the preview of the OTCamera you mounted previously.</p> <p>If it is the correct preview and the time is still not correct, there are two possible solutions:</p> <ol> <li>You have not connected the OTCamera to the internet for some time and have to do so in order to update the internal real-time clock.</li> <li>The button cell of the internal real-time clock is empty and has to be replaced.</li> </ol> <p>In doubt, hold your mobile phone in the camera image so that you can see the time in the video later. If you continue recording with wrong time, we recommend recording 24/7 to avoid missing data.</p> <p>You can now use the right-hand switch \"24/7\" to set whether OTCamera should record permanently or only at the configured time of day.</p> <p>When you switch off the Wi-Fi switch, the yellow LED flashes faster, indicating that the Wi-Fi is about to turn off. The time until it switches off can be set and is by default 15 minutes.</p> <p>Now you can close the outer case. Be careful not to pinch the USB cable. You do not need to open the outer case again.</p> <p>Always have extra USB cables with you :)</p> <p>Once preparation is complete, proceed with Mounting.</p>"},{"location":"OTLabels/","title":"Features","text":"<p>The backbone of OpenTrafficCam.</p> <p>OTLabels is a workflow for retraining AI models to detect road users in videos based on custom video frames and existing open source software packages (CVAT, YOLO v8).</p>"},{"location":"OTLabels/#key-features","title":"Key features","text":"<ul> <li>Annotation of custom video frames using existing open source software (CVAT)</li> <li>Retrain existing neural network models with YOLO v8 for individual optimization of the detection ability</li> <li>Runs on any Windows laptop. However, a decent GPU with CUDA support is strongly recommended.</li> </ul>"},{"location":"OTLabels/#content-of-documentation","title":"Content of documentation","text":"<ul> <li>Training</li> <li>Validation</li> <li>Models</li> </ul> <p>Most of the code describing the workflow will be developed by the OpenTrafficCam contributors (based on python libraries like NumPy, pandas, tkinter and OpenCV).</p>"},{"location":"OTLabels/gettingstarted/data/","title":"Data","text":"<p>Coming soon</p> <p>Unfortunately, there is no content here yet. But we are currently working on completing this site.</p>"},{"location":"OTLabels/gettingstarted/installation/","title":"Installation","text":"<p>Coming soon</p> <p>Unfortunately, there is no content here yet. But we are currently working on completing this site.</p>"},{"location":"OTLabels/gettingstarted/requirements/","title":"Requirements","text":"<p>Coming soon</p> <p>Unfortunately, there is no content here yet. But we are currently working on completing this site.</p>"},{"location":"OTLabels/models/coco6/","title":"COCO 6-class","text":"<p>Coming soon</p> <p>Unfortunately, there is no content here yet. But we are currently working on completing this site.</p>"},{"location":"OTLabels/training/cvat/","title":"CVAT","text":"<p>CVAT is a free video and image annotation tool for computer vision. We recommend the developers guides for Installation as well as the user's guide.</p> <p>If you want to label your own dataset to retrain models, keep in mind that the format of your new labels need to be the same as the format of the original dataset. On this page we define our intended organizational and technical annotation workflow using CVAT.</p> <p>Since YOLOv5 is based on the the MS COCO dataset of 2D bounding boxes, you need to apply your own labels in the frames from your sample videos in the same format. We therefore recommend the following steps:</p> <ol> <li>Install and set up an instance of CVAT either on your local computer or on a server (recommended when working in a team).</li> <li>Import the videos in CVAT and select the frames you want to use for labelling.</li> <li>Download the dataset from CVAT using the YOLO v1.1 format.</li> <li>Pre-annotate your frames with the standard YOLOv5x model. The results do probably not meet the desired quality.</li> <li>However, the pre-annotated labels will save you some time since it is not necessary to draw all labels from scratch.</li> <li>Upload the pre-annotated frames to CVAT and revise the detected labels.</li> <li>Download the final labels from CVAT and convert them to COCO format (using our cvat_to_coco.py script).</li> </ol> <p>The COCO dataset and consequently the YOLOv5 models can detect 80 classes. However, for the detection of traffic, only six classes are relevant.</p>"},{"location":"OTLabels/training/cvat/#target-classes","title":"Target classes","text":"<p>Target classes are all road user classes from the initial COCO data set that are relevant for our sample videos:</p> <ul> <li>Person</li> <li>Bicycle</li> <li>Motorcycle</li> <li>Car</li> <li>Bus</li> <li>Truck</li> </ul> <p>All vehicles or combinations of vehicles are labeled as \"truck\" if they have</p> <ul> <li>twin tires (except for regular service buses or coaches),</li> <li>a superstructure or a loading area or</li> <li>a trailer (also cars with trailer, the 2D box includes vehicle and trailer).</li> </ul> <p>Here, as in the COCO dataset, both the \"person\" and the \"bicycle\" are labeled separately for cyclists.</p>"},{"location":"OTLabels/training/cvat/#object-dimensions","title":"Object dimensions","text":"<p>The 2D box boundaries are defined by the outermost parts of the objects visible in that frame of the video.</p> <p>Even if only a part of an object is visible and the class of the object is recognizable for the annotator, it should be labeled.</p>"},{"location":"OTLabels/training/cvat/#projects-tasks-and-jobs","title":"Projects, Tasks and Jobs","text":"<p>We define certain set of videos as a project. Every project has multiple tasks (each consists of one video to annotate). Each task has a status (\"Pending\", \"In Progress\" and \"Completed\") and a progress bar showing the number of completed jobs. Open the task with click on \"Open\". For each task (or video) multiple jobs with a certain amounts of frames from the video are defined. Each job has a status (\"annotation\", \"validation\" and \"completed\"), an \"assignee\" (responsible for annotation) and a \"reviewer\" (responsible for reviewing the assignees annotation). Open the job with click on \"Job #...\".</p>"},{"location":"OTLabels/training/cvat/#workflow","title":"Workflow","text":"<p>For each job, the project coordinator assigns the roles of the \"assignee\" (= annotator) and the \"reviewer\". The reviewer then starts working on a job by initial labeling of the video frames. The assignee should save his labeling work from time to time and can also stop and later resume working on a job. After finishing the initial labeling, the assignee requests a review (menu - request review, the assigned reviewer should already be named there) and thus sets status of the job to \"validation\". Then the reviewer checks the initial labeling done by the assignee and raises issues when there is a problem or question. After reviewing all video frames of the job, the reviewer submits the review by either accepting and completing the job or rejecting it (which sets the status of the job bach to \"Annotation\"). Now the assignee has to check the issues, update the labels, resolve the issues and again request a review. The review process starts again and if necessary also next annotations, until the reviewer accepts the job and it is marked as \"Completed\". </p> <p>We recommend the following procedure for annotation in CVAT:</p> <ul> <li>Go through all pre-labeled objects on the right</li> <li>Check and delete false positive labels (if there is no object)</li> <li>Check and delete duplicate labels for the same object</li> <li>Check and correct object class for target classes</li> <li>Check and delete objects from other classes if they are classified wrong</li> <li>Zoom in on one quadrant of the image at a time:</li> <li>Check and correct position of object\u00b4s 2D boxes</li> <li>Check if there are objects from the target classes not already pre-labeled, draw new 2D box and annotate object class</li> </ul>"},{"location":"OTLabels/training/cvat/#download-data","title":"Download data","text":"<p>Coming soon</p> <p>Unfortunately, there is no content here yet. But we are currently working on completing this site.</p>"},{"location":"OTLabels/training/preprocessingdata/","title":"Preprocessing Data","text":"<p>Before retraining the YOLOv5 model, you might want to preprocess your labeled data to ensure that your frames are in the right format.</p>"},{"location":"OTLabels/training/preprocessingdata/#convert-cvat-output-to-yolov5","title":"Convert CVAT output to YOLOv5","text":"<p>OTLabels provides the <code>cvat_to_yolo.py</code> script to convert the annotated data to the appropriate format YOLOv5 needs for retraining the model. Before executing the script, you have to setup the configurations for your needs:</p> <ul> <li>Within the script:</li> <li>Set destPath: path where your data is stored (typically ./data/*).</li> <li>Set annFile: path to the COCO-annotation file (if you do not have it yet, see below how to get it).</li> <li>Set catFile: path to a text file containing your CVAT labels (standard: labels_CVAT.txt).</li> <li>Set cvatFile: path to a CVAT output file (containing images and labels, see also section CVAT: Download data) or to a folder containing multiple CVAT output files.</li> <li>Set name: the name of subfolder of destPath/images and destPath/labels to store the data in.</li> <li>labels_CVAT.txt: a text file with two columns separated by actual commas with headers named Cat and CatId containing the name and the ID of your CVAT labels.   For a example, see the labels_CVAT.txt file in the OTLabels repository.   Please note that labels not provided in this file will not be converted and consequently be deleted.</li> </ul> <p>The script performs the following steps:</p> <ol> <li>Unzip the CVAT output file</li> <li>Copy the images to the directory destPath/images/name.</li> <li>Import the label files and convert the CVAT labels to the COCO labels used by YOLOv5.</li> <li>Export the converted label files to the directory destPath/labels/name.</li> </ol>"},{"location":"OTLabels/training/preprocessingdata/#filter-the-labels","title":"Filter the labels","text":"<p>If for some reason you want to filter the labels that are already stored in your data directory, you can use the filter_labels.py script. Before executing the script, you have to setup the configurations for your needs:</p> <ul> <li>Within the script:</li> <li>Set path: path where the data is stored (typically ./data/*).</li> <li>Set name: the name of one or more subfolder of destPath/images and destPath/labels to store the data in.     More than one name must be provided as list.</li> <li>Set annFile: path to the COCO-annotation file (if you do not have it yet, see below how to get it).</li> <li>Set labelsFilter: path to a text file (standard: label_filter.txt) containing the labels you want to keep.</li> <li>label_filter.txt: a text file containing the category names (e.g., \"car\") of the labels you want to keep (without quotes and one category name per row).</li> </ul> <p>Please note that the images will not be copied to the corresponding folder in destPath/images in order to save file space. Instead, a text file with all images still containing the filtered labels is created. This file of filenames can also be referred to by YOLOv5.</p> <p>The script performs the following steps:</p> <ol> <li>Fetch the category IDs to the corresponding category name.</li> <li>Import the label files.</li> <li>Filter the labels by the provided category names.</li> <li>Export the labels to the directory destPath/labels/name_filtered.</li> <li>Create a text file with all image files in the directory path. Please note that image and label files not include any label after filtering are not exported.</li> </ol>"},{"location":"OTLabels/training/preprocessingdata/#get-coco-annotation-file","title":"Get COCO annotation file","text":"<p>This file is necessary to ensure your labels match the labels of the pretrained YOLOv5 models. Use the get_coco_annotation_files.py script to download one or more COCO annotation files. Configurations to be made:</p> <ul> <li>Within the script:</li> <li>Set path: path where the data is stored (typically ./data/*).</li> <li>Set URLFile: path to the config file that stores the URLs of the annotation files</li> <li>coco_annotation_json_URLs.txt: a text file containing the URLs of the annotation files (without quotes and one URL per row)</li> </ul>"},{"location":"OTLabels/training/preprocessingdata/#get-the-original-coco-dataset","title":"Get the original COCO dataset","text":"<p>In some cases you might want to get the original COCO dataset that was used to train the original YOLOv5 models. Therefore, OTLabels provides the get_coco_data.py script to download the data. Please note that you could also use the get_coco.sh script from the YOLOv5 repository. Before executing the script, you have to set up the configurations for your needs:</p> <ul> <li>Within the script:</li> <li>Set imageURLs: path of a text file (standard: coco_image_URLs.txt) containing the URLs of the image data sets.</li> <li>Set annURL: URL to the labels.</li> <li>Set destPath: path where your data is stored (typically ./data/*).</li> <li>coco_image_URLs.txt: a text file containing the URLs of the images for training, validation and testing (without quotes and one URL per row).</li> </ul>"},{"location":"OTLabels/training/retrainmodel/","title":"Retraining a Model","text":"<p>Although the pretrained YOLOv5 models (especially the bigger ones like YOLOv5l or YOLOv5xl) return acceptable detection results, we experienced some shortcomings for certain object classes which are not well represented and/or distributed in the COCO dataset used for training the YOLOv5 models.</p> <ol> <li>Especially county specific differences were identified, since the COCO dataset contains mainly pictures taken in North-American areas (US trucks vs. European trucks).</li> <li>Also, less represented object classes (e.g., bicycles) might cause worse detection rate or even false detections and hence, to an uneven detection accuracy between the object classes.</li> <li>This might lead to biases in the final counts (e.g., every car is detected, but only every second motorcycle).</li> </ol> <p>Further, one might condense the 80 classes of the pretrained YOLOv5 models to the relevant six classes (pedestrian, bicycle, car, bus, truck, motorcycle) for detecting moving traffic to reduce noise from non-relevant classes.</p>"},{"location":"OTLabels/training/retrainmodel/#first-step-setting-up-the-config-files","title":"First step: setting up the config files","text":"<p>For retraining, there are two relevant config files that needs to be set up.</p>"},{"location":"OTLabels/training/retrainmodel/#data-structure-and-labels","title":"Data structure and labels","text":"<p>YOLOv5 needs a config file in yaml format that contains information about file locations and the labels. This config file is usually stored in the data folder in the OTLabels directory.</p>"},{"location":"OTLabels/training/retrainmodel/#data_structure_labelsyaml","title":"data_structure_labels.yaml","text":"<pre><code># COCO 2017 dataset http://cocodataset.org\n# Train command: python train.py --data coco.yaml\n# Default dataset location is next to /yolov5:\n#   /parent_folder\n#     /coco\n#     /yolov5\n\n# download command/URL (optional)\n# download: bash data/scripts/get_coco.sh\n\n# train and val data as \n# 1) directory: path/images/, \n# 2) file: path/images.txt, or \n# 3) list: [path1/images/, path2/images/]\n\ntrain: ../OTLabels/data/path_to_structure_file_training.txt\nval: ../OTLabels/data/coco/path_to_structure_file_validation.txt\ntest: ../OTLabels/data/coco/path_to_structure_file_test.txt\n\n# number of classes\nnc: 6\n\n# class names\nnames: [\"person\", \"bicycle\", \"car\", \"motorcycle\", \"bus\", \"truck\"]\n</code></pre>"},{"location":"OTLabels/training/retrainmodel/#model-structure","title":"Model structure","text":"<p>Further, the configuration file containing the model structure (also in yaml format) needs to be set up or adapted. These configuration files are usually stored in the models folder within the yolov5 directory. Natively, YOLOv5 comes with one configuration file for each model (e.g., yolov5s.yaml). We strongly recommend to keep the model structure itself as it is and only adapt the number of classes, since the retraining process is based on the trained standard weights, which rely on the original structure.</p>"},{"location":"OTLabels/training/retrainmodel/#model_structureyaml","title":"model_structure.yaml","text":"<pre><code># parameters\nnc: 6  # number of classes\n...\n</code></pre>"},{"location":"OTLabels/training/retrainmodel/#second-step-connecting-to-wandb","title":"Second step: connecting to wandb","text":"<p>If you want to have your training process logged and visualized, YOLOv5 comes with the option to connect to weights and biases (wandb). For further instructions, please visit the wandb issue on GitHub. Otherwise you can chose the option not to use wandb when asked.</p>"},{"location":"OTLabels/training/retrainmodel/#third-step-retraining-the-model","title":"Third step: retraining the model","text":"<p>Now, you are ready to start with the retraining of your models. To start the process, simply execute the train.py with the desired options.</p> <p><code>python yolov5/train.py --img **img_size** --batch **batch_size** --epochs **n_epochs** --data path_to/**data_structure_labels**.yaml --weights yolov5/weights/**yolov5_weights** --cfg path_to/**model_structure**.yaml</code></p> <p>For further information about the whole retraining process with YOLOv5, please see the original documentation on GitHub.</p>"},{"location":"OTLabels/validation/gettingstarted/","title":"Getting Started","text":"<p>This section provides a guide on how to install and use OTValidate.</p>"},{"location":"OTLabels/validation/gettingstarted/#installation","title":"Installation","text":"WindowsLinux/ macOS IntelApple M1 <p>Note</p> <p>Installation instructions for Windows are following soon.</p> <p>Install OTValidate by cloning the repository  with git:</p> <pre><code>git clone https://github.com/OpenTrafficCam/OTValidate.git\ncd OTValidate\npip install -r requirements.txt\npip install .\n</code></pre> <p>Or install by using the <code>Makefile</code>:</p> <pre><code>git clone https://github.com/OpenTrafficCam/OTValidate.git\ncd OTValidate\nmake install\n</code></pre> <p>The installation for machines running on Apple's M1 chip is not as straightforward. There are two ways to install <code>OTValidate</code> on an M1 Mac. As a prerequisite the package manager Homebrew is required.</p> <ol> <li> <p>By executing these commands in the following order:</p> <pre><code>git clone https://github.com/OpenTrafficCam/OTValidate.git\ncd OTValidate\nbrew install openblas\nOPENBLAS=$(brew --prefix openblas) CFLAGS=\"-falign-functions=8 ${CFLAGS}\" pip install scipy==1.7.2\npip install -r requirements.txt\npip install .\n</code></pre> </li> <li> <p>By using the <code>Makefile</code></p> <pre><code>git clone https://github.com/OpenTrafficCam/OTValidate.git\ncd OTValidate\nmake install_m1\n</code></pre> </li> </ol>"},{"location":"OTLabels/validation/gettingstarted/#prerequisites","title":"Prerequisites","text":""},{"location":"OTLabels/validation/gettingstarted/#image-annotation-data","title":"Image Annotation Data","text":"<p>The folder containing the ground truth annotations of the images need to be in the YOLO format:</p> <pre><code>annotation_data\n\u2502   obj.data\n\u2502   obj.names\n\u2502   train.txt\n\u2502\n\u2514\u2500\u2500\u2500obj_train_data\n    \u2502   frame_01.png\n    \u2502   frame_01.txt\n    \u2502   frame_02.png\n    \u2502   frame_02.txt\n    \u2502   ...\n    \u2502\n</code></pre>"},{"location":"OTLabels/validation/gettingstarted/#usage","title":"Usage","text":""},{"location":"OTLabels/validation/gettingstarted/#analyse-object-detection-performance","title":"Analyse Object Detection Performance","text":""},{"location":"OTLabels/validation/gettingstarted/#quickstart-guide","title":"Quickstart Guide","text":"<pre><code>from OTValidate  import evaluate_detection_performance\n\n# path to the directory containing the annotated dataset in otdet format\ngt_data = \"path/to/data1\"\n\n# model weights\nmodel1 = \"path/to/model_weights1.pt\"\nmodel2 = \"path/to/model_weights2.pt\"\nmodel3 = \"path/to/model_weights3.pt\"\n</code></pre> <p>Use the <code>evaluate_detection_performance</code> function to calculate a set of object detection metrics of the respective models:</p> <pre><code>evaluate_detection_performance(\n    path_to_model_weights=[model1, model2, model3],\n    yolo_path=yolo_path,\n    otdet_gt_dir=gt_data,\n    is_gt_xyxy_format=False, # whether the ground truth's bounding box is in xyxy or xywh format\n    normalized=True,\n)\n</code></pre>"},{"location":"OTLabels/validation/gettingstarted/#results","title":"Results","text":"<p>The evaluation results of the models will be saved in the directories containing the annotation data. An <code>out</code> directory containing all the results will be created there.</p>"},{"location":"OTLabels/validation/metrics/","title":"Performance Metrics","text":"<p>In this section we briefly go over the different metrics to evaluate our models on how they fare in the tasks of object detection and object tracking. Furthermore, we will look into the different metrics used to evaluate the results in terms of traffic analysis.</p>"},{"location":"OTLabels/validation/metrics/#object-detection","title":"Object Detection","text":"<p>In the task of object detection a model is considered to be good if it is able to detect and classify an object correctly. In this section we are going to have a look in the different object detection performance metrics.</p>"},{"location":"OTLabels/validation/metrics/#intersection-over-union-iou","title":"Intersection Over Union (IOU)","text":"<p>We can tell if a predicted bounding box matches a ground truth bounding box by calculating and looking at the IOU of the two bounding boxes. As Padilla et al. explained in the paper<sup>1</sup>, \"a perfect match is considered when the area and location of the predicted and ground-truth boxes are the same\". Therefore, the IOU is calculated by determining the area of the intersection of the two bounding boxes and dividing it by the area of the union of the two bounding boxes as shown in:</p> <p></p> <p>Illustration adapted from the paper \"Analysis of Object Detection Metrics with a Companion Open-Source Toolkit\"</p> <p>Thus, two bounding boxes are considered a perfect match if the IOU = 1. Meaning the predicted and ground truth bounding boxes share the same location and the same size.</p> <p>On the other hand, the IOU = 0 when there is no intersection between the predicted and the ground truth bounding box.</p> <p>Usually an IOU threshold is defined in order to decide whether a predicted and ground truth bounding box are considered a match.</p>"},{"location":"OTLabels/validation/metrics/#true-positives-false-positives-false-negatives","title":"True Positives, False Positives, False Negatives","text":"<p>This section will explain what true positives, false positives, false negatives and true negatives are in the task of object detection. Thus, we will look at their definitions as defined by Padilla et al. <sup>1</sup>:</p> <p>A True Positive is a correct detection of a ground-truth bounding box. An incorrect detection of a non-existing object or a misplaced detection of an existing object is a False Positive. An undetected ground-truth bounding box is named False Negative.</p>"},{"location":"OTLabels/validation/metrics/#precision","title":"Precision","text":"<p>Padilla et al. <sup>1</sup> explain precision as \"the ability of a model to identify only relevant objects. It is the percentage of correct positive predictions.\"</p> <p>Precision is calculated as:</p> \\[ Precision = \\frac{TP}{TP + FP} \\]"},{"location":"OTLabels/validation/metrics/#recall","title":"Recall","text":"<p>Padilla et al. <sup>1</sup> explain recall as \"the ability of a model to find all relevant cases (all ground-truth bounding boxes). It is the percentage of correct positive predictions among all given ground truths.\"</p> <p>Recall is calculated as:</p> \\[ Recall = \\frac{TP}{TP + FN} \\]"},{"location":"OTLabels/validation/metrics/#average-precision-ap","title":"Average Precision (AP)","text":"<p>As Padilla et al. explained that an object detection model \"can be considered good if, when the confidence threshold decreases, its precision remains high as its recall increases\"[ <sup>1</sup> ]. Taking this into account a model with a large area under a precision-recall curve indicates a high precision and a high recall. Therefore, the average precision \"is a metric based on the area under a [precision-recall curve]\" <sup>1</sup> with a selected IOU threshold. Thus the following notation for example, AP@50 denotes the average precision with IOU threshold at 50%.</p>"},{"location":"OTLabels/validation/metrics/#mean-average-precision-map","title":"Mean Average Precision (mAP)","text":"<p>We need to keep in mind that the average precision needs to be calculated for each class individually. Hence, the mean average precision \"is a metric that is able to represent the exactness of the detections among all classes\" <sup>1</sup>.</p> <p>The mAP is calculated as follows:</p> \\[ mAP = \\frac{1}{C} \\sum_{i=1}^{C} AP_i \\] <p>where C is the total number of classes and \\(AP_i\\) is the average precision of class \\(i\\) <sup>1</sup>.</p>"},{"location":"OTLabels/validation/metrics/#tide-metrics","title":"TIDE Metrics","text":"<p>Bolya et al. created TIDE a General Toolbox for Identifying Object Detection Errors<sup>2</sup>. As Bolya et al. explain in their paper<sup>2</sup> \"mAP succinctly summarizes the performance of a model in one number\". Thus, the mAP performance metric does not give us any insight on what and how the different error types influence its score, that is the mAP score. The aim of TIDE is exactly that, to give us this insight on how the different error types affect the mAP score and as Bolya et al. <sup>2</sup> stated giving us \"a comprehensive analysis of each model's strengths and weaknesses\".</p> <p>TIDE defines six main error types as follows:</p> <p>Info</p> <p>The following descriptions of the error types are directly taken from the TIDE source code</p> <ol> <li> <p>Classification Error: Error caused when a prediction would have been marked positive if it had the correct class.</p> </li> <li> <p>Localization Error: Error caused when a prediction would have been marked positive if it was localized better.</p> </li> <li> <p>Both Cls and Loc Error: This detection didn't fall into any of the other error categories.</p> </li> <li> <p>Duplicate Detection Error: Error caused when a prediction would have been marked positive if the GT wasn't already in use by another detection.</p> </li> <li> <p>Background Error: Error caused when this detection should have been classified as background (IoU &lt; 0.1).</p> </li> <li> <p>Missed Ground Truth Error: Represents GT missed by the model. Doesn't include GT corrected elsewhere in the model.</p> </li> </ol>"},{"location":"OTLabels/validation/metrics/#confusion-matrix","title":"Confusion Matrix","text":"<p>The confusion matrix gives us a visual insight on how our object detection model performs in the classification task.</p> <p>Let us have a look first at the confusion matrix of a binary classification problem:</p> <p></p> <p>Illustration adapted from the paper Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation\" by David M. W. Powers</p> <p>The rows of the above confusion matrix represent the predicted class whereas the columns represent the ground truth class. Thus a prediction can be categorized as follows:</p> <ol> <li> <p>A prediction that has been predicted as positive class and that is found to be an actual/real positive class in the ground truth, is counted as a true positive.</p> </li> <li> <p>A prediction that has been predicted as negative class and is found to be an actual/real negative class is counted as a true negative.</p> </li> <li> <p>A prediction that has been predicted as positive and is found to be not an actual/real positive class is counted as a false positive.</p> </li> <li> <p>A prediction that has been predicted as negative and is found to be not an actual/real negative class is counted as a false negative.</p> </li> </ol> <p>Hence, the confusion matrix gives us a clear visualization of how many of our predictions were classified correctly or incorrectly.</p> <p>The confusion matrix of multi classification problem looks a little bit different:</p> <p></p> <p>As the image above implies, we now have multiple classes. For this example we want to classify the class <code>car</code>, <code>person</code> and <code>truck</code>. The green color coded tiles denote the true positive predictions.</p> <p>Let's take for example the row denoted with the class <code>car</code>. Here is how it is to be interpreted: Out of the 8 cars that have been predicted:</p> <ul> <li>three were correctly classified as a <code>car</code>,</li> <li>none were incorrectly classified as a <code>person</code> and</li> <li>5 were incorrectly classified as <code>truck</code></li> </ul>"},{"location":"OTLabels/validation/metrics/#traffic-measures","title":"Traffic Measures","text":"<p>To see, how well OpenTrafficCam performs see OTAnalytics.</p>"},{"location":"OTLabels/validation/metrics/#references","title":"References","text":"<ol> <li> <p>Padilla, R., Passos, W. L., Dias, T. L., Netto, S. L., &amp; da Silva, E. A. (2021). A comparative analysis of object detection metrics with a companion open-source toolkit. Electronics, 10(3), 279. https://doi.org/10.3390/electronics10030279 \u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Bolya, D., Foley, S., Hays, J., &amp; Hoffman, J. (2020). Tide: A general toolbox for identifying object detection errors. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part III 16 (pp. 558-573). Springer International Publishing. https://dbolya.github.io/tide/paper.pdf \u21a9\u21a9\u21a9</p> </li> </ol>"},{"location":"OTLabels/validation/modelvalidation/","title":"Model Validation","text":"<p>In this section you will find a comparison of the different YOLO models on how well they perform in the object detection. The YOLO models to be evaluated are YOLOv5s, YOLOv5m, YOLOv5l and YOLOv5x. The models are evaluated on a custom dataset consisting of custom video recordings. Thus, we want to investigate which of the four YOLO models might be best suited in the detection of traffic objects.</p>"},{"location":"OTLabels/validation/modelvalidation/#dataset","title":"Dataset","text":"<p>As mentioned above, the dataset consists of different video recordings capturing different traffic scenes. Thus, the video recordings are turned into datasets that will be used to evaluate the object detection models. It is important to note that not every single frame is selected, but only every \\(n\\)-th frame where \\(n \\in \\mathbb{N}\\) and \\(n\\) is arbitrarily chosen by the user.</p> <p>Here is how the scenes look like:</p> Scenes <p>As seen above, there are two instances where scenes were also recorded at night. This makes it interesting to see how the YOLO models perform in detecting traffic objects in low light conditions.</p> <p>The class labels to be considered for our evaluation of the YOLO models are:</p> <ul> <li>person</li> <li>bicycle</li> <li>car</li> <li>motorcycle</li> <li>bus</li> <li>truck</li> </ul>"},{"location":"OTLabels/validation/modelvalidation/#evaluation-procedure","title":"Evaluation Procedure","text":"<p>The YOLO models are evaluated with the help of OTValidate. As a prerequisite, what OTValidate needs in order to start the evaluation are the YOLO models and the labeled ground truth data. In our case the ground truth labeled data is our custom dataset, which needs to be in the CVAT YOLO format. As for the YOLO model, a custom trained or an existing model can be loaded into OTValidate.</p> <p>OTValidate then uses each model to predict the ground truth images. Afterwards, the prediction results and ground truth data are used to calculate the different object detection metrics for model comparison. In our case, we will use the mAP and the TIDE metrics.</p> <p>Info</p> <p>OTValidate uses a list of class labels as a parameter. All predictions and ground truth data are then filtered according to that list of class labels. Meaning, all predictions or ground truth data, whose predicted or labeled class are not contained in the list, are discarded and therefore not regarded in the evaluation process. This is especially useful if the ground truth data contains class labels that the model can't predict.</p>"},{"location":"OTLabels/validation/modelvalidation/#evaluation","title":"Evaluation","text":"<p>In this section you will find the evaluation results of each model as well as the comparison. Specifically, the models are evaluated based on each dataset depicted by the scenes as shown above in the table and on all the ground truth data. The models confidence and IOU threshold are set to 0.25 and 0.5 respectively. Meaning, all detections that have an IOU lower than 0.5 are not regarded as possible detections and all detections with a confidence lower than 0.25 are discarded.</p>"},{"location":"OTLabels/validation/modelvalidation/#evaluation-on-all-data","title":"Evaluation on all data","text":"<p>This diagram depicts each models mAP at different IOU thresholds:</p> <p></p> <p>We can clearly see that the YOLOv5s model's mAP is the lowest compared to the other models. Meaning, by only looking at the mAP metric the YOLOv5s, YOLOv5m, and the YOLOv5x models are to be considered.</p> <p>Nevertheless, let us also have a look at the TIDE metrics to get an insight on the types of errors made by the models:</p> TIDE Metrics <p>What immediately stands out are the Missed Ground Truth Error (Miss) and the Classification Error (Cls). Meaning, the models were not able to detect many ground truth bounding boxes or be able to classify them correctly. Thus, the models were able to detect the majority of the bounding boxes, but had problems predicting the correct classes.</p> <p>The YOLOv5s model had the highest classification error rate out of the four models. Although, the missed ground truth error rate is also the highest, it does not differ much from the other three models.</p> <p>It becomes apparent that the bigger models perform better at detecting bounding boxes than the smaller ones. But there is also a point where the models' performance, namely the YOLOv5m, YOLOv5l and YOLOv5x, don't differ much at all. Implying that there is not much of a trade-off in choosing the YOLOv5m or YOLOv5l over the YOLOv5x.</p> <p>Still, let us try to find out why the Missed Ground Truth Error appears to be a problem for all four models by evaluating them on the data of each scene.</p>"},{"location":"OTLabels/validation/modelvalidation/#evaluation-on-each-dataset-of-a-scene","title":"Evaluation on each dataset of a scene","text":""},{"location":"OTLabels/validation/modelvalidation/#scene-1-2","title":"Scene 1 &amp; 2","text":"Scene 1 Scene 2 <p>Scene 1 captures a three way junction. Compared to the combined dataset, the models' mAP value doesn't differ that much. The missed ground truth error of all four models is above 20%. Our assumption as to why the missed ground truth error is so high might be due to moving objects appearing in the distance. The models might not be able to detect these objects as they appear to be very small in the picture, as marked in red bounding boxes in the following image:</p> <p></p> <p>It is also possible that the timestamp covers up parts of objects making the models not be able to detect them.</p> <p>Scene 2 is similar to scene 1 in respect to the metrics calculated on all four models. The models' mAP evaluated on the dataset of scene 2 is higher than those of scene 1 when looking at the YOLOv5's <code>m</code>, <code>l</code> and <code>x</code> models. Scene 2's TIDE errors behave also very similar to those of scene 1. Also, small objects that have been annotated with small bounding boxes appear in images of the dataset of Scene 2. This could also be related to the high Missed Ground Truth Error.</p> <p></p>"},{"location":"OTLabels/validation/modelvalidation/#scene-3","title":"Scene 3","text":"<p>Scene 3 captures a four-way junction. There are two datasets capturing scene 2 at daylight and nighttime respectively.</p> Scene 3 At Night <p>Comparing these two datasets, it is not surprising that the models' mAP evaluated on the dataset taken at daylight fares much better to the one evaluated at nighttime. The low light condition makes it really hard for the models to detect any of the objects. The biggest drop in performance in terms of the mAP metric is seen from the YOLOv5s model.</p> <p>But then, why is the Missed Ground Truth Error (Miss) much lower and the Classification Error (Cls) that much higher of the nighttime dataset of Scene 3 compared to the one taken at daylight? Let's have a closer look at the two datasets first:</p> Scene 3 at daylight Scene 3 at nighttime <p>Overall, the amount of ground truth bounding boxes in scene 3 taken at daylight is more than twice as big as the one taken at nighttime. This implies that there is less traffic at nighttime, which makes sense. Thus, there is not much to detect at night resulting in a lower Missed Ground Truth Error. On the other hand, the classification error for all models is above 40% with the exception of the YOLOv5l model whose classification error is at approximately 24%. In addition to that the Localization Error (Loc) is very low. Meaning, the model is able to detect objects very well, but has trouble assigning them to the correct class in badly lit areas. Here is an example where a car is barely recognizable:</p> <p></p> <p>Speaking of scene 3 at daylight, the MissedGround Truth Error is relatively high across all models which is due to the same reason as explained in Scene 1. Traffic objects whose bounding boxes are really small might be detected by the models resulting in misses.</p> <p>But it is also important to take into account that there is the possibility of not moving objects appearing in every image at the same spot in the dataset. Such case can be seen in scene 3 at nighttime where cars have been parked on the sidewalk in the following image:</p> <p></p> <p>Thus, such case could influence the values of the calculated metrics for better or worse.</p>"},{"location":"OTLabels/validation/modelvalidation/#scene-4","title":"Scene 4","text":"<p>Scene 4 captures a rural road.</p> Scene 4 <p>The Missed Ground Truth Error (Miss) is pretty high for the YOLOv5s and the YOLOv5x models with an error value over 50% compared to the YOLOv5m and YOLOv5l models. At this moment, we could not come up with an explanation of why the YOLOv5x model's missed ground truth error is that much higher than its <code>m</code> and <code>l</code> counterpart. But there is an explanation why the missed ground truth error is high across all models. The reason lies in the dataset of scene 4 itself:</p> Scene 4 - images with cut off objects <p>The images above contain objects, which are surrounded by red bounding boxes, that are only partly in their respective image. Therefore, a model might not have been able to detect these objects due to a low confidence score and thus resulting in misses. These type of images appear often in the dataset. Another thing to keep in mind is that there is not much traffic on this rural road. Most images in the dataset do not contain bounding box annotations.</p> <p></p> <p>There are a total of 110 labeled objects in a dataset consisting of 401 images. As a result, even a small number of cut off objects could have a great impact on the metrics.</p>"},{"location":"OTLabels/validation/modelvalidation/#scene-5","title":"Scene 5","text":"<p>Scene 5 captures a rural road.</p> Scene 5 At Night <p>Scene 5 is captured at two different times, namely at day and night. The only difference to scene 3 is that scene 5 captures a rural road instead of a four way junction. Unsurprisingly, the models performed tremendously better on the dataset taken during the day compared to the one taken at night when looking at the mAP diagram.</p> <p>It is not surprising that all models evaluated on the nighttime dataset have a high Missed Ground Truth Error (Miss) which is due to the worse light conditions. What is standing out is the fact that the models evaluated on the day dataset also have a high miss error rate. To get behind the possible reason for this we should have quick peek in the ground truth dataset:</p> <p>Objects or vehicles emerging from the road in the upper left corner of the image make it harder for the models to detect and assign them to the correct class. This is reflected in the missed ground truth error and the localization error. Thus the further the objects move to the right area of the image, the better they are detected by the models.</p>"},{"location":"OTLabels/validation/modelvalidation/#conclusion-and-future-work","title":"Conclusion and Future Work","text":"<p>We have evaluated the <code>YOLOv5s</code>, <code>YOLOv5m</code>, <code>YOLOv5x</code>, and the <code>YOLOv5l</code> object detection models on a custom dataset consisting of videos capturing different scenes of traffic junctions and roads. Looking at mAP and TIDE metrics gave us insight to how the YOLOv5 models performed on our custom dataset and we can come to the conclusion that a <code>YOLOv5m</code>, <code>YOLOv5l</code> and <code>YOLOv5x</code> models performed far better than the <code>YOLOv5s</code>. But there is not a big increase in performance in terms of object detection upon choosing the <code>YOLOv5x</code> over the <code>YOLOv5m</code> or <code>YOLOv5l</code> model.</p> <p>Another important aspect to look into in the future is how much time the models under discussion take to detect the images. Depending on the use case and resources at hand, choosing the <code>YOLOv5x</code> model which might need much more time to finish the detection might not be suitable and thus taking the <code>YOLOv5m</code> or <code>YOLOv5l</code> model might be the better choice.</p> <p>What we are also currently working on is to train our own models using the YOLOv5 models as our foundation on a custom dataset. Thus it would be interesting to see if there would be a significant increase in performance by using custom trained models. If there is a significant increase in performance, the next question to ask if it makes sense to invest time in training a custom model.</p>"},{"location":"OTLabels/validation/overview/","title":"Overview","text":"<p>Validation gives insight on how well a model or software performs. In the case of OpenTrafficCam, we want to evaluate and compare different object detection models on how well they perform in object detection and object tracking. Furthermore, we want to analyse the results of OTAnalytics on a set of traffic performance metrics.</p> <p>For this purpose we are currently developing OTValidate which allows the user to analyse and compare the models' performances on object detection and tracking by calculating metrics corresponding to the specific task at hand. As mentioned above OTValidate will also provide tools to analyse the results of OTAnalytics in regards to traffic performance.</p>"},{"location":"OTLabels/validation/overview/#otvalidate","title":"OTValidate","text":"<p>The image below gives a good overview on the structure of <code>OTValidate</code>:</p> <p></p> <p>For the object detection task <code>OTValidate</code> needs two input files namely an <code>.otdet</code> file and the ground truth label data for the object detection task. Alternatively, a custom or an existing model can be given over as an input instead of an <code>.otdet</code> file.</p>"},{"location":"OTVision/","title":"Features","text":"<p>The heart of OpenTrafficCam.</p> <p>OTVision is a collection of algorithms to generate trajectories of road users from traffic videos. Most of the algorithms have been developed in previous open source projects. We modify them and link them with our own developments into a working pipeline.</p> <p>The current pipeline consists of three core functionalities of OTVision: convert, detect and track.</p> <pre><code>graph LR\n    subgraph OTVision[\"OTVision:\"]\n        direction LR\n        conv(&lt;b&gt;convert&lt;/b&gt;\n        ...raw video\n        files to mp4)\n        det(&lt;b&gt;detect&lt;/b&gt;\n        ...road users\n        in single\n        frames)\n        tr(&lt;b&gt;track&lt;/b&gt;\n        ...detected road\n        users over\n        multiple frames)\n        conv --&gt; det --&gt; tr\n    end\n    tr .-&gt; traj[/.ottrk\n    Trajectories/]\n    vf[/.mp4\n    Video Files/] .-&gt; det\n    rvf[/.h264\n    Video Files/] .-&gt; conv</code></pre>"},{"location":"OTVision/#key-features","title":"Key features","text":"<ul> <li>Can be used without programming knowledge</li> <li>Conversion of .h264 video files to other formats   (using ffmpeg)</li> <li>Detection (joint localization and classification) of road users using   state-of-the-art AI object detection models in single video frames   (currently using YOLOv8 by Ultralytics)</li> <li>Tracking of detected road users over multiple frames   (currently using the   IOU-Tracker by Bochinski et al.)   and over multiple videos.</li> <li>Result: Trajectories in pixel coordinates</li> </ul>"},{"location":"OTVision/firstuse/","title":"First Use","text":"<p>OTVision can be executed on a command-line (e.g., a Terminal on macOS and Linux or the Command Prompt on Windows).</p>"},{"location":"OTVision/firstuse/#the-command-line-interface","title":"The Command-Line Interface","text":"<p>We provide a command-line interface (CLI) to run the OTVision pipeline steps. This documentation is written for using the Command Prompt on Windows or the integrated terminals in Linux or macOS.</p>"},{"location":"OTVision/firstuse/#navigate-to-the-otvision-root-directory","title":"Navigate to the OTVision root directory","text":"<p>Open a Terminal and navigate to the OTVision root folder.</p> <pre><code>cd \"path/to/OTVision\"\n</code></pre> <p>Where is the OTVision root directory?</p> <p>It's the folder you downloaded und unzipped.</p> <p>Maybe your OTVision root directory is called <code>OTVision-main</code> after unzipping, if you downloaded it from GitHub. This is the correct directory.</p> <p>Inside the OTVision root directory, there is another directory called <code>OTVision</code> (this child directory is the wrong directory).</p>"},{"location":"OTVision/firstuse/#build-your-command","title":"Build your command","text":"<p>Every command consists of three parts:</p> <ol> <li>Call <code>uv run</code> to run a Python script.</li> <li>Specify the pipeline step you want to run (<code>convert.py</code>, <code>detect.py</code> or <code>track.py</code>).</li> <li> <p>Specify parameters for your script:</p> <p>For basic use, you only have to specify one parameter: The path(s) to the data you want to process. You can specify a file or folder path (in quotation marks) after the <code>-p</code> (or <code>--paths</code>) argument.</p> Some hints about specifying the paths <p>You can just drag a file or folder and drop them into the terminal.</p> <p>If you provide a path to a folder, every file within the folder will be processed.</p> <p>You can also provide multiple paths straight after one another (each in quotation marks).</p> <p>All other parameters are optional. They can also be set via arguments in the CLI (see \"Usage\" section) or via a separate configuration file.</p> </li> </ol> <p>Have a look at the basic examples provided below.</p>"},{"location":"OTVision/firstuse/#convert","title":"Convert","text":"<p>In case you have raw <code>.h264</code> videos (e.g., from OTCamera), you need to convert them to a supported video format (see convert.py) first. Therefore, we provide the <code>convert.py</code> script.</p> <p>To convert <code>.h264</code> videos, run the following command after activating the venv:</p> <pre><code>uv run convert.py -p \"path/to/your/h264 files\"\n</code></pre> <p>where <code>path/to/h264 files</code> is either the path to a single h264 video file or a folder containing multiple h264 video files.</p> <p>Each converted video will by default be saved as a <code>.mp4</code> file in the same folder with the same name as the input <code>.h264</code> file.</p>"},{"location":"OTVision/firstuse/#detect","title":"Detect","text":"<p>If you have converted your video files to one of the accepted file formats (<code>.avi</code>,<code>.mkv</code>,<code>.mov</code>,<code>.mp4</code>) or you already have your video files in one of those formats, you are ready to detect road users in each single frame of the video(s). Therefore, we provide the <code>detect.py</code> script.</p> <p>To detect video files, run the following command after activating your venv:</p> <pre><code>uv run detect.py -p \"path/to/your/video files\" --expected-duration &lt;video duration [sec]&gt;\n</code></pre> <p>where</p> <ul> <li><code>path/to/video files</code> is either the path to a single video file or a folder containing multiple video files and</li> <li><code>video duration [sec]</code> is the duration of the individual videos.</li> </ul> <p>Naming convention for your video files</p> <p>The filenames must contain the date and time of the start of the video in the following pattern:</p> <p><code>YYYY-MM-DD_HH-MM-SS</code></p> <p>Example: <code>path/to/my/video_2023-04-26_12-15-00.mp4</code></p> <p>By specifying this, OTVision can link the detections and tracks in successive videos so that there are no 'cracks' through many individual videos.</p> <p>If you use OTCamera to record your videos, the video files are automatically named according to this pattern.</p> <p>If you do not know the start date and time, you can simply use a dummy date (e.g. <code>1970-01-01_00-00-00</code>).</p> <p>For each video file, the detected objects will be written to a separate <code>.otdet</code> file in the same folder with the same name as the input video file.</p>"},{"location":"OTVision/firstuse/#track","title":"Track","text":"<p>With the detected road users in one or more <code>.otdet</code> files, you are ready to track the road users over multiple frames of the video (or even over multiple consecutive videos). Therefore, we provide the <code>track.py</code> script.</p> <p>To track <code>.otdet</code> files, run the following command after activating your venv:</p> <pre><code>uv run track.py -p \"path/to/your/otdet files\"\n</code></pre> <p>where <code>path/to/otdet files</code> is either the path to a single <code>.otdet</code> file or a folder containing your <code>.otdet</code> files. If you provide a folder, every <code>.otdet</code> file within the folder will be tracked.</p> <p>For each video file, the tracks will be written to a separate <code>.ottrk</code> file in the same folder as the input <code>.otdet</code> file.</p>"},{"location":"OTVision/installation/","title":"Installation","text":"<p>In this section, we provide instructions on how to install OTVision on the most common Operating Systems.</p> <p>Before installing OTVision, make sure your system meets all requirements. Essentially, you need uv to be installed.</p>"},{"location":"OTVision/installation/#install-otvision","title":"Install OTVision","text":"<p>We provide install scripts for the most common operating systems.</p> <p>Download and unzip the latest version of OTVision from GitHub or clone the OTVision repository.</p> WindowsLinux / macOS <p>Inside the unzipped folder open the <code>install.cmd</code> and wait until the installation of the dependencies is complete.</p> <p>In a terminal, navigate to the OTVision folder and run the installer.</p> <pre><code>./install.sh\n</code></pre> <p>The installation of the dependencies could take a moment.</p> What is installed here? <p>The <code>install</code> script will create a virtual environment (.venv) and install the Python packages specified in the pyproject.toml via uv from the Python Package Index.</p>"},{"location":"OTVision/installation/#nvidia-cuda-optional","title":"Nvidia CUDA (optional)","text":"<p>If you have a Windows or Linux PC with a Nvidia graphics card and already installed CUDA, you chose the release with the suffix <code>-cuda</code>. It contains the requirements to use CUDA. If you want to contribute to OTVision and use CUDA, you have to perform additional steps in your Terminal/Command Prompt:</p>"},{"location":"OTVision/installation/#check-cuda-version","title":"Check CUDA version","text":"<p>Check if CUDA is recognized and available.</p> <pre><code>nvcc --version\n</code></pre> <p>Navigate to the OTVision root directory.</p> <pre><code>cd \"path/to/OTVision\"\n</code></pre> <p>Where is the OTVision root directory?</p> <p>It's the folder you downloaded und unzipped.</p> <p>Maybe your OTVision root directory is called <code>OTVision-main</code> after unzipping, if you downloaded it from Github. This is the correct directory.</p> <p>Inside the OTVision root directory, there is another directory called <code>OTVision</code> (this child directory is the wrong directory).</p>"},{"location":"OTVision/installation/#install-torch-and-torchvision-for-cuda","title":"Install torch and torchvision for CUDA","text":"<p>If you downloaded a <code>-cuda</code> release, you are good to go, if the CUDA version in the <code>pyproject.toml</code> matches your system.</p> <p>To install another version you can do so following the PyTorch documentation:</p> <p>Depending on your operating system (Windows or Linux) and your CUDA version you can select, copy and run the install command from the PyTorch site under \"INSTALL PYTORCH\" (choose Build=\"Stable\", Package=\"pip\" and Language=\"Python\"), then adapt the command to use uv.</p> <p>E.g., for CUDA 12.8 and the latest stable PyTorch Build, the command is:</p> <pre><code>uv pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu128\n</code></pre>"},{"location":"OTVision/installation/#if-you-encounter-problems","title":"If you encounter problems","text":"<p>Maybe you also have to install Microsoft Visual C++ 14.0 or greater from the Visual Studio Build Tools.</p> <p>In case of further problems please open an issue in the OTVision repository on GitHub or contact us. We are happy to know about your experience.</p>"},{"location":"OTVision/installation/#contribute","title":"Contribute","text":"<p>We welcome code contributions (e.g., fixing bugs or adding features) from others by forking the repository and creating a pull request. Please check the contribute section of this documentation first.</p> <p>If you want to contribute code, additional requirements should be installed in the virtual environment. Clone the OTVision repository from GitHub. Run the <code>install_dev.sh</code> in your OTVision folder and wait until the installation of the dependencies is complete.</p>"},{"location":"OTVision/requirements/","title":"Requirements","text":"<p>OTVision is designed to run on most modern operating systems (Windows, Linux and Mac). However, a few prerequisites are required before installing OTVision.</p>"},{"location":"OTVision/requirements/#hardware-prerequisites","title":"Hardware prerequisites","text":"<p>OTVision runs on modern desktops and laptops (e.g. Intel i5+ of the last few generations, AMD Zen chips or Apple Silicon processors and 8 GB RAM).</p> <p>If you want to detect road users in videos with OTVision on a regular basis, we strongly recommend purchasing a powerful desktop computer or workstation (&gt;= 8+ cores, &gt;= 16 GB RAM) with a Nvidia graphics card (&gt;= GeForce 10XX Series, better is usually faster). Make sure that the Nvidia drivers as well as the NVidia CUDA Toolkit are installed and up-to-date to get the best performance.</p> <p>Apple Neural Engine</p> <p>Accelerated detection using the Apple Neural Engine is now supported. Detection on Apple silicon chips can utilize the Neural Engine for improved performance.</p>"},{"location":"OTVision/requirements/#software-prerequisites","title":"Software prerequisites","text":""},{"location":"OTVision/requirements/#uv","title":"uv","text":"<p>OTVision uses <code>uv</code> as a fast Python package installer and resolver to manage dependencies. The installation scripts automatically use <code>uv</code> to install Python packages from the Python Package Index.</p> <p>Additionally, <code>uv</code> can manage Python interpreters itself (download, install, and select specific versions), so pre-installing Python is not required. <code>uv</code> will get and use an appropriate Python interpreter automatically through the required version defined in the <code>pyproject.toml</code> file.</p> <p>Please refer to the uv documentation for further information on how to install <code>uv</code> on your system.</p>"},{"location":"OTVision/requirements/#ffmpeg","title":"ffmpeg","text":"<p>If you want to use videos recorded by OTCamera with OTVision, you need to convert the videos to .mp4 files first. For the conversion, we use the Open Source software ffmpeg.</p> <p>Before using the <code>convert.py</code> script, make sure that ffmpeg is installed and available on the whole system:</p> WindowsLinuxmacOS <p>To install ffmpeg on your Windows system, please perform the following steps:</p> <ol> <li>Download the file <code>ffmpeg-git-full.7z</code> from     gyan.dev.</li> <li>Unzip this file by using any file archiver such as 7zip in a folder of     your choice (e.g., <code>C:\\ffmpeg</code>).</li> <li> <p>Now, open a Command Prompt with administrator privileges and set the environment path variable for ffmpeg:</p> <pre><code>setx /m PATH \"path_to_your_ffmpeg_folder\\bin;%PATH%\"\n</code></pre> <p>where <code>path_to_your_ffmpeg_folder</code> represents the folder that you have ffmpeg     unzipped in.</p> <p>If you unzipped to <code>C:\\ffmpeg</code>, for example:</p> <pre><code>setx /m PATH \"C:\\ffmpeg\\bin;%PATH%\"\n</code></pre> </li> <li> <p>Restart your computer and verify the installation by running</p> <pre><code>ffmpeg -version\n</code></pre> </li> </ol>"},{"location":"OTVision/requirements/#ubuntu-repositories","title":"Ubuntu repositories","text":"<p>If you use Ubuntu, you can install ffmpeg using the official Ubuntu repositories.</p> <pre><code>sudo apt install ffmpeg\n</code></pre>"},{"location":"OTVision/requirements/#manual-installation","title":"Manual installation","text":"<p>For the manual installation of ffmpeg on Linux or if you use another distribution, please refer to the instructions on the developers website.</p>"},{"location":"OTVision/requirements/#homebrew","title":"Homebrew","text":"<p>For the installation using Homebrew, use the following command in the terminal:</p> <pre><code>brew install ffmpeg\n</code></pre>"},{"location":"OTVision/requirements/#manual-install","title":"Manual install","text":"<p>For the manual installation of ffmpeg on macOS, please refer to the instructions on the developers website.</p>"},{"location":"OTVision/requirements/#nvidia-cuda-optional","title":"Nvidia CUDA (optional)","text":"<p>If you intend to use OTVision on a Windows or Linux PC with a modern Nvidia graphics card, download and install version 12.8 of the NVIDIA Cuda Toolkit.</p>"},{"location":"OTVision/usage/configuration/","title":"Configuration File","text":"<p>If you\u00b4re getting tired of providing all those parameters to the CLI, you can also specify them in a configuration <code>yaml</code> file and pass it\u00b4s path to the CLI instead.</p> <p>By default, OTVision refers to the <code>user_config.otvision.yaml</code> in the root directory.</p> <p>You can either</p> <ul> <li>modify this file (then you don\u00b4t have to specify it\u00b4s path in the CLI) or</li> <li>build your own file and save it somewhere else (then you have to specify it\u00b4s path   in the CLI).</li> </ul> <p>You can specify parameters for the sub-tasks that are provided by OTVision (<code>convert</code>, <code>detect</code>, <code>track</code>) in separate configuration files or in a single file. The scripts for the respective sub-tasks only read the parameters they need.</p> <p>Warning</p> <p>Any parameter passed to the CLI will overwrite the respective parameter from the config file.</p> <p>In case of any problems, we suggest looking at yaml.info first when you customize your configuration files.</p>"},{"location":"OTVision/usage/convert/","title":"Convert","text":""},{"location":"OTVision/usage/convert/#synopsis","title":"Synopsis","text":"<pre><code>uv run convert.py  [-p paths] [-c config]\n                    [--fps-from-filename] [--input-fps]\n                    [--delete-input] [--overwrite]\n</code></pre>"},{"location":"OTVision/usage/convert/#description","title":"Description","text":"<p>In case you use raw <code>.h264</code> videos (e.g. from OTCamera), you need to convert the videos to a supported video format.</p> <p>As <code>.h264</code> does not include metadata, the frame rate of the video has to be either read from the filename or specified by the user.</p> <p>We suggest converting to <code>.mp4</code> (the default output file type).</p>"},{"location":"OTVision/usage/convert/#parameters","title":"Parameters","text":""},{"location":"OTVision/usage/convert/#paths-required","title":"paths (required)","text":"<p><code>-p \"path/to/h264 files\" \"path/to/other h264 files\"</code></p> <p>or</p> <p><code>--paths \"path/to/h264 files\" \"path/to/other h264 files\"</code></p> <p>One or multiple paths to <code>.h264</code> files or folders containing <code>.h264</code> video files.</p> <p>This parameter is required to run the <code>convert.py</code> script. It has to be specified either using the CLI or in the configuration YAML file.</p>"},{"location":"OTVision/usage/convert/#config","title":"config","text":"<p><code>-c \"path/to/config file\"</code></p> <p>or</p> <p><code>--config \"path/to/config file\"</code></p> <p>Path to a custom user configuration YAML file. Other parameters (including <code>paths</code>) are parsed from this configuration file.</p> <p>This parameter is optional. By default, the <code>user_config.otvision.yaml</code> in the root directory is read. This also overrides the default values of all parameters. If the <code>user_config.otvision.yaml</code> in the root directory is missing and no other configuration file is specified, the default values of the parameters as described below are used.</p> <p>Warning</p> <p>Any parameter passed to the CLI will overwrite the respective parameter from the config file.</p>"},{"location":"OTVision/usage/convert/#fps-from-filename","title":"fps-from-filename","text":"<p><code>--fps-from-filename</code> to parse the video frame rates from the input <code>.h264</code> filenames.</p> <ul> <li>In this case the frame rate of each input <code>.h264</code> file has to be specified in   the filename using the following pattern: <code>_FR&lt;fps&gt;_</code> (where <code>fps</code> is the frame rate)</li> <li>An example would be <code>_FR20_</code> in the following filename:   <code>OTCamera01_FR20_2023-01-01_12-15-00.h264</code></li> <li>In this case, <code>input-fps</code> will be ignored.</li> </ul> <p>This parameter is used by default if <code>--no-fps-from-filename</code> and <code>--input-fps</code> are not specified.</p>"},{"location":"OTVision/usage/convert/#no-fps-from-filename","title":"no-fps-from-filename","text":"<p>Use <code>--no-fps-from-filename</code> to prevent parsing the video frame rates from the input <code>.h264</code> filenames.</p> <ul> <li>In this case, an <code>input-fps</code> has to be specified.</li> </ul>"},{"location":"OTVision/usage/convert/#input-fps","title":"input-fps","text":"<p><code>--input-fps &lt;int&gt;</code> to set the frame rate for all <code>.h264</code> files.</p> <ul> <li><code>input-fps</code> should be an integer value above zero.</li> <li>E.g. if the input <code>.h264</code> have been recorded at 20 frames per second, specify this   parameter as follows:   <code>--input-fps 20</code></li> </ul> <p>If <code>--fps-from-filename</code> is used, <code>input-fps</code> will be ignored.</p> <p>This parameter is optional and defaults to <code>20</code>.</p>"},{"location":"OTVision/usage/convert/#overwrite","title":"overwrite","text":"<p><code>--overwrite</code> to overwrite existing <code>.mp4</code> files.</p> <p><code>--no-overwrite</code> to prevent overwriting existing <code>.mp4</code> files.</p> <p>This parameter is optional and defaults to <code>--overwrite</code>.</p>"},{"location":"OTVision/usage/convert/#delete-input","title":"delete-input","text":"<p><code>--delete-input</code> to delete input <code>.h264</code> files after conversion to <code>.mp4</code> to save disk space.</p> <p><code>--no-delete-input</code> to keep input <code>.h264</code> files after conversion to <code>.mp4</code>.</p> <p>This parameter is optional and defaults to <code>--no-delete-input</code>.</p>"},{"location":"OTVision/usage/detect/","title":"Detect","text":""},{"location":"OTVision/usage/detect/#synopsis","title":"Synopsis","text":"<pre><code>uv run detect.py   [-p paths] [--expected-duration] [-c config]\n                    [-w weights] [--conf] [--iou] [--half] [--force]\n                    [--overwrite] [--detect-start] [--detect-end]\n</code></pre>"},{"location":"OTVision/usage/detect/#description","title":"Description","text":"<p>If you have converted your video files to one of the supported file formats (<code>.avi</code>,<code>.mkv</code>,<code>.mov</code>,<code>.mp4</code>) or you already have such video files, you are ready to start the detection of the road users in each video frame.</p>"},{"location":"OTVision/usage/detect/#parameters","title":"Parameters","text":""},{"location":"OTVision/usage/detect/#paths-required","title":"paths (required)","text":"<p>Filename convention</p> <p>To prevent to set the start date and time in the config file for each individual video file, the current version of OTVision reads the start date and time from the video filenames.</p> <p>Video files recorded by OTCamera already contain the start date and time in the filename.</p> <p>In case you use OTVision to process video files recorded by other camera systems, please make sure that the filename of these files contain the start date and time in the following format: <code>YYYY-MM-DD_hh-mm-ss</code></p> <p><code>-p \"path/to/video files\" \"path/to/other video files\"</code></p> <p>or</p> <p><code>--paths \"path/to/video files\" \"path/to/other video files\"</code></p> <p>One or multiple paths to video files or folders containing video files.</p> <p>This parameter is required to run <code>detect.py</code>. It has to be specified either using the CLI or in the configuration YAML file.</p>"},{"location":"OTVision/usage/detect/#expected-duration","title":"expected-duration","text":"<p><code>--expected-duration &lt;video duration [sec]&gt;</code></p> <p>Expected duration of each video in seconds (must be all the same). This parameter helps to avoid errors if some images are missing in a video.</p> <p>This parameter is optional. By default, the length of the video is read from the video file.</p>"},{"location":"OTVision/usage/detect/#config","title":"config","text":"<p><code>-c \"path/to/config file\"</code></p> <p>or</p> <p><code>--config \"path/to/config file\"</code></p> <p>Path to a custom user configuration YAML file. Other parameters (including <code>paths</code>) are parsed from this configuration file.</p> <p>This parameter is optional. By default, the <code>user_config.otvision.yaml</code> in the root directory is read. This also overrides the default values of all parameters. If the <code>user_config.otvision.yaml</code> in the root directory is missing and no other configuration file is specified, the default values of the parameters as described below are used.</p> <p>Warning</p> <p>Any parameter passed to the CLI will overwrite the respective parameter from the config file.</p>"},{"location":"OTVision/usage/detect/#weights","title":"weights","text":"<p><code>-w &lt;weights&gt;</code> or <code>--weights &lt;weights&gt;</code></p> <p>Name of weights from PyTorch hub or path to weights file.</p> <p>This parameter is optional and defaults to <code>yolov8s.pt</code>.</p>"},{"location":"OTVision/usage/detect/#conf","title":"conf","text":"<p><code>--conf &lt;float&gt;</code></p> <p>The YOLOv8 model confidence threshold. Should be a float value between zero and one.</p> <p>The confidence threshold is the minimum confidence for a detection to be considered a true detection (otherwise this detection will be ignored entirely).</p> <p>This parameter is optional and defaults to <code>0.25</code>.</p>"},{"location":"OTVision/usage/detect/#iou","title":"iou","text":"<p><code>--iou &lt;float&gt;</code></p> <p>The YOLOv8 model IOU threshold. Should be a float value between zero and one.</p> <p>The IOU threshold is the overlap threshold for areas of bounding boxes used in non-maximum suppression to avoid duplicate detections.</p> <p>This parameter is optional and defaults to <code>0.45</code>.</p>"},{"location":"OTVision/usage/detect/#half","title":"half","text":"<p><code>--half</code> to use half precision (FP16) to speed up detection.</p> <p><code>--no-half</code> to not use half precision.</p> <p>This parameter is optional and defaults to <code>--no-half</code>.</p> <p>Half precision only works for running detection on a GPU!</p>"},{"location":"OTVision/usage/detect/#force","title":"force","text":"<p><code>--force</code> to force a reload of a YOLOv8 standard model from PyTorch hub instead of using a cached model from previous detection runs.</p> <p><code>--no-force</code> to prevent forcing this reload.</p> <p>This parameter is optional and defaults to <code>--no-force</code>.</p>"},{"location":"OTVision/usage/detect/#overwrite","title":"overwrite","text":"<p><code>--overwrite</code> to overwrite existing <code>.otdet</code> files.</p> <p><code>--no-overwrite</code> to prevent overwriting existing <code>.otdet</code> files.</p> <p>This parameter is optional and defaults to <code>--overwrite</code>.</p>"},{"location":"OTVision/usage/detect/#start-time","title":"start-time","text":"<p><code>--start-time</code> to specify the start time of the detection in <code>%Y-%m-%d_%H-%M-%S</code> format. If this parameter is not set, the start time will be parsed from the filename in the given format. The start time needs to be in the filename or given via this parameter.</p> <p>This parameter is optional and defaults to <code>None</code>.</p>"},{"location":"OTVision/usage/detect/#detect-start","title":"detect-start","text":"<p><code>--detect-start</code> to specify the start time of the detection in seconds. Frames occurring before the specified start time will be excluded from the detection process.</p> <p>This parameter is optional and defaults to <code>None</code>.</p>"},{"location":"OTVision/usage/detect/#detect-end","title":"detect-end","text":"<p><code>--detect-end</code> to specify the end time of the detection in seconds. Frames occurring at or after the specified end time will be excluded from the detection process.</p> <p>This parameter is optional and defaults to <code>None</code>.</p>"},{"location":"OTVision/usage/track/","title":"Track","text":""},{"location":"OTVision/usage/track/#synopsis","title":"Synopsis","text":"<pre><code>uv run track.py    [-p paths] [-c config]\n                    [--sigma-l] [--sigma-h] [--sigma-iou] [--t-min] [--t-miss-max]\n                    [--overwrite]\n</code></pre>"},{"location":"OTVision/usage/track/#description","title":"Description","text":"<p>With the detected road users in one or more <code>.otdet</code> files, you are ready to start tracking road users over consecutive frames and even consecutive videos.</p>"},{"location":"OTVision/usage/track/#parameters","title":"Parameters","text":""},{"location":"OTVision/usage/track/#paths","title":"paths","text":"<p><code>-p \"path/to/otdet files\" \"path/to/other otdet files\"</code></p> <p>or</p> <p><code>--paths \"path/to/otdet files\" \"path/to/other otdet files\"</code></p> <p>One or multiple paths to <code>.otdet</code> files or folders containing <code>.otdet</code> files.</p> <p>This parameter is required to run the <code>track.py</code> script. It has to be specified either using the CLI or in the configuration YAML file.</p>"},{"location":"OTVision/usage/track/#config","title":"config","text":"<p><code>-c \"path/to/config file\"</code></p> <p>or</p> <p><code>--config \"path/to/config file\"</code></p> <p>Path to a custom user configuration YAML file. Other parameters (including <code>paths</code>) are parsed from this configuration file.</p> <p>This parameter is optional. By default, the <code>user_config.otvision.yaml</code> in the root directory is read. This also overrides the default values of all parameters. If the <code>user_config.otvision.yaml</code> in the root directory is missing and no other configuration file is specified, the default values of the parameters as described below are used.</p> <p>Warning</p> <p>Any parameter passed to the CLI will overwrite the respective parameter from the config file.</p>"},{"location":"OTVision/usage/track/#sigma-l","title":"sigma-l","text":"<p><code>--sigma-l &lt;float&gt;</code></p> <p>The lower confidence threshold for the IOU tracker. Detections with confidences below <code>sigma-l</code> are not considered for tracking at all.</p> <p>This parameter is optional and defaults to <code>0.27</code>.</p>"},{"location":"OTVision/usage/track/#sigma-h","title":"sigma-h","text":"<p><code>--sigma-h &lt;float&gt;</code></p> <p>The upper confidence threshold for the IOU tracker. Tracks are only considered as valid if they contain at least one detection with a confidence above sigma-h.</p> <p>This parameter is optional and defaults to <code>0.42</code>.</p>"},{"location":"OTVision/usage/track/#sigma-iou","title":"sigma-iou","text":"<p><code>--sigma-iou &lt;float&gt;</code></p> <p>Intersection-Over-Union threshold for the IOU tracker. Two detections in subsequent frames are considered to belong to the same track if their IOU value exceeds sigma_iou and this is the highest IOU of all possible combinations of detections.</p> <p>This parameter is optional and defaults to <code>0.38</code>.</p>"},{"location":"OTVision/usage/track/#t-min","title":"t-min","text":"<p><code>--t-min &lt;int&gt;</code></p> <p>Minimum number of detections to count as a valid track. All tracks with less detections will be dismissed.</p> <p>This parameter is optional and defaults to <code>5</code>.</p>"},{"location":"OTVision/usage/track/#t-miss-max","title":"t-miss-max","text":"<p><code>--t-miss-max &lt;int&gt;</code></p> <p>Maximum number of missed detections before continuing a track. If more detections are missing, the track will not be continued.</p> <p>This parameter is optional and defaults to <code>51</code>.</p>"},{"location":"OTVision/usage/track/#overwrite","title":"overwrite","text":"<p><code>--overwrite</code> to overwrite existing <code>.ottrk</code> files.</p> <p><code>--no-overwrite</code> to prevent overwriting existing <code>.ottrk</code> files.</p> <p>This parameter is optional and defaults to <code>--overwrite</code>.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/verkehrszaehlgeraet/","title":"Verkehrsz\u00e4hlger\u00e4t: Welches f\u00fcr eine erfolgreiche Verkehrsplanung kaufen?","text":"<p>Ein Verkehrsz\u00e4hlger\u00e4t liefert die essenzielle Datengrundlage f\u00fcr eine funktionierende Verkehrs-, Raum- und Stadtplanung. Nur wenn belastbare Informationen \u00fcber Verkehrsstr\u00f6me, Fahrzeugarten und Nutzungsverhalten vorliegen, k\u00f6nnen Kommunen und Planungsb\u00fcros fundierte Entscheidungen treffen.</p> <p>Auch platomo stand vor einigen Jahren vor der Herausforderung, die geeignete Technik f\u00fcr Verkehrsz\u00e4hlungen zu finden. Die Erkenntnis: Viele Systeme sind entweder unflexibel, teuer oder datenschutzrechtlich problematisch. Aus dieser Erfahrung heraus entstand OpenTrafficCam \u2013 mit dem Ziel, Verkehrsdaten einfacher, pr\u00e4ziser, rechtskonform und mit dem neusten Stand der Technik zu erfassen.</p> <p>Wir geben Ihnen einen \u00dcberblick, welche Arten von Verkehrsz\u00e4hlger\u00e4ten heute verf\u00fcgbar sind und worauf Sie als Kommune oder Ingenieurb\u00fcro bei der Auswahl achten sollten. Zudem erfahren Sie, warum moderne kamerabasierte Systeme inzwischen den entscheidenden Unterschied machen.</p>"},{"location":"blog/verkehrszaehlgeraet/#verkehrszahlung-warum-messgerate-unverzichtbar-sind","title":"Verkehrsz\u00e4hlung: Warum Messger\u00e4te unverzichtbar sind","text":"<p>Ein Verkehrsz\u00e4hlger\u00e4t stellt die Grundlage f\u00fcr fundierte Entscheidungen in der Verkehrs- und Stadtplanung her. Ohne pr\u00e4zise Verkehrsdaten stochern Verkehrs- und Stadtplaner oft im Nebel: Welcher Knotenpunkt braucht Entlastung? Wo gibt es Engp\u00e4sse im \u00f6ffentlichen Parkraum? Wo lohnt sich der Ausbau von Radwegen?</p> <p>Ger\u00e4te zur Verkehrsz\u00e4hlung liefern gleich f\u00fcr mehrere Anwendungsfelder belastbare Daten:</p> <ul> <li>Planungsgrundlage: Verkehrsz\u00e4hlger\u00e4te liefern objektive Daten f\u00fcr eine   vorausschauende   und strategische Verkehrs- und Stadtplanung \u2013 wenn es zum Beispiel um den Ausbau von   Stra\u00dfen, die Steuerung von Lichtsignalanlagen oder um Ma\u00dfnahmen zur Verkehrsberuhigung   geht.</li> <li>Entscheidungsbasis: Sie zeigen, wo Investitionen in Stra\u00dfen, Radwege oder den   \u00f6ffentlichen Nahverkehr tats\u00e4chlich und als erstes notwendig sind.</li> <li>Effizienzsteigerung: Ein Verkehrsz\u00e4hlger\u00e4t erm\u00f6glicht datenbasierte Analysen, die   bei   der Verkehrsflussoptimierung, Stauvermeidung und Freigabezeitverteilung an Ampeln   unterst\u00fctzen.</li> <li>Nachhaltigkeit: Die erfassten Kennzahlen unterst\u00fctzen Kommunen dabei, den Fu\u00df- und   Radverkehr gezielt zu f\u00f6rdern, CO\u2082-Emissionen zu senken und vorhandene Infrastruktur   besser auszunutzen.</li> <li>Ma\u00dfnahmenbewertung: Daten aus Verkehrsz\u00e4hlger\u00e4ten erm\u00f6glichen die Bewertung   baulicher   oder verkehrslenkender Ma\u00dfnahmen und Verkehrsversuche. So lassen sich zuk\u00fcnftige   Projekte auf Basis realer Daten verbessern.</li> <li>Grundlage f\u00fcr Forschung &amp; Modellierung: Die mit dem Verkehrsz\u00e4hlger\u00e4t gewonnenen   Daten   bilden die Basis f\u00fcr Verkehrsmodelle, Simulationen und Prognosen \u2013 und damit f\u00fcr eine   nachhaltige Mobilit\u00e4tsplanung.</li> </ul>"},{"location":"blog/verkehrszaehlgeraet/#welche-arten-von-verkehrszahlgeraten-gibt-es","title":"Welche Arten von Verkehrsz\u00e4hlger\u00e4ten gibt es?","text":"<p>In der Praxis taucht f\u00fcr automatische Messger\u00e4te h\u00e4ufig auch der Begriff \u201eVerkehrsz\u00e4hler\u201c auf. Fachlich korrekt handelt es sich jedoch um Verkehrsz\u00e4hlger\u00e4te oder automatische Z\u00e4hlsysteme. Die folgenden Technologien z\u00e4hlen zu den g\u00e4ngigen Varianten. Sie unterscheiden sich in Erfassungsprinzip, Genauigkeit, Flexibilit\u00e4t und Kosten:</p> <ul> <li>Induktionsschleifen: Induktionsschleifen erkennen metallische Fahrzeuge und werden   zur kontinuierlichen   Verkehrsz\u00e4hlung eingesetzt. Sie k\u00f6nnen, je nach Installation und Anwendungszweck,   pr\u00e4zise sein und sind   vergleichsweise aufw\u00e4ndig zu installieren. Die Erfassung verschiedener   Verkehrsteilnehmer wie Fahrr\u00e4der oder   Fu\u00dfg\u00e4nger ist mit einem solchen Verkehrsz\u00e4hlger\u00e4t aber nur sehr eingeschr\u00e4nkt oder gar   nicht m\u00f6glich. Auf   au\u00dfer\u00f6rtlichen Stra\u00dfen sind sie oft der Standard, ebenso wie zur Steuerung von Ampeln.   Der gr\u00f6\u00dfte Nachteil ist die   feste Installation im Asphalt.</li> <li>Radar- und Mikrowellensensoren (z. B. Seitenradar): Sie messen Bewegungen mittels   elektromagnetischer Wellen und eignen sich besonders gut f\u00fcr Kfz-Z\u00e4hlungen.   Seitenradarsysteme   werden in vielen tempor\u00e4ren Z\u00e4hlprojekten eingesetzt, k\u00f6nnen jedoch in   innerst\u00e4dtischen   Umgebungen durch Reflexionen zu Doppelz\u00e4hlungen f\u00fchren. In Kombination mit Mikrofonen   lassen   sich Fahrzeugtypen genauer klassifizieren. Das ist jedoch nicht der Standard. Die   Qualit\u00e4t der   Erfassung h\u00e4ngt stark vom jeweiligen Ger\u00e4tetyp ab. Radverkehr ist nur eingeschr\u00e4nkt   und   Fu\u00dfverkehr meist nicht erfassbar. Ein wichtiger Vorteil der Sensoren: Sie sind einfach   installierbar und weisen einen geringen Energieverbrauch auf.</li> <li>Infrarot- und Lichtschrankensysteme: Diese Systeme erkennen Unterbrechungen eines   Lichtstrahls und werden h\u00e4ufig   f\u00fcr die Z\u00e4hlung von Fu\u00dfg\u00e4ngern und Radfahrern eingesetzt. Sie sind allerdings anf\u00e4llig   f\u00fcr St\u00f6rungen durch   Witterungseinfl\u00fcsse oder Verschmutzung.</li> <li>Magnetfeldsensoren: Sie registrieren Ver\u00e4nderungen im Erdmagnetfeld, die durch   vorbeifahrende Fahrzeuge   entstehen. Magnetfeldsensoren sind wartungsarm, aber ausschlie\u00dflich f\u00fcr die Erfassung   von Kraftfahrzeugen geeignet.</li> <li>Akustische Sensoren: Diese Systeme analysieren die Ger\u00e4usche vorbeifahrender   Fahrzeuge. Aufgrund ihrer hohen   St\u00f6ranf\u00e4lligkeit \u2013 etwa durch Wind, Regen oder Umgebungsl\u00e4rm \u2013 nutzen Kommunen die   Verkehrsz\u00e4hlger\u00e4te jedoch nur   selten.</li> <li>Thermalsensoren / Infrarot: Die Sensoren ermitteln W\u00e4rmeunterschiede und k\u00f6nnen so   Personen und Kraftfahrzeuge   gut erkennen. Sie eignen sich prim\u00e4r f\u00fcr Dauerz\u00e4hlstellen und f\u00fcr Standorte mit   herausfordernden Lichtverh\u00e4ltnissen.   Im Prinzip funktionieren sie \u00e4hnlich wie Kameras, nur in einem anderen   Frequenzbereich, sodass \u00e4hnliche   Datenschutzfragen zu kl\u00e4ren sind. Die Sensoren sind verh\u00e4ltnism\u00e4\u00dfig teuer in der   Anschaffung und haben oft eine   geringe Klassifizierungsgenauigkeit. Bei hohen Umgebungstemperaturen k\u00f6nnen Fehler   entstehen.</li> <li>Kamerabasierte Systeme: Sie erfassen Verkehrsszenen visuell und liefern mithilfe   angeschlossener Software   detaillierte Informationen \u00fcber Verkehrsfluss, Fahrzeugklassen und Bewegungsmuster.   Kamerabasierte Messger\u00e4te zur   Verkehrsz\u00e4hlung sind schon lange die flexibelste und datenreichste L\u00f6sung. Ihr   besonderer Vorteil: Kommunen k\u00f6nnen   aufgezeichnete Situationen im Nachhinein erneut ansehen, sodass sie bei Bedarf manuell   nachz\u00e4hlen oder kontrollieren   k\u00f6nnen. Dieses Feature bietet keine der anderen Technologien. Dabei ist zu   unterscheiden in Kameras, die nur die   Videos vor Ort aufzeichnen und diese dann an anderer Stelle verarbeitet werden, und   \u201eEdge-Kameras\u201d, die noch vor Ort   das Material auswerten. Letztere sind jedoch deutlich klobiger und weisen eine k\u00fcrzere   Akkulaufzeit auf. Und:   Handelt es sich um Edge-Kameras ohne Speicher, k\u00f6nnen Planungsteams die Aufnahmen   nachtr\u00e4glich nicht nochmal pr\u00fcfen.</li> <li>Lidar (2D/3D-Laserscanner): Lidar erfasst die Umgebung mittels Laserimpuls und   erzeugt Punktwolken von   Fahrzeugen. Sie sind besonders geeignet f\u00fcr komplexe Verkehrssituationen und k\u00f6nnen   meist auch Fu\u00dfverkehr gut und   Radverkehr eingeschr\u00e4nkt erfassen. Der gro\u00dfe Vorteil ist, dass Lidar unabh\u00e4ngig von   der Beleuchtungssituation   funktioniert. Allerdings sind die Ger\u00e4te sehr teuer. Zudem entstehen durch die   3D-Abbildung von Personen und Gesichtern   oft Datenschutzprobleme, an die man nicht auf Anhieb denkt. Die Datenverarbeitung ist   au\u00dferdem sehr anspruchsvoll. In   der Praxis sind die Laserscanner daher wenig relevant.</li> </ul>"},{"location":"blog/verkehrszaehlgeraet/#verkehrszahlgerat-kaufen-worauf-kommunen-und-planungsburos-achten-sollten","title":"Verkehrsz\u00e4hlger\u00e4t kaufen: Worauf Kommunen und Planungsb\u00fcros achten sollten","text":"<p>Bevor Sie ein Verkehrsz\u00e4hlger\u00e4t kaufen, sollten Sie sich dar\u00fcber klar werden, welche Art von Daten Sie ben\u00f6tigen. Einige Systeme liefern lediglich Z\u00e4hldaten, andere erm\u00f6glichen durch Videoaufzeichnung und automatisierte Auswertung deutlich tiefere Einblicke in das Verkehrsgeschehen. Je klarer Sie Ihre Anforderungen vorab definieren, desto leichter finden Sie das f\u00fcr Sie passende Messger\u00e4t f\u00fcr die Verkehrsz\u00e4hlung.</p>"},{"location":"blog/verkehrszaehlgeraet/#einsatzzweck-messziel","title":"Einsatzzweck &amp; Messziel","text":"<p>Entscheidend ist, ob Sie f\u00fcr die Verkehrsz\u00e4hlung mit dem Ger\u00e4t nur Fahrzeugmengen ( Verkehrsst\u00e4rken) des Kfz-Verkehrs erfassen oder ob Sie zus\u00e4tzlich Bewegungsmuster, weitere Verkehrsteilnehmerklassen und gesamte Verkehrsbewegungen analysieren m\u00f6chten \u2013 inklusive sogenannter \u201eschwacher\u201c Verkehrsteilnehmender wie Radfahrer oder Fu\u00dfg\u00e4nger. Selbst wenn einer dieser Zwecke nur gelegentlich ben\u00f6tigt wird, lohnt sich die Nutzung flexibler Systeme.</p> <p>Klassische Ger\u00e4te liefern meist nur Z\u00e4hldaten, w\u00e4hrend videobasierte Systeme wie die OpenTrafficCam Aufzeichnung und KI-gest\u00fctzte Auswertung vereinen und damit auch komplexere Anwendungsf\u00e4lle bedienen k\u00f6nnen.</p> <p>Unabh\u00e4ngig davon, ob die Auswertung direkt in der Kamera (edge) oder \u00fcber eine externe Software im B\u00fcro erfolgt \u2013 beide Varianten k\u00f6nnen datenschutzkonform umgesetzt werden. Und gleichzeitig gilt: Bei beiden Ans\u00e4tzen k\u00f6nnen erhebliche Datenschutzrisiken entstehen, wenn Prozesse oder Konfigurationen nicht sauber abgestimmt sind.</p> <p>Datenschutz &amp; Auswertung</p> <p>Genau hier setzt platomo an: Gemeinsam mit spezialisierten Fachkanzleien pr\u00fcfen wir die datenschutzrechtlichen Anforderungen und geben Kommunen sowie Planungsb\u00fcros klare, rechtssichere Leitlinien an die Hand \u2013 sowohl f\u00fcr den Einsatz unserer Kameras als auch f\u00fcr die korrekte, DSGVO-konforme Auswertung des aufgenommenen Materials. Alternativ \u00fcbernehmen wir die datenschutzkonforme Auswertung auch vollst\u00e4ndig f\u00fcr Sie \u2013 inklusive KI-basierter Videoverarbeitung und verkehrsanalytischer Auswertung.</p>"},{"location":"blog/verkehrszaehlgeraet/#dauer-ort-und-zweck-der-messung","title":"Dauer, Ort und Zweck der Messung","text":"<p>Bei Dauerz\u00e4hlstellen kommen eher fest installierte Systeme wie Induktionsschleifen zum Einsatz. F\u00fcr den Leistungsf\u00e4higkeitsnachweis nach HBS ben\u00f6tigen St\u00e4dte und Kommunen eine tempor\u00e4re Messung an der spezifischen Infrastruktur. Sofern dort keine Dauerz\u00e4hlstellen vorhanden sind, m\u00fcssen sie tempor\u00e4re Messungen mit mobilen Systemen durchf\u00fchren. Baustellen, Verkehrsstudien oder Verkehrsversuche erfordern f\u00fcr die Verkehrsmessung daher vor allem einfach montierbare Ger\u00e4te.</p>"},{"location":"blog/verkehrszaehlgeraet/#art-der-verkehrsteilnehmer","title":"Art der Verkehrsteilnehmer","text":"<p>Nicht jedes Verkehrsz\u00e4hlger\u00e4t erfasst alle Verkehrskategorien und Fahrzeugtypen. Seitenradarger\u00e4te z\u00e4hlen z. B. in der Regel nur Kfz, w\u00e4hrend Kameras oder Infrarot-Systeme auch Rad- und Fu\u00dfverkehr erfassen k\u00f6nnen.</p>"},{"location":"blog/verkehrszaehlgeraet/#genauigkeit-datentiefe","title":"Genauigkeit &amp; Datentiefe","text":"<p>Abh\u00e4ngig vom Planungsziel ben\u00f6tigen Kommunen und Ingenieurb\u00fcros unterschiedliche Genauigkeitsstufen f\u00fcr die Verkehrsz\u00e4hlung. Diese reichen von reiner Messung von Frequenz und Verkehrsst\u00e4rke \u00fcber strom- und klassenfeine Z\u00e4hldaten bis hin zu detaillierten Bewegungsprofilen und Analysen des Verkehrsflusses. Videosysteme haben den Vorteil, dass sie die Gesamtsituation erfassen. Zudem k\u00f6nnen Teams in der Stadt- und Verkehrsplanung bei Systemen, die nicht direkt auf der Kamera das Bild verarbeiten, Spezialauswertungen erg\u00e4nzen, wenn das Softwaresystem gen\u00fcgend Flexibilit\u00e4t bietet.</p>"},{"location":"blog/verkehrszaehlgeraet/#datenauswertung-schnittstellen","title":"Datenauswertung &amp; Schnittstellen","text":"<p>Wie sollen die Daten der Verkehrsz\u00e4hlung aus den Messger\u00e4ten \u00fcbertragen, bereitgestellt und weiterverarbeitet werden? Per USB-Stick, lokalem Speicher oder automatischer \u00dcbertragung in Analyse-Tools? Muss sp\u00e4ter eine Qualit\u00e4ts\u00fcberpr\u00fcfung m\u00f6glich sein oder vertraut man den erzeugten Daten immer und vollst\u00e4ndig? Ben\u00f6tigt man weitergehende Auswertungen oder reichen die Rohdaten? Gut zu wissen: Systeme mit offenen Schnittstellen sparen langfristig Zeit und Aufwand.</p> <p>Bei unserer OpenTrafficCam haben Sie zwei Optionen: Entweder wir \u00fcbernehmen die Auswertung vollst\u00e4ndig f\u00fcr Sie, oder Sie f\u00fchren sie selbst durch \u2013 mit unserer Analyse-Software OTAnalytics, die eine flexible, projektbezogene Aufbereitung erm\u00f6glicht. Wenn Sie besonders detailliert arbeiten m\u00f6chten, k\u00f6nnen Sie zudem die erfassten Trajektorien direkt als CSV-Datei exportieren und somit eigene Auswertungslogiken anwenden.</p>"},{"location":"blog/verkehrszaehlgeraet/#datenschutz-rechtskonformitat","title":"Datenschutz &amp; Rechtskonformit\u00e4t","text":"<p>Gerade f\u00fcr \u00f6ffentliche Auftraggeber ist eine DSGVO-konforme Datenspeicherung und -verarbeitung unverzichtbar. Dazu sind eine Anonymisierung und die Kontrolle \u00fcber Datenfl\u00fcsse Pflicht. Eine datenschutzkonforme Kamera erf\u00fcllt beispielsweise mit geringer Aufl\u00f6sung und absichtlicher Defokussierung die DSGVO-Vorgaben. Verwenden Verkehrsbeh\u00f6rden und Planungsstellen KI-Modelle zur Auswertung, m\u00fcssen diese speziell auf das Videomaterial trainiert werden.</p>"},{"location":"blog/verkehrszaehlgeraet/#wartung-energieversorgung-betriebskosten","title":"Wartung, Energieversorgung &amp; Betriebskosten","text":"<p>Qualit\u00e4tsbewusste Verkehrsz\u00e4hlger\u00e4te-Hersteller achten auf eine robuste Hardware und eine einfache Energieversorgung. Das reduziert den Wartungsaufwand und senkt so laufende Kosten. Dabei schaffen offene Systeme Unabh\u00e4ngigkeit und Souver\u00e4nit\u00e4t vom Hersteller und sichern eine langfristige Einsetzbarkeit und kontrollierbare Folgekosten.</p>"},{"location":"blog/verkehrszaehlgeraet/#wirtschaftlichkeit-uber-den-projektzyklus","title":"Wirtschaftlichkeit \u00fcber den Projektzyklus","text":"<p>Wenn Sie ein Verkehrsz\u00e4hlger\u00e4t kaufen, sollten Sie neben den Preisen auch die laufenden Ausgaben f\u00fcr Betrieb, Datenaufbereitung und Personal im Auge haben. G\u00fcnstige Hardware, die regelm\u00e4\u00dfig manuell betreut werden muss, verursacht \u00fcber die Projektlaufzeit oft h\u00f6here Kosten als erwartet. Deshalb setzen viele Stadt- und Verkehrsplanungsb\u00fcros auf eine externe Auswertung: Fachdienstleister verf\u00fcgen \u00fcber die technische Infrastruktur, die n\u00f6tige Erfahrung im Umgang mit sensiblen Verkehrsdaten und eingespielte Qualit\u00e4tsprozesse. Das reduziert internen Aufwand, vermeidet Fehler und sorgt daf\u00fcr, dass auch komplexe Fragestellungen pr\u00e4zise beantwortet werden k\u00f6nnen \u2013 wirtschaftlich, zuverl\u00e4ssig und ohne zus\u00e4tzlichen Personalbedarf.</p>"},{"location":"blog/verkehrszaehlgeraet/#zukunftssicherheit-erweiterbarkeit","title":"Zukunftssicherheit &amp; Erweiterbarkeit","text":"<p>Messger\u00e4te zur Verkehrsz\u00e4hlung mit modularer Software oder KI-Updates sind besonders zukunftsf\u00e4hig: Sie k\u00f6nnen auch neue Verkehrstrends wie E-Scooter oder sp\u00e4ter entwickelte Fahrzeugtypen zuverl\u00e4ssig erfassen. Au\u00dferdem lassen sich so auch im Nachgang weitere Analysen f\u00fcr neue Zwecke erstellen (wie Parkraumerhebungen oder Sicherheitsanalysen).</p>"},{"location":"blog/verkehrszaehlgeraet/#mit-einem-verkehrszahlgerat-den-verkehr-verstehen-die-planung-optimieren","title":"Mit einem Verkehrsz\u00e4hlger\u00e4t den Verkehr verstehen &amp; die Planung optimieren","text":"<p>Ein Verkehrsz\u00e4hlger\u00e4t ist viel mehr als ein simples Z\u00e4hlinstrument. Es ist ein elementarer Pfeiler jeder intelligenten Verkehrsplanung. Es liefert genau die aussagekr\u00e4ftigen Daten, die Kommunen, Planungsb\u00fcros und Ingenieure brauchen, um Verkehr effizient, sicher und nachhaltig zu gestalten.</p> <p>Besonders moderne Kameras zur Verkehrsz\u00e4hlung wie die OTCamera von OpenTrafficCam erm\u00f6glichen eine pr\u00e4zise Erfassung, rechtskonforme Datenverarbeitung und flexible digitale Auswertung in einer offenen und damit zukunftssicheren L\u00f6sung. Sie k\u00f6nnen so s\u00e4mtliche Verkehrsteilnehmer erfassen, Bewegungsmuster analysieren und Planungen optimal umsetzen. Dabei behalten Sie vollst\u00e4ndige Kontrolle \u00fcber Ihre Daten und Prozesse \u2013 und gewinnen zugleich die Flexibilit\u00e4t, Auswertungen so umzusetzen, wie es Ihr Projekt erfordert.</p> <p>Sie m\u00f6chten verstehen, welche Verkehrsz\u00e4hlger\u00e4t-Funktionen Ihre Projekte effizienter, sicherer und datenbasierter machen k\u00f6nnen? Wir beraten Sie gern, wie Sie unsere moderne Z\u00e4hltechnik made in Germany gezielt einsetzen \u2013 von der Planung bis zur Analyse und Auswertung. Kontaktieren Sie uns jetzt f\u00fcr eine individuelle Beratung!</p>"},{"location":"contribute/","title":"Getting Started","text":"<p>Become a part of OpenTrafficCam</p> <p>We are happy if you contribute your own code to OpenTrafficCam. This can be bugfixes in the current code, or even completely new stuff. It is best to start at GitHub to get a first overview.</p> <p>We are also happy to receive reports about the experience using OpenTrafficCam. If you find a bug in the software or the documentation, please report it as an issue in the corresponding repository on GitHub or via email.</p> <p>From time to time we are looking to expand the core team behind OpenTrafficCam. So if you are an enthusiastic developer or engineer who has programming experience and would like to work at the intersection of mobility analysis and planning, software and hardware, feel free to get in touch :)</p> <p>If you have any questions, feel free to send us an email.</p>"},{"location":"contribute/coding/","title":"Coding (Python)","text":"<p>Coding standards are not about right and wrong or good and bad. They are about uniformity, so people can easily read and contribute to the code regardless who worked on the software before.</p> <p>For now, nearly all code is Python so we just agreed on coding conventions for the Python programming language. We basically follow the PEP 8 Style Guide for Python Code. A good summary and some additions can be found in the Hitchhikers Guide or at RealPython.com. Below we defined what we consider the most important standards.</p>"},{"location":"contribute/coding/#naming-conventions","title":"Naming Conventions","text":"<p>All names should be as short as possible but as long as necessary to understand them.</p>"},{"location":"contribute/coding/#general","title":"General","text":"<p>The following table from RealPython.com summarizes the PEP8 naming conventions:</p> Type Naming Convention Examples Function Use a lowercase word or words. Separate words by underscores to improve readability. <code>function</code>, <code>my_function</code> Variable Use a lowercase single letter, word, or words. Separate words with underscores to improve readability. <code>x</code>, <code>var</code>, <code>my_variable</code> Class Start each word with a capital letter. Do not separate words with underscores. This style is called camelCase. <code>Model</code>, <code>MyClass</code> Method Use a lowercase word or words. Separate words with underscores to improve readability. <code>class_method</code>, <code>method</code> Constant Use an uppercase single letter, word, or words. Separate words with underscores to improve readability. <code>CONSTANT</code>, <code>MY_CONSTANT</code>, <code>MY_LONG_CONSTANT</code> Module Use a short, lowercase word or words. Separate words with underscores to improve readability. <code>module.py</code>, <code>my_module.py</code> Package Use a short, lowercase word or words. Do not separate words with underscores. <code>package</code>, <code>mypackage</code>"},{"location":"contribute/coding/#files-folder-dirs","title":"Files, Folder, Dirs","text":"Naming Convention Examples dir D:\\tmp\\ filename testfile suffix _detections file type .csv file D:\\tmp\\testfile_detections.csv path \"D:\\tmp\\\" or \"D:\\tmp\\testfile_detections.csv\""},{"location":"contribute/coding/#file-extensions","title":"File extensions","text":"<p>Files get an extension according to their content.</p> Extension Description .otdet detections as bounding boxes .ottrk trajectories in pixel (and UTM) coordinates .otrfpts reference points to convert pixel to UTM coordinates"},{"location":"contribute/coding/#code-documentation","title":"Code documentation","text":"<p>Each module, function, class or method should be described in a docstring (Google style)</p>"},{"location":"contribute/coding/#docstrings-for-modules","title":"Docstrings for Modules","text":"<p>Each file should start with the license snippet followed by a docstring describing the contents and usage of the module:</p> <pre><code>\"\"\"A one line summary of the module or program, terminated by a period.\n\nLeave one blank line.  The rest of this docstring should contain an\noverall description of the module or program.  Optionally, it may also\ncontain a brief description of exported classes and functions and/or usage\nexamples.\n\n  Typical usage example:\n\n  foo = ClassFoo()\n  bar = foo.FunctionBar()\n\"\"\"\n</code></pre>"},{"location":"contribute/coding/#docstrings-for-functions","title":"Docstrings for Functions","text":"<p>Each function must be described by a docstring:</p> <pre><code>def hello_world(message: str = \"Hello World\", author: str = \"Santa Claus\"):\n    \"\"\"Deliver a message from an author to the world.\n\n    \"author says message\"\n\n    Args:\n        message (str, optional): Message to deliver. Defaults to \"Hello World\".\n        author (str, optional): Author name. Defaults to \"Santa Claus\".\n\n    Returns:\n        str: the message said\n    \"\"\"\n    msg = author + \" says \" + message\n    print(msg)\n\n    return msg\n</code></pre> <p>Note</p> <p>If you are using Visual Studio Code, you may want to use the Python Docstring Generator extension.</p>"},{"location":"contribute/coding/#comments","title":"Comments","text":"<p>If it is necessary to explain your code in between, use single line or block comments for some (or all) code that follows:</p> <pre><code># This is an example for a single line comment\n\n# This is an\n# example for a\n# block comment\n</code></pre> <p>Try to avoid inline comments.</p>"},{"location":"contribute/coding/#dependencies","title":"Dependencies","text":"<p>We try to develop OpenTrafficCam using as few dependencies as possible. For basic functionality we prefer packages that come with the Python standard distribution (like pathlib or Tkinter). However, OpenTrafficCam would not be possible without code from third party libraries, especially when it comes to videos, images, data analysis, neural networks or web development. These are the libraries we intend to use for specific functionalities throughout the whole framework (we are grateful to the authors):</p> Functionality Library Paths pathlib (distributed with Python since 3.4) Videos, Images OpenCV Neural networks PyTorch, torchvision Data handling pandas, NumPy Shape handling GeoPandas, Shapely Plotting, graphing Plotly User interface Tkinter Web Dash (dashboards)"},{"location":"contribute/coding/#lintingautoformatting","title":"Linting/Autoformatting","text":"<p>To match PEP8 specs and write pretty code we use flake8 to check against structural and style errors and black for autoformatting. Additionally the package flake8-docstrings can be used to lint the docstrings.</p> <p>The following settings are set in the <code>.flake8</code> file:</p> <pre><code>[flake8]\nmax-line-length = 88\ndocstring-convention=google\n</code></pre>"},{"location":"contribute/documentation/","title":"Documentation","text":"<p>This documentation of OpenTrafficCam is generated by MkDocs using the awesome material theme. The documentation is fully written in Markdown.</p> <p>You can help improving and extending this documentation creating a fork and a pull request on GitHub. After review and discussion (if necessary) we will merge your pull request into the documentation repository and then it will be also visible on the site.</p> <p>Have a look at the README file of the documentation repository for more information. There you will also find a description of how this site is rendered and how inserting a new page into the navigation works.</p> <p>If you are not willing to contribute directly, you can also write us about your suggestions: team@opentrafficcam.org</p>"},{"location":"contribute/github/","title":"GitHub","text":"<p>All code of OpenTrafficCam is hosted on GitHub. For each core module a separate repository exists:</p> <ul> <li>OTCamera</li> <li>OTVision</li> <li>OTAnalytics</li> </ul> <p>Following the Feature Branch Workflow each repository has a master branch, which at all times should contain a stable version of the software. All coding should be committed to separate feature branches. Once you completed coding on a feature, issue or bugfix, you create a pull request for the respective feature branch (preferably labeled using the GitHub labeling system). Your contributed code will then be reviewed by the main developers, discussed (if necessary) and merged into the master branch.</p> <p>Even without coding you can contribute to the project: If you encounter any bug or got any idea for enhancement, feel free to open an issue in the corresponding repository.</p> <p>All pull requests will be checked by GitHub's super-linter.</p>"},{"location":"contribute/otanalytics/","title":"OTAnalytics","text":""},{"location":"contribute/otanalytics/#development-environment-setup","title":"Development Environment Setup","text":"<p>OTAnalytics uses uv package manager for dependency management. This provides faster installs, better dependency resolution, and reproducible environments.</p>"},{"location":"contribute/otanalytics/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.12</li> <li>uv package manager</li> </ul>"},{"location":"contribute/otanalytics/#quick-setup","title":"Quick Setup","text":"<ol> <li>Clone the repository:</li> </ol> <pre><code>git clone https://github.com/OpenTrafficCam/OTAnalytics.git\ncd OTAnalytics\n</code></pre> <ol> <li>Set up a development environment:</li> </ol> <p>Linux/macOS:</p> <pre><code>./install_dev.sh\n</code></pre> <p>Windows:</p> <pre><code>install_dev.cmd\n</code></pre> <p>This will:</p> <ul> <li>Install the project and all dependencies using uv</li> <li>Set up development dependencies (testing, linting, formatting tools)</li> <li>Install pre-commit hooks</li> </ul>"},{"location":"contribute/otanalytics/#uv-workflow","title":"uv Workflow","text":""},{"location":"contribute/otanalytics/#installing-dependencies","title":"Installing Dependencies","text":"<ul> <li>Production installation: <code>uv sync</code></li> <li>Development installation: <code>uv sync --extra dev</code></li> <li>Adding new dependencies: Edit <code>pyproject.toml</code> and run <code>uv sync</code></li> </ul>"},{"location":"contribute/otanalytics/#running-commands","title":"Running Commands","text":"<ul> <li>Run Python scripts: <code>uv run python script.py</code></li> <li>Run tests: <code>uv run pytest</code></li> <li>Run application: <code>uv run otanalytics</code></li> </ul>"},{"location":"contribute/otanalytics/#managing-dependencies","title":"Managing Dependencies","text":"<p>Dependencies are defined in <code>pyproject.toml</code>:</p> <ul> <li>Main dependencies in <code>[project.dependencies]</code></li> <li>Development dependencies in <code>[project.optional-dependencies.dev]</code></li> </ul> <p>The <code>uv.lock</code> file ensures reproducible builds and should be committed to the repository.</p>"},{"location":"contribute/otanalytics/#development-tools","title":"Development Tools","text":"<p>All development tools are installed automatically with the dev dependencies:</p> <ul> <li>Code formatting: <code>uv run black .</code></li> <li>Import sorting: <code>uv run isort .</code></li> <li>Type checking: <code>uv run mypy OTAnalytics</code></li> <li>Linting: <code>uv run flake8 OTAnalytics</code></li> <li>Testing: <code>uv run pytest</code></li> </ul>"},{"location":"contribute/otanalytics/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Pre-commit hooks are automatically installed and will run:</p> <ul> <li>Black (code formatting)</li> <li>isort (import sorting)</li> <li>Flake8 (linting)</li> <li>MyPy (type checking)</li> </ul>"},{"location":"contribute/otanalytics/#migration-from-piprequirementstxt","title":"Migration from pip/requirements.txt","text":"<p>If you're migrating from the old pip-based setup:</p> <ol> <li>Remove old virtual environment: <code>rm -rf venv</code> or <code>rmdir /s venv</code></li> <li>Run the new installation scripts as described above</li> <li>uv will create a new <code>.venv</code> directory and <code>uv.lock</code> file</li> </ol>"},{"location":"contribute/otanalytics/#cli-commands","title":"CLI Commands","text":"<p>The project provides a <code>otanalytics</code> CLI command when installed:</p> <pre><code>uv run otanalytics  # Run the application\n</code></pre>"},{"location":"contribute/otanalytics/#troubleshooting","title":"Troubleshooting","text":"<p>uv not found: Install uv</p> <p>Lockfile issues: Regenerate with <code>uv lock --upgrade</code></p> <p>Dependency conflicts: Use <code>uv tree</code> to inspect dependency graph</p> <p>Pre-commit issues: Reinstall hooks with <code>uv run pre-commit install --install-hooks</code></p>"},{"location":"contribute/vscode/","title":"VS Code","text":"<p>We are developing OpenTrafficCam using Visual Studio Code using the following extensions:</p> <ul> <li>GitHub Pull Requests and Issues</li> <li>markdownlint</li> <li>Pylance</li> <li>Python</li> <li>Python Docstring Generator</li> <li>yaml Language Support</li> </ul>"},{"location":"contribute/vscode/#pylance-settings","title":"Pylance Settings","text":"<p>To solve import errors in the repository, we need to add an extra path to the pylance config.</p> <p>For example in the workspace settings of OTCamera add:</p> <pre><code>\"python.analysis.extraPaths\": [\n  \"./OTCamera\"\n]\n</code></pre> <p>Or set it using the Settings-UI (Workspace -&gt; Pylance -&gt; Python - Analysis: Extra Path -&gt; add './OTCamera')</p>"},{"location":"contribute/vscode/#snippets","title":"Snippets","text":"<p>To comfortably insert code use snippets. You can open the configuration file for a specific language to add or modify the user snippets: <code>Ctrl</code> + <code>Shift</code> + <code>p</code> and <code>Preferences: Configure User Snippets</code>.</p> <p>You can insert the user snippets by start typing the \"prefix\" value. For example start typing <code>gpl_</code> and autocompletection should recognize the user snippets stated below.</p>"},{"location":"contribute/vscode/#gpl-license-information","title":"GPL License Information","text":"<p>Add the following snippet to <code>vscodedata/user/snippets/python.json</code></p> <pre><code>\"gpl_license_header\": {\n        \"prefix\": \"gpl_add_head\",\n        \"body\":[\n            \"$LINE_COMMENT ${1:Program Name and Function}\",\n            \"$LINE_COMMENT Copyright (C) $CURRENT_YEAR OpenTrafficCam Contributors\",\n            \"$LINE_COMMENT &lt;https://github.com/OpenTrafficCam\",\n            \"$LINE_COMMENT &lt;team@opentrafficcam.org&gt;\",\n            \"$LINE_COMMENT\",\n            \"$LINE_COMMENT This program is free software: you can redistribute it and/or modify\",\n            \"$LINE_COMMENT it under the terms of the GNU General Public License as published by\",\n            \"$LINE_COMMENT the Free Software Foundation, either version 3 of the License, or\",\n            \"$LINE_COMMENT (at your option) any later version.\",\n            \"$LINE_COMMENT\",\n            \"$LINE_COMMENT This program is distributed in the hope that it will be useful,\",\n            \"$LINE_COMMENT but WITHOUT ANY WARRANTY; without even the implied warranty of\",\n            \"$LINE_COMMENT MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\",\n            \"$LINE_COMMENT GNU General Public License for more details.\",\n            \"$LINE_COMMENT\",\n            \"$LINE_COMMENT You should have received a copy of the GNU General Public License\",\n            \"$LINE_COMMENT along with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\",\n            \"\",\n        ],\n        \"description\": \"Add GPLv3 license information in source code.\"\n    }\n</code></pre> <p>Add a short description and use tab to jump to the end of the snippet.</p>"},{"location":"contribute/vscode/#mkdocs-configuration-support","title":"MKDocs Configuration Support","text":"<p>In order to minimize friction and maximize productivity, Material for MkDocs provides its own schema.json1 for mkdocs.yml. If your editor supports YAML schema validation, it's definitely recommended to set it up.</p> <p>Source</p>"},{"location":"overview/","title":"Welcome to OpenTrafficCam","text":"<p>You are looking for a tool that makes analyzing traffic easier, faster and more efficient?</p> <p>Welcome to the documentation of OpenTrafficCam - the only fully integrated open source workflow for video-based recording and automated analysis of road traffic.</p> <p>On this site you will find all the information to set up OpenTrafficCam.</p> <p>You want to know more? Check out our GitHub page for downloads and codes.</p>"},{"location":"overview/#motivation","title":"Motivation","text":"<p>Thanks to the rapid development of computer hardware and machine learning in the last decade, automatic object detection is state of the art in many business sectors. However, most of the time road traffic is still surveyed manually to a great expense. There are tools that optimize some of the processing steps, but they can hardly be used without programming skills, or they cause significant costs. This is why research and planning still have to be conducted with minimal or sometimes without any data at all. This lack of data primarily affects sustainable modes of transportation such as walking and cycling.</p> <p>OpenTrafficCam aims to address this gap for a stronger data-base for transportation planning, policy and research. In doing so, we ourselves benefit substantially from other open source projects. Consequently, we publish large parts of OpenTrafficCam as open source hardware and software under the GNU General Public License v3.0. This also helps in efficiently using limited public resources for road infrastructure planning and research and in avoiding substantial vendor locks. So all transport professionals and anyone else interested are very welcome to try OpenTrafficCam.</p> <p>You want to support?</p> <p>Developing, maintaining and organizing open source requires quite a lot of time and money. But fortunately, there are several ways to support us:</p> <ul> <li>Traffic engineers can help by spreading the word and submitting questions, bugs or feature ideas as issues     in the corresponding GitHub repositories of the modules     OTCamera,     OTVision,     OTAnalytics or     OTLabels.</li> <li>Researchers and developers can support by contributing code     or contacting us for scientific collaboration.</li> <li>Users, such as municipalities or engineering companies, can contact us to fund specific enhancements to     meet their use case and thus help all other users.</li> </ul>"},{"location":"overview/#how-it-works","title":"How it works","text":"<p>OpenTrafficCam consists of multiple modules. The core is composed of three:</p> <ol> <li>The OTCamera hardware to record videos,</li> <li>OTVision, a collection of algorithms to generate trajectories    of objects (road users) based on the videos and</li> <li>OTAnalytics to gather traffic measures based on these trajectories.</li> </ol> <p>Each of the three modules comes with (easy) installation, a basic user interface and a documentation. The Open Source version covers the most common use cases of video-based traffic surveying, such as traffic counts.</p> <p>In addition to the three main modules, with OTLabels we will provide a set of labelled images of German road users and vehicles along with algorithms to train object detection models on custom data sets.</p> <p></p> <p>As described on the following pages of this documentation, some manual work, patience and basic skills are required to assemble OTCamera. And in order to regularly extract trajectories from videos using OTVision or train your own custom object detection models using OTLabels, you will need a powerful machine with a strong graphics card.</p> <p>Don\u00b4t want to deal with it? We got your back!</p> <p>We offer various services related to OpenTrafficCam:</p> <p> Selling and renting OTCameras</p> <p> Processing and analyzing your videos</p> <p> Setting up the OpenTrafficCam pipeline in your environment</p> <p> Onboarding and support to get the most out of OpenTrafficCam</p> <p> Conducting whole traffic surveys for you</p> <p> Developing features beyond the current open source version</p> <p> Consulting on traffic analysis, modeling and optimization</p> <p> Contact us for more information</p>"},{"location":"overview/dataprivacy/","title":"Data Privacy (Germany)","text":"<p>German law only</p> <p>The following FAQ on data protection are intentionally only available in German, as they refer exclusively to German law. They also do not apply to other countries with German as an official language (e.g. Austria, Switzerland).</p> <p>Die nachfolgenden FAQ zum Datenschutz sind mit Absicht nur in deutscher Sprache verf\u00fcgbar, da sie sich ausschlie\u00dflich auf deutsches Recht beziehen. Sie gelten daher auch nicht f\u00fcr andere L\u00e4nder mit Deutsch als Amtssprache (z. B. \u00d6sterreich, Schweiz).</p>"},{"location":"overview/dataprivacy/#vorwort","title":"Vorwort","text":"<p>In einem Workshop im Rahmen der durch das BMVI gef\u00f6rderten mFund-Machbarkeitsstudie zu OpenTrafficCam haben wir gemeinsam mit anderen Verkehrsfachleuten h\u00e4ufige Fragen zum Datenschutz bei der Videoerhebung des Stra\u00dfenverkehrs gesammelt. Diese Fragen sind nachfolgend aufgelistet.</p> <p>Die Beantwortung der folgenden Fragen erfolgte in mehreren Interviews mit dem Datenschutzbeauftragten der TU Dresden. Sie dienen als initiale Informationsquelle zur Einhaltung des Datenschutzes bei der Videoaufzeichnung in Forschungsprojekten. Sie ersetzen keine Rechtsberatung und k\u00f6nnen verst\u00e4ndlicherweise nicht jedes einzelne Verfahren sowie die rechtlichen Spezifika der Bundesl\u00e4nder ber\u00fccksichtigen.</p> <p>Deshalb ist es auf jeden Fall erforderlich, die/den jeweils zust\u00e4ndigen Datenschutzbeauftragten fr\u00fchzeitig in das konkrete Projekt einzubeziehen. Als ein Ergebnis der Gespr\u00e4che hat sich auch gezeigt, dass zur Kl\u00e4rung detaillierter und \u00fcber den Forschungsbereich hinausgehender Fragen des Datenschutzes bei der automatisierten, videobasierten Verkehrserfassung eine dezidierte Besch\u00e4ftigung einer spezialisierten Anwaltskanzlei zielf\u00fchrend sein kann. Zuk\u00fcnftig ist eine Erweiterung der FAQ um Inhalte zum Datenschutz bei der Videoerhebung in nichtwissenschaftlichen Projekten geplant.</p>"},{"location":"overview/dataprivacy/#faq-zum-datenschutz-bei-videobasierter-verkehrserfassung-in-forschungsprojekten","title":"FAQ zum Datenschutz bei videobasierter Verkehrserfassung in Forschungsprojekten","text":""},{"location":"overview/dataprivacy/#grundsatzliches","title":"Grunds\u00e4tzliches","text":"Wer ist in einem Projekt zust\u00e4ndig f\u00fcr den Datenschutz und tr\u00e4gt die Verantwortung? Die Auftragnehmenden oder die Auftraggebenden? <p>Bei Forschungsf\u00f6rderung: Die erhebende Institution ist verantwortlich. Hier ist der Forschungsparagraph des jeweiligen Landesdatenschutzgesetzes relevant, der relativ viele Freiheitsgrade beinhaltet. Dementsprechend ist die Freiheit von Forschung und Lehre besonders sch\u00fctzenswert und ist mit dem Recht auf Schutz personenbezogener Daten abzuw\u00e4gen. Grundsatz dabei ist immer die weitgehende Minimierung der Eingriffe in das Pers\u00f6nlichkeitsrecht.</p> <p>Bei Forschungsauftr\u00e4gen: Hier kommt es darauf an, in wessen Interesse die Datenerhebung liegt.</p> <p>Wenn die Forschungsmethodik durch die Auftraggebenden vorgegeben wird oder die Forschungsfrage nicht mit alternativen Erhebungsdesigns beantwortet werden kann (z.B. falls Videobeobachtungen in geringer Aufl\u00f6sung ohne die Erfassung personenbezogener Daten daf\u00fcr nicht ausreichen), liegt die Verantwortung aus rechtlicher Sicht allein beim Auftraggeber. Dies gilt auch, wenn die Auftragnehmenden die (zu diesem Zeitpunkt bereits anonymisierten) Daten \u00fcber den Projektgegenstand hinaus publizieren (es z\u00e4hlt der initiale Zweck der Datenerfassung). In diesem Fall sind die Auftraggebenden nicht nur verpflichtet, nach einem Datenschutzkonzept zu fragen. Sie sind auch dazu verpflichtet, Vorgaben f\u00fcr den Datenschutz in der Ausschreibung zu formulieren und Angebote darauf zu pr\u00fcfen, dass diese eigenen Vorgaben des Datenschutzes eingehalten werden.</p> <p>Wenn die Ausschreibung und die zu beantwortenden Forschungsfragen es zulassen, dass die Auftragnehmenden das Erhebungsdesign frei w\u00e4hlen, gilt die gemeinsame Verantwortlichkeit der Auftraggebenden und Auftragnehmenden gem\u00e4\u00df Art. 26 DSGVO (\"gemeinsames Interesse an der Datenauswertung\" / Gemeinsame Verantwortlichkeit).</p> Ist dar\u00fcber hinaus eine Instanz (z. B. Stra\u00dfenbaulasttr\u00e4ger) befugt, eine Messung aus Gr\u00fcnden des Datenschutzes zu verbieten? <p>Nein, weder der Stra\u00dfenbaulasttr\u00e4ger, noch eine Gebietsk\u00f6rperschaft oder der entsprechende Landesdatenschutzbeauftragte d\u00fcrfen die Datenerhebung auf \u00f6ffentlichen Fl\u00e4chen aus Gr\u00fcnden des Datenschutzes erlauben oder verbieten. Die Einhaltung des Datenschutzes liegt wie bereits beschrieben entweder in der Verantwortung der Forschungseinrichtung oder des Auftraggebers.</p> <p>Achtung!</p> <p>Neben dem Datenschutz k\u00f6nnten jedoch der Stra\u00dfenbaulasttr\u00e4ger bzw. die Gebietsk\u00f6rperschaft aufgrund ihrer Verkehrssicherungspflicht Einw\u00e4nde haben oder aufgrund der Montage von Kameras an Einbauten oder B\u00e4umen in deren Zust\u00e4ndigkeit.</p> Welche sind dabei die relevanten Rechtsvorschriften und wo/f\u00fcr wen gelten sie? Gibt es bereichsspezifische Regelungen? <p>Grunds\u00e4tzlich gilt zun\u00e4chst immer die europ\u00e4ische DSGVO, die jedoch gewisse Punkte offenl\u00e4sst, die wiederum von den Gesetzen nachgeordneter Gebietsk\u00f6rperschaften geregelt werden.</p> <p>F\u00fcr die Privatwirtschaft und Institutionen des Bundes gilt dabei spezialgesetzlich das BDSG.</p> <p>F\u00fcr \u00f6ffentliche Einrichtungen der Bundesl\u00e4nder (also die meisten Hochschulen), der Landkreise und kreisfreien St\u00e4dte gilt das jeweilige Landesdatenschutzgesetz, welches sich in der Regel bzgl. Forschung nicht stark vom BDSG unterscheidet.</p> <p>Der Geltungsbereich bereichsspezifischer Regelungen wird also durch die rechtliche Zuordnung der Institution zu einer Gebietsk\u00f6rperschaft bestimmt und nicht durch den Ort der Datenerhebung. Wenn mehrere Institutionen gemeinsam f\u00fcr den Datenschutz verantwortlich sind, gilt f\u00fcr jede das f\u00fcr sie relevante Gesetz bzw. die Gesetze.</p> Gibt es Unterschiede je nach Erhebungszweck bzw. Zweck des Projekts? <p>Es gibt Unterschiede zwischen Datenerhebungen zum Zweck der Forschung und anderen Zwecken wie Wirtschaft oder Verwaltung (z.B. Wahrung der \u00f6ffentlichen Sicherheit) sowie in der Verantwortung der Beteiligten. Ansonsten sind keine weiteren Unterschiede nach dem Erhebungszweck bekannt.</p> Wie \"dehnbar\" ist das Datenschutzrecht, wie unterschiedlich kann es ausgelegt werden? <p>Es existieren tats\u00e4chlich viele Grauzonen aufgrund der Nichtregulierung spezifischer Einzelf\u00e4lle (zu denen auch die videobasierte, automatisierte Verkehrserfassung f\u00fcr die Forschung und die Planung z\u00e4hlt), die im Falle rechtlicher Streitigkeiten separat bewertet werden m\u00fcssen.</p> <p>Wenn f\u00fcr rechtliche Fragestellungen keine dezidierten Rechtsnormen vorliegen, kann sich ein Gericht einerseits auf Pr\u00e4zedenzurteile berufen. Zur videobasierten Verkehrserfassung sind bisher jedoch nur wenige Urteile bekannt (siehe Frage \"Gibt es Pr\u00e4zedenz-Urteile zur videobasierten Verkehrserfassung?\").</p> <p>Andererseits ist eine rechtliche Anerkennung niedergeschriebener Branchenstandards m\u00f6glich, die in Zusammenarbeit zwischen den datenerhebenden Institutionen und den Aufsichtsbeh\u00f6rden erstellt wurden. Ein Beispiel daf\u00fcr ist der Arbeitskreis \"Datenschutz\" f\u00fcr Gesundheitsdaten, der in Abstimmung mit den Aufsichtsbeh\u00f6rden ein Rahmenkonzept entwickelt hat, auf das sich im Falle rechtlicher Streitigkeiten berufen werden kann. Zur Schaffung einer gewissen Rechtssicherheit bei den Details der videobasierten Verkehrserhebung f\u00fcr die Forschung und Planung kann ein \u00e4quivalentes Vorgehen sinnvoll sein.</p> Gibt es Pr\u00e4zedenz-Urteile zur videobasierten Verkehrserfassung? <p>Urteile zur Verkehrserfassung per Video zum Zweck der allgemeinen Verkehrsdatenerhebung f\u00fcr oder zur Unterst\u00fctzung von Forschung oder Verwaltungst\u00e4tigkeiten sind uns nicht bekannt. Folgende Urteile zum Datenschutz bei der Bilderfassung im Stra\u00dfenverkehr sind dar\u00fcber hinaus bekannt:</p> OLG Frankfurt, 06.11.2019 - 2 Ss - OWi 942/19 <p>Das OLG Frankfurt hat 2019 entschieden, dass private Dienstleister keine Verkehrs\u00fcberwachungen durf\u00fchren d\u00fcrfen und entsprechende Bu\u00dfgeldbescheide gesetzeswidrig sind.</p> BGH 15.05.2018 \u2013 VI ZR 233/17 <p>Der Bundesgerichtshof hat 2018 das sogenannte Dashcam-Urteil gef\u00e4llt, nachdem es f\u00fcr private Personen datenschutzrechtlich unzul\u00e4ssig ist, personenbezogene Merkmale mit einer Kamera aus einem fahrenden Auto \"permanent und anlasslos\" aufzuzeichnen. Kameras mit Ringspeicher, die das Geschehen nur im Falle eines Unfalls permanent speichern, w\u00e4ren demnach zul\u00e4ssig. Im entsprechenden Fall hat das Gericht die permanente Videoaufzeichnung dennoch als Beweismittel zugelassen.</p> Muss eine Verkehrskamera als solche erkennbar und ausgewiesen sein oder darf man \"verdeckt\" filmen? <p>Falls keine personenbezogenen Daten erfasst werden, ist keine Information der Verkehrsbeteiligten notwendig.</p> <p>Falls personenbezogene Daten erfasst werden, ist grunds\u00e4tzlich eine Einverst\u00e4ndniserkl\u00e4rung der jeweiligen Personen einzuholen. Da dies bei Videobeobachtungen im Verkehrsraum meist nicht m\u00f6glich ist, soll stattdessen eine gute erkennbare Information der Verkehrsteilnehmenden erfolgen (z.B. \u00fcber ein auch f\u00fcr Kfz-F\u00fchrende gut lesbares Schild \"Verkehrserhebung\") und Detailinformationen zur Erhebung an bzw. in der N\u00e4he der Kamera (mind. erhebende Institution und Kontaktinformationen, weitere Details z.B. auf einer Website, die per QR-Code erreichbar ist).</p> <p>Falls personenbezogene Daten erfasst werden und im Rahmen von Forschungsprojekten eine Information der Verkehrsteilnehmenden \u00fcber die Verkehrserhebung die Verl\u00e4sslichkeit der angestrebten Ergebnisse beeintr\u00e4chtigen w\u00fcrde (z.B. bei der Untersuchung regelkonformen oder sicherheitsrelevanten Verkehrsverhaltens), kann in Einzelf\u00e4llen darauf verzichtet werden. Grundlage hierf\u00fcr ist der Forschungsparagraph des jeweiligen Landesdatenschutzgesetzes, nach dem die Freiheit von Forschung und Lehre besonders sch\u00fctzenswert ist und das Pers\u00f6nlichkeitsrecht \u00fcberwiegen kann. Dieses Vorgehen sollte jedoch ausf\u00fchrlich begr\u00fcndet und von Beginn an dokumentiert werden, um etwaigen Klagen durch Verkehrsteilnehmende vor Gericht standzuhalten. Eine Abstimmung mit der zust\u00e4ndigen Stelle f\u00fcr Informationssicherheit wird dringend empfohlen.</p>"},{"location":"overview/dataprivacy/#vermeidung-der-erfassung-personenbezogener-daten","title":"Vermeidung der Erfassung personenbezogener Daten","text":"Wie sind personenbezogene Daten definiert? Z\u00e4hlen dazu neben Gesicht und Nummernschild auch die Gr\u00f6\u00dfe der Person, die Art ihres Gangs, die Art und Farbe ihrer Kleidung oder auff\u00e4llige Frisuren? <p>Laut Art. 4 Abs. 1 DSGVO sind alle Namen, Kennnummern, Standortdaten und Merkmale von nat\u00fcrlichen Personen als \"personenbezogene Daten\" definiert, die R\u00fcckschl\u00fcsse auf deren Identit\u00e4t erlauben. Es gilt als grunds\u00e4tzlich anerkannt, dass im Bereich des Stra\u00dfenverkehrs das Kfz-Kennzeichen und das Gesicht einer Person als personenbezogene Daten gelten. In Einzelf\u00e4llen k\u00f6nnte auch die Kombination anderer Merkmale wie Gr\u00f6\u00dfe, Kleidung und Frisur mit weiteren Informationen (z.B. dem Standort) die Identifikation einer Person erm\u00f6glichen. Hier liegt die Beweispflicht jedoch bei potentiellen Klagenden und die Identifikation der Person im Bild/Video muss beispielhaft durch unabh\u00e4ngige Personen nachgewiesen werden (d.h. es reicht nicht aus, wenn die Person selbst oder eine ihr bekannte Person sie im Bild/Video identifizieren kann). Es sind Klagen und Urteile zur Identifikation von Personen auf Fotographien des \u00f6ffentlichen Raums bekannt, in denen f\u00fcr die Kl\u00e4ger entschieden wurde \u2013 diese sind jedoch nicht einfach auf Videoerhebungen im Stra\u00dfenverkehr zum Zwecke der allgemeinen Verkehrsdatenerfassung \u00fcbertragbar.</p> Welche Entfernung zur Kamera z\u00e4hlt f\u00fcr die Datenschutzkonformit\u00e4t? <p>Die Kamera ist grunds\u00e4tzlich so zu konfigurieren, dass nur der relevante Untersuchungsbereich erfasst wird. Wenn dies technisch nicht m\u00f6glich ist, dann muss im Nachgang eine Sichtung und L\u00f6schung erfolgen. Dies gilt z. B. auch f\u00fcr F\u00e4lle, in denen eine Person sich aktiv auf H\u00f6he der Kamera begibt, um diese aus der N\u00e4he zu betrachten und dadurch selbst deutlich erkennbar wird. Sichtung und L\u00f6schung k\u00f6nnen auch automatisiert im Nachgang erfolgen (technische Ma\u00dfnahme, falls z.B. Objekte automatisiert detektiert werden und ein Grenzwert f\u00fcr die Gr\u00f6\u00dfe des erfassten Objekts \u00fcberschritten wird).</p> Reicht es aus, in den Videos sichtbare Gesichter und Kennzeichen zu verpixeln? Wann und wie muss das geschehen? <p>Grunds\u00e4tzlich sollte wenn m\u00f6glich bereits durch niedrige Aufl\u00f6sung und De-Fokussierung die Erfassung von Kennzeichen und Gesichtern vermieden werden.</p> <p>Wenn dies f\u00fcr bestimmte, nicht interessierende Bildbereiche nicht m\u00f6glich ist, kann als erste Alternative eine Nicht-Erfassung (z. B. Schw\u00e4rzung) dieser Bildbereiche bereits im Kamerasystem gepr\u00fcft und technisch umgesetzt werden.</p> <p>Wenn auch dies nicht m\u00f6glich ist, k\u00f6nnen die entsprechenden Bildbereiche direkt auf der Kamera live und permanent verpixelt und somit anonymisiert werden. Diese Methode setzt eine automatisierte Detektion des Kennzeichens bzw. Gesichts voraus. Wenn daraus keine weiteren Daten abgeleitet und gespeichert oder \u00fcbermittelt werden (z.B. das Kennzeichen als Klartext), gilt diese Methode als anerkannt, da sie dem Stand der Technik entspricht. Vor dem Einsatz dieser Methode wird jedoch dringend empfohlen, die Konformit\u00e4t mit der entsprechend geltenden Rechtsgrundlage zu pr\u00fcfen.</p> <p>Das zeitversetzte, nachtr\u00e4gliche (manuelle oder automatische) Verpixeln von Kennzeichen oder Gesichtern bei der Sichtung/Auswertung des Videomaterials sollte auf Einzelf\u00e4lle beschr\u00e4nkt bleiben (siehe Frage \"Welche Entfernung zur Kamera z\u00e4hlt f\u00fcr die Datenschutzkonformit\u00e4t?\"). Sollten die zuvor genannten Ma\u00dfnahmen nicht zielf\u00fchrend sein und sind dennoch geh\u00e4uft Kennzeichen oder Gesichter in den Videos zu vermuten, kann nicht mehr von einer anonymen Erhebung (Vermeidung der Erhebung personenbezogener Daten) gesprochen werden. In diesem Fall m\u00fcssen die nachfolgend beschriebenen Rahmenbedingungen f\u00fcr die Erhebung personenbezogener Daten eingehalten werden.</p>"},{"location":"overview/dataprivacy/#ablauf-in-fallen-in-denen-die-erfassung-personenbezogener-daten-nicht-vermieden-werden-kann","title":"Ablauf in F\u00e4llen, in denen die Erfassung personenbezogener Daten nicht vermieden werden kann","text":"Unter welchen Umst\u00e4nden d\u00fcrfen personenbezogene Daten erhoben werden? <p>Grunds\u00e4tzlich ist zu pr\u00fcfen, ob f\u00fcr den Erhebungszweck auch eine Videobeobachtung ohne die Erfassung personenbezogener Daten (bzw. eine Methode g\u00e4nzlich ohne Videobeobachtung) m\u00f6glich ist. Wenn dies nicht m\u00f6glich ist und das Interesse der Allgemeinheit an den Ergebnissen der Videobeobachtung die Interessen der einzelnen Personen (deren Datenschutz) \u00fcberwiegt, ist eine videobasierte Erfassung personenbezogener Daten grunds\u00e4tzlich denkbar.</p> <p>Dabei muss die Zustimmung der betroffenen Personen eingeholt werden oder zumindest eine Information der betroffenen Personen erfolgen (siehe Frage \"Muss eine Verkehrskamera als solche erkennbar und ausgewiesen sein oder darf man \"verdeckt\" filmen?\").</p> Was beinhaltet ein Datenschutzkonzept und in welchen F\u00e4llen braucht man es? Wer muss dieses \"absegnen\"? <p>Die Erstellung eines Datenschutzkonzepts ist nicht in allen F\u00e4llen n\u00f6tig, in denen die Erhebung personenbezogener Daten geplant ist. Es ist jedoch sinnvoll, ein solches Konzept zu erstellen, da eine Rechenschaftspflicht besteht, d.h. eine Dokumentation zu f\u00fchren ist. In der DSGVO und in den Landesdatenschutzgesetzen ist f\u00fcr viele Aspekte auch grunds\u00e4tzlich geregelt, ob und wie personenbezogene Daten erhoben werden d\u00fcrfen. Wenn die Konformit\u00e4t mit diesen Rechtsgrundlagen eingehalten wird, reicht es aus, dies im Nachgang belegen zu k\u00f6nnen. Falls diesbez\u00fcglich Unklarheiten existieren, empfiehlt sich die R\u00fccksprache mit Datenschutzfachleuten (z.B. den zust\u00e4ndigen Beauftragten f\u00fcr Informationssicherheit und dem/der Datenschutzbeauftragten). In Zusammenarbeit mit diesen kann au\u00dferdem in F\u00e4llen, in denen vereinzelte Grunds\u00e4tze der Verarbeitung personenbezogener Daten nicht eingehalten werden k\u00f6nnen, eine sogenannte \"Datenschutzfolgenabsch\u00e4tzung\" erarbeitet werden. In dieser werden Erhebungszweck und -design, der Umgang mit den Daten sowie Gr\u00fcnde f\u00fcr die vereinzelten Abweichungen von den Rechtsgrundlagen detailliert erl\u00e4utert.</p> Sind die Daten bereits w\u00e4hrend der Messung vor Diebstahl zu sch\u00fctzen? Ist es datenschutzkonform m\u00f6glich, einen externen Zugang auf die Kamera w\u00e4hrend der Messung herzustellen? <p>Wenn personenbezogenen Daten erfasst werden, muss bereits w\u00e4hrend der Erfassung technisch sichergestellt werden, dass unbefugte Personen keinen Zugriff auf diese Daten haben. Daher sollten die Daten auf Kameras im Verkehrsraum nach dem aktuellen Stand der Technik gesch\u00fctzt werden. Momentan k\u00f6nnte dies durch ein sicheres Passwort erfolgen. Falls personenbezogene Daten \u00fcbermittelt werden, sind verschl\u00fcsselte Verbindungen zu empfehlen (z. B. per SSH, HTTPS, VPN).</p> Unter welchen Umst\u00e4nden darf man die Videos speichern und aufheben? D\u00fcrfen die Videos f\u00fcr einen anderen als f\u00fcr den urspr\u00fcnglichen Zweck verwendet werden? <p>Personenbezogene Daten m\u00fcssen so gespeichert werden, dass ausschlie\u00dflich befugte Personen Zugriff zu diesen haben. Au\u00dferdem d\u00fcrfen sie grunds\u00e4tzlich nur so lange gespeichert werden bis deren spezifischer Erhebungszweck erf\u00fcllt wurde (also z.B. die spezifische Forschungsfrage innerhalb eines Projekts beantwortet wurde). Anschlie\u00dfend m\u00fcssen die Daten gel\u00f6scht oder um die Eigenschaften reduziert werden, die einen Personenbezug erm\u00f6glichen (bei Videos z.B. Verringerung der Aufl\u00f6sung). Personenbezogene Daten d\u00fcrfen nur dann auch nach Erf\u00fcllung des spezifischen Erhebungszwecks vorgehalten werden, wenn die betroffenen Personen dem ausdr\u00fccklich zugestimmt haben. Vor der Verwendung gespeicherter personenbezogener Daten f\u00fcr weitere Zwecke ist jedoch erneut die Konformit\u00e4t des Zwecks sowohl mit den Rechtsgrundlagen als auch mit dem Wortlaut der Einwilligung der betroffenen Personen zu pr\u00fcfen.</p> Darf ich die Verkehrsvideos weitergeben? Was muss ich dabei beachten? (Stichworte \"Nachnutzbarkeit\" und \"Datensparsamkeit\") <p>Wenn es sich um personenbezogene Daten handelt und der \u00f6ffentliche F\u00f6rder- oder Auftraggeber f\u00fcr den Datenschutz verantwortlich ist, liegt diese Entscheidung bei ihm (siehe Frage \"Wer ist in einem Projekt zust\u00e4ndig f\u00fcr den Datenschutz und tr\u00e4gt die Verantwortung? Die Auftragnehmenden oder die Auftraggebenden?\").</p> <p>Wenn der F\u00f6rder- oder Auftragnehmer selbst f\u00fcr den Datenschutz verantwortlich ist und die Anfrage von \u00f6ffentlichen Stellen (z.B. Gebietsk\u00f6rperschaft, Beh\u00f6rde, Universit\u00e4t) kommt, ist die anfragende \u00f6ffentliche Stelle auch f\u00fcr die \u00dcbermittlung verantwortlich. Das bedeutet, dass die anfragende Stelle sicherstellen muss, dass die \u00dcbermittlung der Daten gerechtfertigt ist.</p> <p>Einer \u00dcbermittlung an eine \u00f6ffentliche Stelle steht also nichts im Wege. Dies gilt auch, falls die \u00f6ffentliche Stelle die Daten f\u00fcr einen anderen als den urspr\u00fcnglichen Erhebungszweck verarbeiten m\u00f6chte (diese \u00f6ffentliche Stelle muss dann selbst pr\u00fcfen, ob dies rechtskonform ist). Vor einer \u00dcbermittlung an eine nicht\u00f6ffentliche Stelle (z.B. an ein Ingenieurb\u00fcro) empfehlen wir dringend die Konsultation von Fachleuten f\u00fcr Datenschutz. Die \u00dcbermittlung anonymisierter Verkehrsvideos stellt aus datenschutzrechtlicher Sicht nat\u00fcrlich kein Problem dar und kann aus dieser Hinsicht an alle Stellen erfolgen.</p>"},{"location":"blog/category/technologie/","title":"Technologie","text":""}]}