{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to OpenTrafficCam \u00b6 Open Traffic Cam makes traffic detection easier, faster and more efficient. Welcome to OpenTrafficCam , the only fully integrated OpenSource-workflow for video-based traffic detection. This website will cover all the documentation in one place. Please also check out our github page for further information and the code. How it works \u00b6 OpenTrafficCam consists of three modules. The OTCamera hardware itself to record videos, OTVision , a collection of algorithms to generate trajectories of objects (road users) based on the videos and OTAnalytics to gather traffic measures based on the trajectories. In addition to these three modules, we will provide a set of labelled images of German traffic objects with OTLabels . Use Cases \u00b6 Motivation and Development \u00b6 OpenTrafficCam is funded by the German Federal Ministry of Transport and Digital Infrastructure as part of the research initiative mFUND.","title":"Welcome"},{"location":"#welcome-to-opentrafficcam","text":"Open Traffic Cam makes traffic detection easier, faster and more efficient. Welcome to OpenTrafficCam , the only fully integrated OpenSource-workflow for video-based traffic detection. This website will cover all the documentation in one place. Please also check out our github page for further information and the code.","title":"Welcome to OpenTrafficCam"},{"location":"#how-it-works","text":"OpenTrafficCam consists of three modules. The OTCamera hardware itself to record videos, OTVision , a collection of algorithms to generate trajectories of objects (road users) based on the videos and OTAnalytics to gather traffic measures based on the trajectories. In addition to these three modules, we will provide a set of labelled images of German traffic objects with OTLabels .","title":"How it works"},{"location":"#use-cases","text":"","title":"Use Cases"},{"location":"#motivation-and-development","text":"OpenTrafficCam is funded by the German Federal Ministry of Transport and Digital Infrastructure as part of the research initiative mFUND.","title":"Motivation and Development"},{"location":"OTAnalytics/","text":"In a Nutshell \u00b6 The brain of OpenTrafficCam. OTAnalytics is a set of self developed algorithms to gather traffic measures based on trajectories of road users. Key features \u00b6 Generation of traffic flow matrices and node flow diagrams Measurement of time gaps, velocities and accelerations Long-term: Analysis of near-accidents (e.g. TTC, PET) Visualization of traffic data Import of trajectories from other systems (e.g. DataFromSky) Runs on any Windows laptop Content of documentation \u00b6 Most of the code will be developed by the OpenTrafficCam contributors (based on python libraries like NumPy , pandas , tkinter and OpenCV ).","title":"In a Nutshell"},{"location":"OTAnalytics/#in-a-nutshell","text":"The brain of OpenTrafficCam. OTAnalytics is a set of self developed algorithms to gather traffic measures based on trajectories of road users.","title":"In a Nutshell"},{"location":"OTAnalytics/#key-features","text":"Generation of traffic flow matrices and node flow diagrams Measurement of time gaps, velocities and accelerations Long-term: Analysis of near-accidents (e.g. TTC, PET) Visualization of traffic data Import of trajectories from other systems (e.g. DataFromSky) Runs on any Windows laptop","title":"Key features"},{"location":"OTAnalytics/#content-of-documentation","text":"Most of the code will be developed by the OpenTrafficCam contributors (based on python libraries like NumPy , pandas , tkinter and OpenCV ).","title":"Content of documentation"},{"location":"OTAnalytics/gettingstarted/firstuse/","text":"First Use \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"First Use"},{"location":"OTAnalytics/gettingstarted/firstuse/#first-use","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"First Use"},{"location":"OTAnalytics/gettingstarted/installation/","text":"Installation \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Installation"},{"location":"OTAnalytics/gettingstarted/installation/#installation","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Installation"},{"location":"OTAnalytics/gettingstarted/requirements/","text":"Requirements \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Requirements"},{"location":"OTAnalytics/gettingstarted/requirements/#requirements","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Requirements"},{"location":"OTAnalytics/preparingtracks/data/","text":"Data \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Data"},{"location":"OTAnalytics/preparingtracks/data/#data","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Data"},{"location":"OTAnalytics/preparingtracks/movements/","text":"Movements \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Movements"},{"location":"OTAnalytics/preparingtracks/movements/#movements","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Movements"},{"location":"OTAnalytics/preparingtracks/sections/","text":"Sections \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Sections"},{"location":"OTAnalytics/preparingtracks/sections/#sections","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Sections"},{"location":"OTAnalytics/trafficanalytics/counts/","text":"Traffic Counts \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Traffic Counts"},{"location":"OTAnalytics/trafficanalytics/counts/#traffic-counts","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Traffic Counts"},{"location":"OTCamera/","text":"Features \u00b6 The eye of OpenTrafficCam. OTCamera is a mobile camera system for capturing traffic videos. It is not for sale, but for DIY. You don't want to build the camera yourself and preferably don't want to have anything to do with the measurement? Well, just write us . Key features \u00b6 Based on Raspberry Pi Zero W Waterproof case Operation with buttons and/or smartphone Continuous recording for about one week Privacy compliant recording Under 400 \u20ac per camera system Content of this documentation \u00b6 List of hardware components (and where to purchase them) Software needed for video capturing and camera calibration Instructions for assembling the hardware and installing the software Recommendations for mounting, alignment and settings of the camera We are providing this information for two different prototypes. The Development Version is intended to actively participate in the development of OTCamera. It is also recommended to set up a development version to adapt the software and hardware to your own requirements. In addition, getting started with this prototype is very easy and can easily be done directly at your desk without the need for additional hardware. For the Field Version you need the ability to solder (simple) components to a blank circuit board. For this you get the most buttons (yay buttons) and the easiest operation in the field. All parts can be purchased from various dealers and are then easily plugged together. Also, the power consumption is the lowest (and the battery life the longest) and you can fully customize the hardware to your own needs.","title":"Features"},{"location":"OTCamera/#features","text":"The eye of OpenTrafficCam. OTCamera is a mobile camera system for capturing traffic videos. It is not for sale, but for DIY. You don't want to build the camera yourself and preferably don't want to have anything to do with the measurement? Well, just write us .","title":"Features"},{"location":"OTCamera/#key-features","text":"Based on Raspberry Pi Zero W Waterproof case Operation with buttons and/or smartphone Continuous recording for about one week Privacy compliant recording Under 400 \u20ac per camera system","title":"Key features"},{"location":"OTCamera/#content-of-this-documentation","text":"List of hardware components (and where to purchase them) Software needed for video capturing and camera calibration Instructions for assembling the hardware and installing the software Recommendations for mounting, alignment and settings of the camera We are providing this information for two different prototypes. The Development Version is intended to actively participate in the development of OTCamera. It is also recommended to set up a development version to adapt the software and hardware to your own requirements. In addition, getting started with this prototype is very easy and can easily be done directly at your desk without the need for additional hardware. For the Field Version you need the ability to solder (simple) components to a blank circuit board. For this you get the most buttons (yay buttons) and the easiest operation in the field. All parts can be purchased from various dealers and are then easily plugged together. Also, the power consumption is the lowest (and the battery life the longest) and you can fully customize the hardware to your own needs.","title":"Content of this documentation"},{"location":"OTCamera/calibration/calibrate/","text":"How to calibrate OTCamera \u00b6 OTCamera gives you the opportunity to receive distinctive camera parameters. These include intrinsics, extrinsics and distortion coefficients. By using our web browser based program and multiple images of a calibration pattern, like a chessboard you can calculate a camera matrix and distortion coefficients. You will be able to evaluate your calibration with a reprojection error which is used to quantify how closely you can recreate true projection. In other words: You can undistort pictures, trajectories or any other pixel coordinates. First step First you need to download a chessboard pattern (you can search for calibration board). The pattern size and dimension will play no big role for now, so feel free to use any chessboard. Attach your printout to a solid object. The image should be as flat as possible. For best results its unavoidable to let your calibration pattern be manufactured by professionals. Procedure Start the calibration application on your raspberrypi. Create a new calibration with a self-chosen CamID. Enter the number of chessboardcolumns, -rows and the squaresize in mm. Choose a number of wanted successful calibration pictures and resolution. Start the calibrationprocess. By clicking on \"receive coefficients\" your camera parameters will be stored as json-file on your raspberrypi. Evaluate the reprojection error displayed in the userinterface.","title":"How to calibrate OTCamera"},{"location":"OTCamera/calibration/calibrate/#how-to-calibrate-otcamera","text":"OTCamera gives you the opportunity to receive distinctive camera parameters. These include intrinsics, extrinsics and distortion coefficients. By using our web browser based program and multiple images of a calibration pattern, like a chessboard you can calculate a camera matrix and distortion coefficients. You will be able to evaluate your calibration with a reprojection error which is used to quantify how closely you can recreate true projection. In other words: You can undistort pictures, trajectories or any other pixel coordinates. First step First you need to download a chessboard pattern (you can search for calibration board). The pattern size and dimension will play no big role for now, so feel free to use any chessboard. Attach your printout to a solid object. The image should be as flat as possible. For best results its unavoidable to let your calibration pattern be manufactured by professionals. Procedure Start the calibration application on your raspberrypi. Create a new calibration with a self-chosen CamID. Enter the number of chessboardcolumns, -rows and the squaresize in mm. Choose a number of wanted successful calibration pictures and resolution. Start the calibrationprocess. By clicking on \"receive coefficients\" your camera parameters will be stored as json-file on your raspberrypi. Evaluate the reprojection error displayed in the userinterface.","title":"How to calibrate OTCamera"},{"location":"OTCamera/gettingstarted/assembly/","text":"Assembly \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Assembly"},{"location":"OTCamera/gettingstarted/assembly/#assembly","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Assembly"},{"location":"OTCamera/gettingstarted/firstuse/","text":"First Use \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"First Use"},{"location":"OTCamera/gettingstarted/firstuse/#first-use","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"First Use"},{"location":"OTCamera/gettingstarted/installation/","text":"Installation \u00b6 No matter which version you want to install, you will always use a Raspberry Pi as a base. This guide describes the basic steps you need to perform for each version. You will need: Raspberry Pi 2B/3B(+)/4/Zero W and power supply Micro SD card (a High Endurance version is recommended) SD Card Reader Raspberry Pi Imager . Warning Even though we have completed the guide ourselves, every PC is different and Raspberry OS can change as well. There are steps described with which you can break your Windows on the PC. You should know what you are doing. We are not responsible for any damage that may occur. Prepare the SD Card \u00b6 Download and install the Raspberry Pi Imager , insert the SD-Card and start the Imager. It's recommended to use the Raspberry Pi Imager Advanced Options to do some basic setup. Just press Ctrl + Shift + X on startscreen to access advanced options. Setup a hostname, enable SSH, either password based or using public-key authentication (recommended!), configure your Wifi credentials and choose the correct locale for Wifi, timezone and keyboard layout. You can also skip the first-run wizard. How to generate a public key Generate SSH-Keys for password-less connection. On your desktop computer open a command line terminal (CMD or Powershell on Windows or a bash on Linux) and run ssh-keygen to generate a public private key combination. Add the private key to your ssh-agent (you may need to update OpenSSH on Windows ). Now insert the SD card into the PC. Select Raspberry Pi OS Lite (32-bit) as operating system under \"Raspberry Pi (Other)\". Then select the SD card on which the operating system will be installed. Warning It is possible to select not only the SD card, but also the hard drive or SSD where your operating system or other PC data is stored. Make sure that you really select the right SD card. Most of the time, you can tell by the size. Press the Write button and wait until the Raspberry Pi Imager has completely written and verified your SD card. Tip Sometimes the verification may fail. Try another USB port on your PC or another card reader. If this does not help, maybe the SD card is broken. Setup without Raspberry Pi Imager Add an empty file named ssh to the boot partition to enable ssh on first boot . Add your Wifi credentials as described in the Raspberry Pi Documentation Connect to the pi using ssh ( ssh pi@raspberry )to the Pi using password authentication. Create and edit the needed authorized_keys file. mkdir -p ~/.ssh nano ~/.ssh/authorized_keys Copy your public key on the host and paste it on the pi, save&close using Ctrl+X - Enter - Y. Setup the Raspberry \u00b6 Now, take the SD-Card and insert it in the Raspberry. You can now boot the Pi on LAN or within your Wifi by connecting it to the power supply. Try to connect to the Pi using a command line or PowerShell: ssh pi @otcamera01 If everything is setup correctly, you will be asked to add the host key to the list of known hosts (answer: yes) and you should be connected to your Raspberry Pi. warning: agent returned different signature type ssh-rsa (expected rsa-sha2-512) If you are on Windows you may need to update OpenSSH if you ar getting this error: warning : agent returned different signature type ssh-rsa ( expected rsa-sha2 - 512 ) Warning This guide should run on Windows 10 but you are modifing your system files. Please do not do anything you do not understand! Read the source for more information. We are not liable for any damage you may cause to your system. But we followed the steps ourselves and everything went well. Download the latest OpenSSH for Windows release here . Depending on your Windows version it is probably OpenSSH-Win64.zip . Open Windows Explorer and navigate to your Download folder. You should see the OpenSSH-Win64.zip . Open an elevated PowerShell via File \u2192 Open Windows PowerShell \u2192 Open Windows PowerShell as administrator. If you copy and paste the following code, you will unzip OpenSSH and overwrite the system's default OpenSSH version. # Overwrite windows installed bins $openSshBins = ( Get-ChildItem 'C:\\WINDOWS\\System32\\OpenSSH\\' ). Name Expand-Archive -Path .\\ OpenSSH-Win64 . zip -DestinationPath . takeown . exe / a / r / f C :\\ Windows \\ System32 \\ OpenSSH \\ icacls . exe 'C:\\Windows\\System32\\OpenSSH' / grant 'BUILTIN\\Administrators:(OI)(CI)F' icacls . exe 'C:\\Windows\\System32\\OpenSSH' / grant 'BUILTIN\\Administrators:F' / t Stop-Service ssh-agent $openSshBins | %{ Copy-Item -Path .\\ OpenSSH-Win64 \\ $_ -Destination C :\\ Windows \\ System32 \\ OpenSSH \\ } Start-Service ssh-agent If you have successfully logged in now, we can configure the Raspberry Pi for the OpenTrafficCam. Update the pi by running apt and reboot. sudo apt update && sudo apt upgrade -y sudo reboot A new version of configuration file is available If you get this message, don't worry. Keep the local version currently installed, since we changed the ssh server configuration using the RPi Imager. Reconnect to your pi (open PowerShell and run ssh pi@otcamera01 ) and run the raspberry configuration tool. sudo raspi-config Change the following settings to appropriate values: System Options \u2192 Password (choose a new password for security reasons) Interface Options \u2192 Camera \u2192 enable Interface Options \u2192 Serial Port \u2192 no \u2192 Serial Hardware \u2192 yes Setup without Raspberry Pi Imager If you did not use the Raspberry Pi Imager, you will need to setup a few more things. System Options Hostname Localization Options Timezone (Europe/Berlin) WLAN Country (DE) Exit the raspi-config selecting \"Finish\" and reboot the Pi afterwards. Setup Python and Dependencies \u00b6 By default, Raspberry OS light doesn't come with PIP installed. We will need it, to install required packages. sudo apt install python3-pip -y Raspberry OS ships with python 2 and python 3. By default python 2 is used. We want to change that to python 3 by adding two single lines to .bashrc . echo \"alias python='/usr/bin/python3'\" >> ~/.bashrc echo \"alias pip=pip3\" >> ~/.bashrc source ~/.bashrc python --version pip --version Both commands should state, that they are (using) python 3.(x). OTCamera requires additional python packages, which need to be installed. sudo apt install python3-picamera python3-gpiozero pysimpleguiweb -y pip3 install art psutil Note In the future, we would like to offer a ready-to-use image for the Raspberry Pi, which can be easily installed. The setup will then be much easier.","title":"Installation"},{"location":"OTCamera/gettingstarted/installation/#installation","text":"No matter which version you want to install, you will always use a Raspberry Pi as a base. This guide describes the basic steps you need to perform for each version. You will need: Raspberry Pi 2B/3B(+)/4/Zero W and power supply Micro SD card (a High Endurance version is recommended) SD Card Reader Raspberry Pi Imager . Warning Even though we have completed the guide ourselves, every PC is different and Raspberry OS can change as well. There are steps described with which you can break your Windows on the PC. You should know what you are doing. We are not responsible for any damage that may occur.","title":"Installation"},{"location":"OTCamera/gettingstarted/installation/#prepare-the-sd-card","text":"Download and install the Raspberry Pi Imager , insert the SD-Card and start the Imager. It's recommended to use the Raspberry Pi Imager Advanced Options to do some basic setup. Just press Ctrl + Shift + X on startscreen to access advanced options. Setup a hostname, enable SSH, either password based or using public-key authentication (recommended!), configure your Wifi credentials and choose the correct locale for Wifi, timezone and keyboard layout. You can also skip the first-run wizard. How to generate a public key Generate SSH-Keys for password-less connection. On your desktop computer open a command line terminal (CMD or Powershell on Windows or a bash on Linux) and run ssh-keygen to generate a public private key combination. Add the private key to your ssh-agent (you may need to update OpenSSH on Windows ). Now insert the SD card into the PC. Select Raspberry Pi OS Lite (32-bit) as operating system under \"Raspberry Pi (Other)\". Then select the SD card on which the operating system will be installed. Warning It is possible to select not only the SD card, but also the hard drive or SSD where your operating system or other PC data is stored. Make sure that you really select the right SD card. Most of the time, you can tell by the size. Press the Write button and wait until the Raspberry Pi Imager has completely written and verified your SD card. Tip Sometimes the verification may fail. Try another USB port on your PC or another card reader. If this does not help, maybe the SD card is broken. Setup without Raspberry Pi Imager Add an empty file named ssh to the boot partition to enable ssh on first boot . Add your Wifi credentials as described in the Raspberry Pi Documentation Connect to the pi using ssh ( ssh pi@raspberry )to the Pi using password authentication. Create and edit the needed authorized_keys file. mkdir -p ~/.ssh nano ~/.ssh/authorized_keys Copy your public key on the host and paste it on the pi, save&close using Ctrl+X - Enter - Y.","title":"Prepare the SD Card"},{"location":"OTCamera/gettingstarted/installation/#setup-the-raspberry","text":"Now, take the SD-Card and insert it in the Raspberry. You can now boot the Pi on LAN or within your Wifi by connecting it to the power supply. Try to connect to the Pi using a command line or PowerShell: ssh pi @otcamera01 If everything is setup correctly, you will be asked to add the host key to the list of known hosts (answer: yes) and you should be connected to your Raspberry Pi. warning: agent returned different signature type ssh-rsa (expected rsa-sha2-512) If you are on Windows you may need to update OpenSSH if you ar getting this error: warning : agent returned different signature type ssh-rsa ( expected rsa-sha2 - 512 ) Warning This guide should run on Windows 10 but you are modifing your system files. Please do not do anything you do not understand! Read the source for more information. We are not liable for any damage you may cause to your system. But we followed the steps ourselves and everything went well. Download the latest OpenSSH for Windows release here . Depending on your Windows version it is probably OpenSSH-Win64.zip . Open Windows Explorer and navigate to your Download folder. You should see the OpenSSH-Win64.zip . Open an elevated PowerShell via File \u2192 Open Windows PowerShell \u2192 Open Windows PowerShell as administrator. If you copy and paste the following code, you will unzip OpenSSH and overwrite the system's default OpenSSH version. # Overwrite windows installed bins $openSshBins = ( Get-ChildItem 'C:\\WINDOWS\\System32\\OpenSSH\\' ). Name Expand-Archive -Path .\\ OpenSSH-Win64 . zip -DestinationPath . takeown . exe / a / r / f C :\\ Windows \\ System32 \\ OpenSSH \\ icacls . exe 'C:\\Windows\\System32\\OpenSSH' / grant 'BUILTIN\\Administrators:(OI)(CI)F' icacls . exe 'C:\\Windows\\System32\\OpenSSH' / grant 'BUILTIN\\Administrators:F' / t Stop-Service ssh-agent $openSshBins | %{ Copy-Item -Path .\\ OpenSSH-Win64 \\ $_ -Destination C :\\ Windows \\ System32 \\ OpenSSH \\ } Start-Service ssh-agent If you have successfully logged in now, we can configure the Raspberry Pi for the OpenTrafficCam. Update the pi by running apt and reboot. sudo apt update && sudo apt upgrade -y sudo reboot A new version of configuration file is available If you get this message, don't worry. Keep the local version currently installed, since we changed the ssh server configuration using the RPi Imager. Reconnect to your pi (open PowerShell and run ssh pi@otcamera01 ) and run the raspberry configuration tool. sudo raspi-config Change the following settings to appropriate values: System Options \u2192 Password (choose a new password for security reasons) Interface Options \u2192 Camera \u2192 enable Interface Options \u2192 Serial Port \u2192 no \u2192 Serial Hardware \u2192 yes Setup without Raspberry Pi Imager If you did not use the Raspberry Pi Imager, you will need to setup a few more things. System Options Hostname Localization Options Timezone (Europe/Berlin) WLAN Country (DE) Exit the raspi-config selecting \"Finish\" and reboot the Pi afterwards.","title":"Setup the Raspberry"},{"location":"OTCamera/gettingstarted/installation/#setup-python-and-dependencies","text":"By default, Raspberry OS light doesn't come with PIP installed. We will need it, to install required packages. sudo apt install python3-pip -y Raspberry OS ships with python 2 and python 3. By default python 2 is used. We want to change that to python 3 by adding two single lines to .bashrc . echo \"alias python='/usr/bin/python3'\" >> ~/.bashrc echo \"alias pip=pip3\" >> ~/.bashrc source ~/.bashrc python --version pip --version Both commands should state, that they are (using) python 3.(x). OTCamera requires additional python packages, which need to be installed. sudo apt install python3-picamera python3-gpiozero pysimpleguiweb -y pip3 install art psutil Note In the future, we would like to offer a ready-to-use image for the Raspberry Pi, which can be easily installed. The setup will then be much easier.","title":"Setup Python and Dependencies"},{"location":"OTCamera/gettingstarted/requirements/","text":"Requirements \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Requirements"},{"location":"OTCamera/gettingstarted/requirements/#requirements","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Requirements"},{"location":"OTCamera/recording/getvideos/","text":"Get the Videos \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Get the Videos"},{"location":"OTCamera/recording/getvideos/#get-the-videos","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Get the Videos"},{"location":"OTCamera/recording/mounting/","text":"Mounting \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Mounting"},{"location":"OTCamera/recording/mounting/#mounting","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Mounting"},{"location":"OTCamera/recording/preparation/","text":"Preparation \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Preparation"},{"location":"OTCamera/recording/preparation/#preparation","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Preparation"},{"location":"OTCamera/recording/record/","text":"Record \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Record"},{"location":"OTCamera/recording/record/#record","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Record"},{"location":"OTCamera/recording/safetywarning/","text":"Safety Warning \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Safety Warning"},{"location":"OTCamera/recording/safetywarning/#safety-warning","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Safety Warning"},{"location":"OTCamera/recording/settings/","text":"Settings \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Settings"},{"location":"OTCamera/recording/settings/#settings","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Settings"},{"location":"OTLabels/","text":"Features \u00b6 The backbone of OpenTrafficCam. OTLabels is a workflow for retraining AI models to detect road users in videos based on custom video frames and existing open source software packages ( CVAT , YOLO v5 ). Key features \u00b6 Annotation of custom video frames using existing open source software ( CVAT ) Retrain existing neural network models with YOLO v5 for individual optimization of the detection ability Runs on any Windows laptop. However, a decent GPU with CUDA support is strongly recommended. Content of documentation \u00b6 Annotation Training Most of the code describing the workflow will be developed by the OpenTrafficCam contributors (based on python libraries like NumPy , pandas , tkinter and OpenCV ).","title":"Features"},{"location":"OTLabels/#features","text":"The backbone of OpenTrafficCam. OTLabels is a workflow for retraining AI models to detect road users in videos based on custom video frames and existing open source software packages ( CVAT , YOLO v5 ).","title":"Features"},{"location":"OTLabels/#key-features","text":"Annotation of custom video frames using existing open source software ( CVAT ) Retrain existing neural network models with YOLO v5 for individual optimization of the detection ability Runs on any Windows laptop. However, a decent GPU with CUDA support is strongly recommended.","title":"Key features"},{"location":"OTLabels/#content-of-documentation","text":"Annotation Training Most of the code describing the workflow will be developed by the OpenTrafficCam contributors (based on python libraries like NumPy , pandas , tkinter and OpenCV ).","title":"Content of documentation"},{"location":"OTLabels/annotation/cvat/","text":"CVAT \u00b6 CVAT is a free video and image annotation tool for computer vision. We recommend the developers guides for Installation as well as the user's guide . If you want to label your own dataset to retrain models, keep in mind that the format of your new labels needs to be the same as the format of the original dataset. On this page we define our intended organizational and technical annotation workflow using CVAT. Since YOLOv5 is based on the the MS COCO dataset of 2D bounding boxes, you need to apply your own labels in the frames from your sample videos in the same format. We therefore recommend the following steps: Install and setup an instance of CVAT either on your local computer or on a server (recommended when working in a team). Import the videos in CVAT and select the frames you want to use for labelling. Download the dataset from CVAT using the YOLO v1.1 format. Pre-annotate your frames with the standard YOLOv5x model. The results do probably not meet the desired quality. However, the pre-annotated labels will save you some time since it not nesscary to draw all labels from scratch. Upload the pre-annotated frames to CVAT and revise the detected labels. Download the final labels from CVAT and convert them to COCO format (using our cvat_to_coco.py script). The COCO dataset and consequently the YOLOv5 models can detect 80 classes. However, for the detection of traffic, only six classes are relevant. Target classes \u00b6 Target classes are all road user classes from the initial COCO data set that are relevant for our sample videos: Person Bicycle Motorcycle Car Bus Truck All vehicles or combinations of vehicles are labeled as \"truck\" if they have twin tires (except for regular service buses or coaches), a superstructure or a loading area or a trailer (also cars with trailer, the 2D box includes vehicle and trailer). Here, as in the COCO dataset, both the \"person\" and the \"bicycle\" are labeled separately for bicyclists. Object dimensions \u00b6 The 2D box boundaries are defined by the outermost parts of the objects visible in that frame of the video. Even if only a part of an object is visible and the class of the object is recognizable for the annotator, it should be labeled. Projects, Tasks and Jobs \u00b6 We define certain set of videos as a project . Every project has multiple tasks (each consists of one video to annotate). Each task has a status (\"Pending\", \"In Progress\" and \"Completed\") and a progress bar showing the number of completed jobs. Open the task with klick on \"Open\". For each task (or video) multiple jobs with a certain amounts of frames from the video are defined. Each job has a status (\"annotation\", \"validation\" and \"completed\"), an \"assignee\" (responsible for annotation) and a \"reviewer\" (responsible for reviewing the assignees annotation). Open the job with klick on \"Job #...\". Import datafix \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website. Workflow \u00b6 For each job, the project coordinator assigns the roles of the \"assignee\" (= annotator) and the \"reviewer\". The reviewer then starts working on a job by initial labeling (see Annotation ) of the video frames. The assignee should save his labeling work from time to time and can also stop and later repeat working on a job. After finishing the initial labeling, the assignee requests a review (menu - request review, the assigned reviewer should already be named there) and thus sets status of the job to \"validation\". Then the reviewer checks the initial labeling done by the assignee and raises issues when there is a problem or question. After reviewing all video frames of the job, the reviewer submits the review (see see Review by either accepting and completing the job or rejecting it (which sets the status of the job bach to \"Annotation\"). Now the assignee has to check the issues, update the labels, resolve the issus and again request a review. The review process starts again and if necessary also next annotations, until the reviewer accepts the job and it is marked as \"Completed\". We recommend the following procedure for annotation in CVAT: Go through all pre-labeled objects on the right Check and delete false positive labels (if there is no object) Check and delete duplicate labels for the same object Check and correct object class for target classes Check and delete objects from other classes if they are classified wrong Zoom in on one quadrant of the image at a time: Check and correct position of object\u00b4s 2D boxes Check if there are objects from the target classes not already pre-labeled, draw new 2D box and annotate object class Download data \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"CVAT"},{"location":"OTLabels/annotation/cvat/#cvat","text":"CVAT is a free video and image annotation tool for computer vision. We recommend the developers guides for Installation as well as the user's guide . If you want to label your own dataset to retrain models, keep in mind that the format of your new labels needs to be the same as the format of the original dataset. On this page we define our intended organizational and technical annotation workflow using CVAT. Since YOLOv5 is based on the the MS COCO dataset of 2D bounding boxes, you need to apply your own labels in the frames from your sample videos in the same format. We therefore recommend the following steps: Install and setup an instance of CVAT either on your local computer or on a server (recommended when working in a team). Import the videos in CVAT and select the frames you want to use for labelling. Download the dataset from CVAT using the YOLO v1.1 format. Pre-annotate your frames with the standard YOLOv5x model. The results do probably not meet the desired quality. However, the pre-annotated labels will save you some time since it not nesscary to draw all labels from scratch. Upload the pre-annotated frames to CVAT and revise the detected labels. Download the final labels from CVAT and convert them to COCO format (using our cvat_to_coco.py script). The COCO dataset and consequently the YOLOv5 models can detect 80 classes. However, for the detection of traffic, only six classes are relevant.","title":"CVAT"},{"location":"OTLabels/annotation/cvat/#target-classes","text":"Target classes are all road user classes from the initial COCO data set that are relevant for our sample videos: Person Bicycle Motorcycle Car Bus Truck All vehicles or combinations of vehicles are labeled as \"truck\" if they have twin tires (except for regular service buses or coaches), a superstructure or a loading area or a trailer (also cars with trailer, the 2D box includes vehicle and trailer). Here, as in the COCO dataset, both the \"person\" and the \"bicycle\" are labeled separately for bicyclists.","title":"Target classes"},{"location":"OTLabels/annotation/cvat/#object-dimensions","text":"The 2D box boundaries are defined by the outermost parts of the objects visible in that frame of the video. Even if only a part of an object is visible and the class of the object is recognizable for the annotator, it should be labeled.","title":"Object dimensions"},{"location":"OTLabels/annotation/cvat/#projects-tasks-and-jobs","text":"We define certain set of videos as a project . Every project has multiple tasks (each consists of one video to annotate). Each task has a status (\"Pending\", \"In Progress\" and \"Completed\") and a progress bar showing the number of completed jobs. Open the task with klick on \"Open\". For each task (or video) multiple jobs with a certain amounts of frames from the video are defined. Each job has a status (\"annotation\", \"validation\" and \"completed\"), an \"assignee\" (responsible for annotation) and a \"reviewer\" (responsible for reviewing the assignees annotation). Open the job with klick on \"Job #...\".","title":"Projects, Tasks and Jobs"},{"location":"OTLabels/annotation/cvat/#import-datafix","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Import datafix"},{"location":"OTLabels/annotation/cvat/#workflow","text":"For each job, the project coordinator assigns the roles of the \"assignee\" (= annotator) and the \"reviewer\". The reviewer then starts working on a job by initial labeling (see Annotation ) of the video frames. The assignee should save his labeling work from time to time and can also stop and later repeat working on a job. After finishing the initial labeling, the assignee requests a review (menu - request review, the assigned reviewer should already be named there) and thus sets status of the job to \"validation\". Then the reviewer checks the initial labeling done by the assignee and raises issues when there is a problem or question. After reviewing all video frames of the job, the reviewer submits the review (see see Review by either accepting and completing the job or rejecting it (which sets the status of the job bach to \"Annotation\"). Now the assignee has to check the issues, update the labels, resolve the issus and again request a review. The review process starts again and if necessary also next annotations, until the reviewer accepts the job and it is marked as \"Completed\". We recommend the following procedure for annotation in CVAT: Go through all pre-labeled objects on the right Check and delete false positive labels (if there is no object) Check and delete duplicate labels for the same object Check and correct object class for target classes Check and delete objects from other classes if they are classified wrong Zoom in on one quadrant of the image at a time: Check and correct position of object\u00b4s 2D boxes Check if there are objects from the target classes not already pre-labeled, draw new 2D box and annotate object class","title":"Workflow"},{"location":"OTLabels/annotation/cvat/#download-data","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Download data"},{"location":"OTLabels/annotation/preprocessingdata/","text":"Preprocessing Data \u00b6 Before retraining the YOLOv5 model, you might want to preprocess your labeled data to ensure that your frames are in the right format. Convert CVAT output to COCO \u00b6 OTLabels provides the cvat_to_coco.py script to convert the annotated data to the appropriate format YOLOv5 needs for retraining the model. Before executing the script, you have to setup the configurations for your needs: Within the script: Set destPath: path where your data is stored (typically ./data/*). Set annFile: path to the COCO-annotation file (if you do not have it yet, see below how to get it). Set catFile: path to a textfile containing your CVAT labels (standard: labels_CVAT.txt). Set cvatFile: path to a CVAT output file (containing images and lables, see also section CVAT : Download data) or to a folder containing multiple CVAT output files. Set name: the name of subfolder of destPath/images and destPath/labels to store the data in. labels_CVAT.txt: a textfile with two columns seperated by acutal commas with headers named Cat and CatId containing the name and the ID of your CVAT labels. For a exapmple, see the labels_CVAT.txt file in the OTLabels repository. Please note that labels not provided in this file will not be converted and consequently be deleted . The script performs the following steps: Unzip the CVAT output file Copy the images to the directory destPath/images/name . Import the label files and convert the CVAT labels to the COCO labels used by YOLOv5. Export the converted label files to the directory destPath/labels/name . Filter the labels \u00b6 If for some reason you want to filter the labels that are already stored in your data directory, you can use the filter_labels.py script. Before executing the script, you have to setup the configurations for your needs: Within the script: Set path: path where the data is stored (typically ./data/*). Set name: the name of one or more subfolder of destPath/images and destPath/lables to store the data in. More than one name must be provided as list. Set annFile: path to the COCO-annotation file (if you do not have it yet, see below how to get it). Set labelsFilter: path to a text file (standard: label_filter.txt) containing the labels you want to keep. label_filter.txt: a textfile containing the category names (e.g., \"car\") of the labels you want to keep (without quotes and one category name per row). Please note that the images will not be copied to the corresponding folder in destPath/images in order to save file space. Instead, a textfile with all images still containing the filtered labels is created. This file of file names can also be referred to by YOLOv5. The script performs the following steps: Get the category ids to the corresponding category name. Import the label files. Filter the labels by the provided category names. Export the lables to the directory destPath/labels/name_filtered . Create a textfile with all image files in the directory path . Please note that images and label files not including any label after filtering are not exported . Get COCO annotation file \u00b6 This file is necessary to ensure your labels match the labels of the pretrained YOLOv5 models. Use the get_coco_annotation_files.py script to download one or more COCO annotation files. Configurations to be made: Within the script: Set path: path where the data is stored (typically ./data/*). Set URLFile: path to the config file that stores the URLs of the annotation files coco_annotation_json_URLs.txt: a textfile containing the URLS of the annotation files (without quotes and one URL per row) Get the original COCO dataset \u00b6 In some cases you might want to get the original COCO dataset that was used to train the original YOLOv5 models. Therefore, OTLabels provides the get_coco_data.py script to download the data. Please note that you could also use the get_coco.sh script from the YOLOv5 repository . Before executing the script, you have to setup the configurations for your needs: Within the script: Set imageURLs: path of a textfile (standard: coco_image_URLs.txt) containing the URLs of the image data sets. Set annURL: URL to the labels. Set destPath: path where your data is stored (typically ./data/*). coco_image_URLs.txt: a textfile containing the URLS of the images for training, validation and testing (without quotes and one URL per row).","title":"Preprocessing Data"},{"location":"OTLabels/annotation/preprocessingdata/#preprocessing-data","text":"Before retraining the YOLOv5 model, you might want to preprocess your labeled data to ensure that your frames are in the right format.","title":"Preprocessing Data"},{"location":"OTLabels/annotation/preprocessingdata/#convert-cvat-output-to-coco","text":"OTLabels provides the cvat_to_coco.py script to convert the annotated data to the appropriate format YOLOv5 needs for retraining the model. Before executing the script, you have to setup the configurations for your needs: Within the script: Set destPath: path where your data is stored (typically ./data/*). Set annFile: path to the COCO-annotation file (if you do not have it yet, see below how to get it). Set catFile: path to a textfile containing your CVAT labels (standard: labels_CVAT.txt). Set cvatFile: path to a CVAT output file (containing images and lables, see also section CVAT : Download data) or to a folder containing multiple CVAT output files. Set name: the name of subfolder of destPath/images and destPath/labels to store the data in. labels_CVAT.txt: a textfile with two columns seperated by acutal commas with headers named Cat and CatId containing the name and the ID of your CVAT labels. For a exapmple, see the labels_CVAT.txt file in the OTLabels repository. Please note that labels not provided in this file will not be converted and consequently be deleted . The script performs the following steps: Unzip the CVAT output file Copy the images to the directory destPath/images/name . Import the label files and convert the CVAT labels to the COCO labels used by YOLOv5. Export the converted label files to the directory destPath/labels/name .","title":"Convert CVAT output to COCO"},{"location":"OTLabels/annotation/preprocessingdata/#filter-the-labels","text":"If for some reason you want to filter the labels that are already stored in your data directory, you can use the filter_labels.py script. Before executing the script, you have to setup the configurations for your needs: Within the script: Set path: path where the data is stored (typically ./data/*). Set name: the name of one or more subfolder of destPath/images and destPath/lables to store the data in. More than one name must be provided as list. Set annFile: path to the COCO-annotation file (if you do not have it yet, see below how to get it). Set labelsFilter: path to a text file (standard: label_filter.txt) containing the labels you want to keep. label_filter.txt: a textfile containing the category names (e.g., \"car\") of the labels you want to keep (without quotes and one category name per row). Please note that the images will not be copied to the corresponding folder in destPath/images in order to save file space. Instead, a textfile with all images still containing the filtered labels is created. This file of file names can also be referred to by YOLOv5. The script performs the following steps: Get the category ids to the corresponding category name. Import the label files. Filter the labels by the provided category names. Export the lables to the directory destPath/labels/name_filtered . Create a textfile with all image files in the directory path . Please note that images and label files not including any label after filtering are not exported .","title":"Filter the labels"},{"location":"OTLabels/annotation/preprocessingdata/#get-coco-annotation-file","text":"This file is necessary to ensure your labels match the labels of the pretrained YOLOv5 models. Use the get_coco_annotation_files.py script to download one or more COCO annotation files. Configurations to be made: Within the script: Set path: path where the data is stored (typically ./data/*). Set URLFile: path to the config file that stores the URLs of the annotation files coco_annotation_json_URLs.txt: a textfile containing the URLS of the annotation files (without quotes and one URL per row)","title":"Get COCO annotation file"},{"location":"OTLabels/annotation/preprocessingdata/#get-the-original-coco-dataset","text":"In some cases you might want to get the original COCO dataset that was used to train the original YOLOv5 models. Therefore, OTLabels provides the get_coco_data.py script to download the data. Please note that you could also use the get_coco.sh script from the YOLOv5 repository . Before executing the script, you have to setup the configurations for your needs: Within the script: Set imageURLs: path of a textfile (standard: coco_image_URLs.txt) containing the URLs of the image data sets. Set annURL: URL to the labels. Set destPath: path where your data is stored (typically ./data/*). coco_image_URLs.txt: a textfile containing the URLS of the images for training, validation and testing (without quotes and one URL per row).","title":"Get the original COCO dataset"},{"location":"OTLabels/gettingstarted/data/","text":"Data \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Data"},{"location":"OTLabels/gettingstarted/data/#data","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Data"},{"location":"OTLabels/gettingstarted/installation/","text":"Installation \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Installation"},{"location":"OTLabels/gettingstarted/installation/#installation","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Installation"},{"location":"OTLabels/gettingstarted/requirements/","text":"Requirements \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Requirements"},{"location":"OTLabels/gettingstarted/requirements/#requirements","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Requirements"},{"location":"OTLabels/models/coco6/","text":"COCO 6-class \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"COCO 6-class"},{"location":"OTLabels/models/coco6/#coco-6-class","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"COCO 6-class"},{"location":"OTLabels/training/extendmodel/","text":"Extend Model \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Extend Model"},{"location":"OTLabels/training/extendmodel/#extend-model","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Extend Model"},{"location":"OTLabels/training/newmodel/","text":"New Model \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"New Model"},{"location":"OTLabels/training/newmodel/#new-model","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"New Model"},{"location":"OTLabels/training/performancemeasures/","text":"Performance Measures \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Performance Measures"},{"location":"OTLabels/training/performancemeasures/#performance-measures","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Performance Measures"},{"location":"OTVision/","text":"Features \u00b6 The heart of OpenTrafficCam. OTVision , is a collection of algorithms to generate trajectories of road users in traffic videos. Most of the algorithms have been developed in previous open source projects. We modify and link them to a working pipeline. Key features \u00b6 Automated video analysis Detection using neural networks Can be used without programming knowledge Result: Trajectories in image and world coordinates Necessary: Windows computer with powerful graphics card (approx. 2.500 \u20ac) Content of this documentation \u00b6 Conversion of h264 video files to other formats ( ffmpeg ) Detection (joint localization and classification) of road users in single frames ( YOLOv5 by Jocher/Ultralytics ) Tracking of detected road users between single frames ( IOU-Tracker by Bochinski et al. ) Correction of trajectories for lens distortion ( OpenCV-Library ) Transformation of trajectories to world coordinates ( OpenCV-Library )","title":"Features"},{"location":"OTVision/#features","text":"The heart of OpenTrafficCam. OTVision , is a collection of algorithms to generate trajectories of road users in traffic videos. Most of the algorithms have been developed in previous open source projects. We modify and link them to a working pipeline.","title":"Features"},{"location":"OTVision/#key-features","text":"Automated video analysis Detection using neural networks Can be used without programming knowledge Result: Trajectories in image and world coordinates Necessary: Windows computer with powerful graphics card (approx. 2.500 \u20ac)","title":"Key features"},{"location":"OTVision/#content-of-this-documentation","text":"Conversion of h264 video files to other formats ( ffmpeg ) Detection (joint localization and classification) of road users in single frames ( YOLOv5 by Jocher/Ultralytics ) Tracking of detected road users between single frames ( IOU-Tracker by Bochinski et al. ) Correction of trajectories for lens distortion ( OpenCV-Library ) Transformation of trajectories to world coordinates ( OpenCV-Library )","title":"Content of this documentation"},{"location":"OTVision/gettingstarted/firstuse/","text":"First Use \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"First Use"},{"location":"OTVision/gettingstarted/firstuse/#first-use","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"First Use"},{"location":"OTVision/gettingstarted/installation/","text":"Installation \u00b6 Install Python 3.9 \u00b6 Install the 64-bit version Python 3.9.x via Windows installer from www.python.org/downloads/ as follows (Python 3.6.x to 3.8.x should also work, the 32-bit version is not supported): Install NVIDIA Cuda 11.0 \u00b6 If you have a modern NVidia graphics card. Download and install the latest version of the NVIDIA Cuda Toolkit with default settings from the NVIDIA toolkit. Install OTVision and OTAnalytics \u00b6 Download and unzip the latest versions of OTVision and OTAnalytics from Github. In both folders Double-click the \"install.bat\" and wait until the installation of the dependencies is complete. To run OTVision and OTAnalytics \u00b6 ... double click the \"OTVision.bat\" or the \"OTAnalytics.bat\" to run the Software via graphical user interface. If you encounter problems \u00b6 Maybe you also have to manually install Microsoft Visual C++ 14.0 or greater from the Visual Studio Build Tools . For further problems please open an issue in the OTVision or OTAnalytics repositories on Github.","title":"Installation"},{"location":"OTVision/gettingstarted/installation/#installation","text":"","title":"Installation"},{"location":"OTVision/gettingstarted/installation/#install-python-39","text":"Install the 64-bit version Python 3.9.x via Windows installer from www.python.org/downloads/ as follows (Python 3.6.x to 3.8.x should also work, the 32-bit version is not supported):","title":"Install Python 3.9"},{"location":"OTVision/gettingstarted/installation/#install-nvidia-cuda-110","text":"If you have a modern NVidia graphics card. Download and install the latest version of the NVIDIA Cuda Toolkit with default settings from the NVIDIA toolkit.","title":"Install NVIDIA Cuda 11.0"},{"location":"OTVision/gettingstarted/installation/#install-otvision-and-otanalytics","text":"Download and unzip the latest versions of OTVision and OTAnalytics from Github. In both folders Double-click the \"install.bat\" and wait until the installation of the dependencies is complete.","title":"Install OTVision and OTAnalytics"},{"location":"OTVision/gettingstarted/installation/#to-run-otvision-and-otanalytics","text":"... double click the \"OTVision.bat\" or the \"OTAnalytics.bat\" to run the Software via graphical user interface.","title":"To run OTVision and OTAnalytics"},{"location":"OTVision/gettingstarted/installation/#if-you-encounter-problems","text":"Maybe you also have to manually install Microsoft Visual C++ 14.0 or greater from the Visual Studio Build Tools . For further problems please open an issue in the OTVision or OTAnalytics repositories on Github.","title":"If you encounter problems"},{"location":"OTVision/gettingstarted/requirements/","text":"Requirements \u00b6 Hardware prerequisites \u00b6 Both OTVision and OTAnalytics run on modern 64 bit Windows 10 desktops and laptops (e.g. i5 processor and 8 GB RAM). However, if you want to detect road users in videos with OTVision on a regular basis, we strongly recommend purchasing a powerful desktop computer (>= i7, >= 64 GB RAM) with a modern NVidia graphics card (>= GeForce 3070). Make sure that the drivers of the graphics card are installed.","title":"Requirements"},{"location":"OTVision/gettingstarted/requirements/#requirements","text":"","title":"Requirements"},{"location":"OTVision/gettingstarted/requirements/#hardware-prerequisites","text":"Both OTVision and OTAnalytics run on modern 64 bit Windows 10 desktops and laptops (e.g. i5 processor and 8 GB RAM). However, if you want to detect road users in videos with OTVision on a regular basis, we strongly recommend purchasing a powerful desktop computer (>= i7, >= 64 GB RAM) with a modern NVidia graphics card (>= GeForce 3070). Make sure that the drivers of the graphics card are installed.","title":"Hardware prerequisites"},{"location":"OTVision/objectdetection/convert/","text":"Convert \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Convert"},{"location":"OTVision/objectdetection/convert/#convert","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Convert"},{"location":"OTVision/objectdetection/detect/","text":"Detect \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Detect"},{"location":"OTVision/objectdetection/detect/#detect","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Detect"},{"location":"OTVision/tracking/track/","text":"Track \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Track"},{"location":"OTVision/tracking/track/#track","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Track"},{"location":"OTVision/tracking/transform/","text":"Transform \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Transform"},{"location":"OTVision/tracking/transform/#transform","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Transform"},{"location":"OTVision/tracking/undistort/","text":"Undistort \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Undistort"},{"location":"OTVision/tracking/undistort/#undistort","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Undistort"},{"location":"contribute/","text":"Getting Started \u00b6 Become a part of Open Traffic Cam We are happy if you contribute your own code to OpenTrafficCam. This can be bug fixes in the current code, or even completely new stuff. It is best to start at GitHub to get a first overview. We are also happy to receive reports about the experience using OpenTrafficCam. If you find a bug in the software or the documentation, please report it as an issue in the corresponding repository on GitHub or via email. If you have any questions, feel free to send us an email .","title":"Getting Started"},{"location":"contribute/#getting-started","text":"Become a part of Open Traffic Cam We are happy if you contribute your own code to OpenTrafficCam. This can be bug fixes in the current code, or even completely new stuff. It is best to start at GitHub to get a first overview. We are also happy to receive reports about the experience using OpenTrafficCam. If you find a bug in the software or the documentation, please report it as an issue in the corresponding repository on GitHub or via email. If you have any questions, feel free to send us an email .","title":"Getting Started"},{"location":"contribute/coding/","text":"Coding (Python) \u00b6 Coding standards are not about right and wrong or good and bad. They are about uniformity, so people can easily read and contribute to the code regardless who worked on the software before. For now, nearly all code is Python so we just agreed on coding conventions for the Python programming language. We basically follow the PEP 8 Style Guide for Python Code . A good summary and some additions can be found in the Hitchhikers Guide or at RealPython.com . Below we defined what we consider the most important standards. Naming Conventions \u00b6 All names should be as short as possible but as long as necessary to understand them. General \u00b6 The following table from RealPython.com summarizes the PEP8 naming conventions: Type Naming Convention Examples Function Use a lowercase word or words. Separate words by underscores to improve readability. function, my_function Variable Use a lowercase single letter, word, or words. Separate words with underscores to improve readability. x, var, my_variable Class Start each word with a capital letter. Do not separate words with underscores. This style is called camel case. Model, MyClass Method Use a lowercase word or words. Separate words with underscores to improve readability. class_method, method Constant Use an uppercase single letter, word, or words. Separate words with underscores to improve readability. CONSTANT, MY_CONSTANT, MY_LONG_CONSTANT Module Use a short, lowercase word or words. Separate words with underscores to improve readability. module.py, my_module.py Package Use a short, lowercase word or words. Do not separate words with underscores. package, mypackage Files, Folder, Dirs \u00b6 Naming Convention Examples dir D:\\tmp\\ filename testfile suffix _detections filetype .csv file D:\\tmp\\testfile_detections.csv path \"D:\\tmp\\\" or \"D:\\tmp\\testfile_detections.csv\" Suffix \u00b6 Files get a suffix according to their content. Suffix Description _detections detections as bounding boxes _tracks-px trajectories in pixel coordinates _tracks-corr trajectories in pixel coordinates corrected for lens distortion _tracks-utm trajectories transformed to UTM coordinates _refpts reference points to convert pixel to UTM coordinates Code documentation \u00b6 Each module, function, class or method should be described in a docstring ( Google style ) Docstrings for Modules \u00b6 Each file should start with the license snippet followed by a docstring describing the contents and usage of the module: \"\"\"A one line summary of the module or program, terminated by a period. Leave one blank line. The rest of this docstring should contain an overall description of the module or program. Optionally, it may also contain a brief description of exported classes and functions and/or usage examples. Typical usage example: foo = ClassFoo() bar = foo.FunctionBar() \"\"\" Docstrings for Functions \u00b6 Each function must be described by a docstring: def hello_world ( message : str = \"Hello World\" , author : str = \"Santa Claus\" ): \"\"\"Deliver a message from an author to the world. \"author says message\" Args: message (str, optional): Message to deliver. Defaults to \"Hello World\". author (str, optional): Author name. Defaults to \"Santa Claus\". Returns: str: the message said \"\"\" msg = author + \" says \" + message print ( msg ) return msg Note If you are using VS Code, you may want to use the Python Docstring Generator extension. Comments \u00b6 If it is necessary to explain your code in between, use single line or block comments for some (or all) code that follows: # This is an example for a single line comment # This is an # example for a # block comment Try to avoid inline comments. Dependencies \u00b6 We try to develop OpenTrafficCam using as few dependencies as possible. For basic functionality we prefer packages that come with the Python standard distribution (like pathlib). However, OpenTrafficCam would not be possible without code from third party libraries, especially when it comes to videos, images, data analysis, neural networks or GUI. These are the libraries we intend to use for specific functionalities throughout the whole framework (we are grateful to the authors): Functionality Library Paths pathlib (distributed with Python since 3.4) Videos, Images OpenCV Neural networks PyTorch , torchvision Data handling pandas , NumPy Shape handling GeoPandas , Shapely Plotting, graphing Plotly User interface PySimpleGui Web PySimpleGUIWeb (camera system), Dash (dashboards) Linting/Autoformatting \u00b6 To match PEP8 specs and write pretty code we use flake8 to check against structural and style errors and black for autoformatting. Additionally the package flake8-docstrings can be used to lint the docstrings. The following settings are set in the .flake8 file: [flake8] max-line-length = 88 docstring-convention = google","title":"Coding (Python)"},{"location":"contribute/coding/#coding-python","text":"Coding standards are not about right and wrong or good and bad. They are about uniformity, so people can easily read and contribute to the code regardless who worked on the software before. For now, nearly all code is Python so we just agreed on coding conventions for the Python programming language. We basically follow the PEP 8 Style Guide for Python Code . A good summary and some additions can be found in the Hitchhikers Guide or at RealPython.com . Below we defined what we consider the most important standards.","title":"Coding (Python)"},{"location":"contribute/coding/#naming-conventions","text":"All names should be as short as possible but as long as necessary to understand them.","title":"Naming Conventions"},{"location":"contribute/coding/#general","text":"The following table from RealPython.com summarizes the PEP8 naming conventions: Type Naming Convention Examples Function Use a lowercase word or words. Separate words by underscores to improve readability. function, my_function Variable Use a lowercase single letter, word, or words. Separate words with underscores to improve readability. x, var, my_variable Class Start each word with a capital letter. Do not separate words with underscores. This style is called camel case. Model, MyClass Method Use a lowercase word or words. Separate words with underscores to improve readability. class_method, method Constant Use an uppercase single letter, word, or words. Separate words with underscores to improve readability. CONSTANT, MY_CONSTANT, MY_LONG_CONSTANT Module Use a short, lowercase word or words. Separate words with underscores to improve readability. module.py, my_module.py Package Use a short, lowercase word or words. Do not separate words with underscores. package, mypackage","title":"General"},{"location":"contribute/coding/#files-folder-dirs","text":"Naming Convention Examples dir D:\\tmp\\ filename testfile suffix _detections filetype .csv file D:\\tmp\\testfile_detections.csv path \"D:\\tmp\\\" or \"D:\\tmp\\testfile_detections.csv\"","title":"Files, Folder, Dirs"},{"location":"contribute/coding/#suffix","text":"Files get a suffix according to their content. Suffix Description _detections detections as bounding boxes _tracks-px trajectories in pixel coordinates _tracks-corr trajectories in pixel coordinates corrected for lens distortion _tracks-utm trajectories transformed to UTM coordinates _refpts reference points to convert pixel to UTM coordinates","title":"Suffix"},{"location":"contribute/coding/#code-documentation","text":"Each module, function, class or method should be described in a docstring ( Google style )","title":"Code documentation"},{"location":"contribute/coding/#docstrings-for-modules","text":"Each file should start with the license snippet followed by a docstring describing the contents and usage of the module: \"\"\"A one line summary of the module or program, terminated by a period. Leave one blank line. The rest of this docstring should contain an overall description of the module or program. Optionally, it may also contain a brief description of exported classes and functions and/or usage examples. Typical usage example: foo = ClassFoo() bar = foo.FunctionBar() \"\"\"","title":"Docstrings for Modules"},{"location":"contribute/coding/#docstrings-for-functions","text":"Each function must be described by a docstring: def hello_world ( message : str = \"Hello World\" , author : str = \"Santa Claus\" ): \"\"\"Deliver a message from an author to the world. \"author says message\" Args: message (str, optional): Message to deliver. Defaults to \"Hello World\". author (str, optional): Author name. Defaults to \"Santa Claus\". Returns: str: the message said \"\"\" msg = author + \" says \" + message print ( msg ) return msg Note If you are using VS Code, you may want to use the Python Docstring Generator extension.","title":"Docstrings for Functions"},{"location":"contribute/coding/#comments","text":"If it is necessary to explain your code in between, use single line or block comments for some (or all) code that follows: # This is an example for a single line comment # This is an # example for a # block comment Try to avoid inline comments.","title":"Comments"},{"location":"contribute/coding/#dependencies","text":"We try to develop OpenTrafficCam using as few dependencies as possible. For basic functionality we prefer packages that come with the Python standard distribution (like pathlib). However, OpenTrafficCam would not be possible without code from third party libraries, especially when it comes to videos, images, data analysis, neural networks or GUI. These are the libraries we intend to use for specific functionalities throughout the whole framework (we are grateful to the authors): Functionality Library Paths pathlib (distributed with Python since 3.4) Videos, Images OpenCV Neural networks PyTorch , torchvision Data handling pandas , NumPy Shape handling GeoPandas , Shapely Plotting, graphing Plotly User interface PySimpleGui Web PySimpleGUIWeb (camera system), Dash (dashboards)","title":"Dependencies"},{"location":"contribute/coding/#lintingautoformatting","text":"To match PEP8 specs and write pretty code we use flake8 to check against structural and style errors and black for autoformatting. Additionally the package flake8-docstrings can be used to lint the docstrings. The following settings are set in the .flake8 file: [flake8] max-line-length = 88 docstring-convention = google","title":"Linting/Autoformatting"},{"location":"contribute/documentation/","text":"Documentation \u00b6 You can help correcting, improving and extending this documentation by clicking on the \"edit\" button on top right of each page that brings you directly to the corresponding file in the documentation repository on Github. You can edit the page using the rather simple markdown language . Then you commit your changes by choosing a meaningful commit meassage and the \"Create a new branch for this commit and start a pull request\" option. After internal review and discussion (if necessery) we will merge your pull request into the documentation repository and then it will be also visible on the website. If you want to create new pages, have a look at the README file of the documentation repository . There you will also find a description of how this website is rendered and how inserting a new page into the navigation works. The documentation website of OpenTrafficCam is generated by MkDocs using the awesome material theme. The documentation is fully written in Markdown .","title":"Documentation"},{"location":"contribute/documentation/#documentation","text":"You can help correcting, improving and extending this documentation by clicking on the \"edit\" button on top right of each page that brings you directly to the corresponding file in the documentation repository on Github. You can edit the page using the rather simple markdown language . Then you commit your changes by choosing a meaningful commit meassage and the \"Create a new branch for this commit and start a pull request\" option. After internal review and discussion (if necessery) we will merge your pull request into the documentation repository and then it will be also visible on the website. If you want to create new pages, have a look at the README file of the documentation repository . There you will also find a description of how this website is rendered and how inserting a new page into the navigation works. The documentation website of OpenTrafficCam is generated by MkDocs using the awesome material theme. The documentation is fully written in Markdown .","title":"Documentation"},{"location":"contribute/github/","text":"GitHub \u00b6 All code of OpenTrafficCam is hosted on GitHub . For each core module a separate repository exists: OTCamera OTVision OTAnalytics OTLabels Following the Feature Branch Workflow each repository has a master branch, which at all times should contain a stable version of the software. All coding should be committed to separate feature branches. Once you completed coding on a feature, issue or bugfix, you create a pull request for the respective feature branch (preferably labeled using the GitHub labeling system ). Your contributed code will then be reviewed by the main developers, discussed (if necessary) and merged into the master branch. Even without coding you can contribute to the project: If you encounter any bug or got any idea for enhancement, feel free to open an issue in the corresponding repository. Every issue should at least have two labels assigned: One label indicating the maturaty of the issue ( question , bug , feature or idea ) and one assigning the repository-specific subpackage or section ( black colored labels ). All pull request will be checked by GitHub's super-linter .","title":"GitHub"},{"location":"contribute/github/#github","text":"All code of OpenTrafficCam is hosted on GitHub . For each core module a separate repository exists: OTCamera OTVision OTAnalytics OTLabels Following the Feature Branch Workflow each repository has a master branch, which at all times should contain a stable version of the software. All coding should be committed to separate feature branches. Once you completed coding on a feature, issue or bugfix, you create a pull request for the respective feature branch (preferably labeled using the GitHub labeling system ). Your contributed code will then be reviewed by the main developers, discussed (if necessary) and merged into the master branch. Even without coding you can contribute to the project: If you encounter any bug or got any idea for enhancement, feel free to open an issue in the corresponding repository. Every issue should at least have two labels assigned: One label indicating the maturaty of the issue ( question , bug , feature or idea ) and one assigning the repository-specific subpackage or section ( black colored labels ). All pull request will be checked by GitHub's super-linter .","title":"GitHub"},{"location":"contribute/gui/","text":"GUI \u00b6 For now we decided on using PysimpleGUI to build the OpenTrafficCam user interface, as it is lightweight and easy to start with. Whenever possible, we try to separate the gui code from all other code. All the gui modules have a place in the \"gui\" subpackage of the respective OT repository: OTCamera/OTCamera/gui OTVision/OTVision/gui OTAnalytics/OTAnalytics/gui For the future we can also imagine using a web framework for user interaction. This would also help to enable remote access to local GPU machines or high performance computing centers.","title":"GUI"},{"location":"contribute/gui/#gui","text":"For now we decided on using PysimpleGUI to build the OpenTrafficCam user interface, as it is lightweight and easy to start with. Whenever possible, we try to separate the gui code from all other code. All the gui modules have a place in the \"gui\" subpackage of the respective OT repository: OTCamera/OTCamera/gui OTVision/OTVision/gui OTAnalytics/OTAnalytics/gui For the future we can also imagine using a web framework for user interaction. This would also help to enable remote access to local GPU machines or high performance computing centers.","title":"GUI"},{"location":"contribute/otcamera/","text":"OTCamera Dev Version \u00b6 The Raspberry Pi Zero is wonderful to use outside (it consumes little power, is cheap, ...). But the development of OTCamera is more fun on a big Pi. Everything is much faster. You will need a prepared Raspberry Pi 2B/3B(+)/4 (the 2 GB works well) Camera module (no USB webcam) Setup VS Code Remote Development Extension \u00b6 Install the Remote-SSH extension on your desktop using the marketplace. Add the Pi as remote host . Connect to the Pi using the Remote-SSH extension (rightclick on the ne host - \"Connect to Host in New Window\"). When asked for the operating system of the host, choose \"Linux\". VS code will download and install the necessary dependencies on the Pi. Open the extension manager in the new windows an install all necessary extensions. Install the linter and autoformatter. pip install black flake8 Setup Git and GitHub \u00b6 Install git using apt. sudo apt install git -y To setup your git commit name and email, login to your github account and copy your private commit email . On the pi run git config --global user.name \"Your Name\" git config --global user.email \"123456+username@users.noreply.github.com\" The easiest way to setup your GitHub credentials is to use vs code. In the file explorer panel click \"Clone Repository\" and choose \"Clone from GitHub\". Depending on your desktop computer settings, a browser tab will open to login into your GitHub account. Afterwards, you can search for \"OpenTrafficCam/OTCamera\" inside the vs code command prompt and select a folder to pull (choose /home/pi by default). Install Screen to Run OTCamera in Background \u00b6 Quote Screen is a full-screen window manager that multiplexes a physical terminal between several processes, typically interactive shells. To install screen on the developer pi just run sudo apt install screen -y You can now start screen and use it as an usual terminal. You can run the python OTCamera to start the camera in an active screen session. Hit Ctrl + A and then D to detach the current session. Screen will still run in background including the OTCamera. If you start screen including the -r option you can reconnect to this terminal session: screen -r You can safely disconnect you ssh connection to the pi, while the screen session is still running. Ready to Develop \u00b6 You should now be ready to pull the OTCamera repository and start developing.","title":"OTCamera Dev Version"},{"location":"contribute/otcamera/#otcamera-dev-version","text":"The Raspberry Pi Zero is wonderful to use outside (it consumes little power, is cheap, ...). But the development of OTCamera is more fun on a big Pi. Everything is much faster. You will need a prepared Raspberry Pi 2B/3B(+)/4 (the 2 GB works well) Camera module (no USB webcam)","title":"OTCamera Dev Version"},{"location":"contribute/otcamera/#setup-vs-code-remote-development-extension","text":"Install the Remote-SSH extension on your desktop using the marketplace. Add the Pi as remote host . Connect to the Pi using the Remote-SSH extension (rightclick on the ne host - \"Connect to Host in New Window\"). When asked for the operating system of the host, choose \"Linux\". VS code will download and install the necessary dependencies on the Pi. Open the extension manager in the new windows an install all necessary extensions. Install the linter and autoformatter. pip install black flake8","title":"Setup VS Code Remote Development Extension"},{"location":"contribute/otcamera/#setup-git-and-github","text":"Install git using apt. sudo apt install git -y To setup your git commit name and email, login to your github account and copy your private commit email . On the pi run git config --global user.name \"Your Name\" git config --global user.email \"123456+username@users.noreply.github.com\" The easiest way to setup your GitHub credentials is to use vs code. In the file explorer panel click \"Clone Repository\" and choose \"Clone from GitHub\". Depending on your desktop computer settings, a browser tab will open to login into your GitHub account. Afterwards, you can search for \"OpenTrafficCam/OTCamera\" inside the vs code command prompt and select a folder to pull (choose /home/pi by default).","title":"Setup Git and GitHub"},{"location":"contribute/otcamera/#install-screen-to-run-otcamera-in-background","text":"Quote Screen is a full-screen window manager that multiplexes a physical terminal between several processes, typically interactive shells. To install screen on the developer pi just run sudo apt install screen -y You can now start screen and use it as an usual terminal. You can run the python OTCamera to start the camera in an active screen session. Hit Ctrl + A and then D to detach the current session. Screen will still run in background including the OTCamera. If you start screen including the -r option you can reconnect to this terminal session: screen -r You can safely disconnect you ssh connection to the pi, while the screen session is still running.","title":"Install Screen to Run OTCamera in Background"},{"location":"contribute/otcamera/#ready-to-develop","text":"You should now be ready to pull the OTCamera repository and start developing.","title":"Ready to Develop"},{"location":"contribute/vscode/","text":"VS Code \u00b6 We are developing OpenTrafficCam using Visual Studio Code using the following extensions: GitHub Pull Requests and Issues markdownlint Pylance Python Python Docstring Generator Pylance Settings \u00b6 To solve import errors in the repository, we need to add an extra path to the pylance config. For example in the workspace settings of OTCamera add: \"python.analysis.extraPaths\" : [ \"./OTCamera\" ] Or set it using the Settings-UI (Workspace -> Pylance -> Python - Analysis: Extra Path -> add './OTCamera') Snippets \u00b6 To comfortably insert code use snippets. You can open the configuration file for a specific language to add or modify the user snippets: Ctrl + Shift + p and Preferences: Configure User Snippets . You can insert the user snippets by start typing the \"prefix\" value. For example start typing gpl_ and autocompletection should recognize the user snippets stated below. GPL License Information \u00b6 Add the following snippet to vscodedata/user/snippets/python.json \"gpl_license_header\" : { \"prefix\" : \"gpl_add_head\" , \"body\" :[ \"$LINE_COMMENT ${1:Program Name and Function}\" , \"$LINE_COMMENT Copyright (C) $CURRENT_YEAR OpenTrafficCam Contributors\" , \"$LINE_COMMENT <https://github.com/OpenTrafficCam\" , \"$LINE_COMMENT <team@opentrafficcam.org>\" , \"$LINE_COMMENT\" , \"$LINE_COMMENT This program is free software: you can redistribute it and/or modify\" , \"$LINE_COMMENT it under the terms of the GNU General Public License as published by\" , \"$LINE_COMMENT the Free Software Foundation, either version 3 of the License, or\" , \"$LINE_COMMENT (at your option) any later version.\" , \"$LINE_COMMENT\" , \"$LINE_COMMENT This program is distributed in the hope that it will be useful,\" , \"$LINE_COMMENT but WITHOUT ANY WARRANTY; without even the implied warranty of\" , \"$LINE_COMMENT MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\" , \"$LINE_COMMENT GNU General Public License for more details.\" , \"$LINE_COMMENT\" , \"$LINE_COMMENT You should have received a copy of the GNU General Public License\" , \"$LINE_COMMENT along with this program. If not, see <https://www.gnu.org/licenses/>.\" , \"\" , ], \"description\" : \"Add GPLv3 license information in source code.\" } Add a short description and use tab to jump to the end of the snippet.","title":"VS Code"},{"location":"contribute/vscode/#vs-code","text":"We are developing OpenTrafficCam using Visual Studio Code using the following extensions: GitHub Pull Requests and Issues markdownlint Pylance Python Python Docstring Generator","title":"VS Code"},{"location":"contribute/vscode/#pylance-settings","text":"To solve import errors in the repository, we need to add an extra path to the pylance config. For example in the workspace settings of OTCamera add: \"python.analysis.extraPaths\" : [ \"./OTCamera\" ] Or set it using the Settings-UI (Workspace -> Pylance -> Python - Analysis: Extra Path -> add './OTCamera')","title":"Pylance Settings"},{"location":"contribute/vscode/#snippets","text":"To comfortably insert code use snippets. You can open the configuration file for a specific language to add or modify the user snippets: Ctrl + Shift + p and Preferences: Configure User Snippets . You can insert the user snippets by start typing the \"prefix\" value. For example start typing gpl_ and autocompletection should recognize the user snippets stated below.","title":"Snippets"},{"location":"contribute/vscode/#gpl-license-information","text":"Add the following snippet to vscodedata/user/snippets/python.json \"gpl_license_header\" : { \"prefix\" : \"gpl_add_head\" , \"body\" :[ \"$LINE_COMMENT ${1:Program Name and Function}\" , \"$LINE_COMMENT Copyright (C) $CURRENT_YEAR OpenTrafficCam Contributors\" , \"$LINE_COMMENT <https://github.com/OpenTrafficCam\" , \"$LINE_COMMENT <team@opentrafficcam.org>\" , \"$LINE_COMMENT\" , \"$LINE_COMMENT This program is free software: you can redistribute it and/or modify\" , \"$LINE_COMMENT it under the terms of the GNU General Public License as published by\" , \"$LINE_COMMENT the Free Software Foundation, either version 3 of the License, or\" , \"$LINE_COMMENT (at your option) any later version.\" , \"$LINE_COMMENT\" , \"$LINE_COMMENT This program is distributed in the hope that it will be useful,\" , \"$LINE_COMMENT but WITHOUT ANY WARRANTY; without even the implied warranty of\" , \"$LINE_COMMENT MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\" , \"$LINE_COMMENT GNU General Public License for more details.\" , \"$LINE_COMMENT\" , \"$LINE_COMMENT You should have received a copy of the GNU General Public License\" , \"$LINE_COMMENT along with this program. If not, see <https://www.gnu.org/licenses/>.\" , \"\" , ], \"description\" : \"Add GPLv3 license information in source code.\" } Add a short description and use tab to jump to the end of the snippet.","title":"GPL License Information"},{"location":"overview/dataprivacy/","text":"Data Privacy (Germany) \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Data Privacy (Germany)"},{"location":"overview/dataprivacy/#data-privacy-germany","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Data Privacy (Germany)"},{"location":"overview/gettingstarted/","text":"Getting Started \u00b6 All OTC Modules are integrated in a workflow The Framework of OpenTrafficCam's Core Functions","title":"Getting Started"},{"location":"overview/gettingstarted/#getting-started","text":"All OTC Modules are integrated in a workflow The Framework of OpenTrafficCam's Core Functions","title":"Getting Started"},{"location":"overview/usecases/trafficcounts/","text":"Traffic Counts \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Traffic Counts"},{"location":"overview/usecases/trafficcounts/#traffic-counts","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Traffic Counts"},{"location":"overview/usecases/trafficsafety/","text":"Traffic Safety \u00b6 The rest of the iceberg \u00b6 Traditionally, accident data has primarily been used to analyse the road safety of a traffic facility. However, this is a reactive approach, which means that accident occurrences must first be observed over many years before measures can be taken to improve road safety. In addition, the accident statistics show a certain number of unreported cases and there is only limited information on the sequence of events that led to the accident. Assuming that accidents are only the tip of the iceberg, traffic flow can also be observed and analysed in terms of conflicts or near misses in a complementary or supplementary manner. Near misses or traffic conflicts are defined by Amundsen and Hyd\u00e9n (1977) as \"observable situations in which two or more road users approach each other in space and time to such an extent that there is a risk of collision if their movements remain unchanged\". In a simplified way, the traffic flow can be divided into the following hierarchy of severity: Undisturbed passages - Interactions - Near misses/Conflicts - Accidents. The near misses (similar to accidents) can be divided into further severity levels with regard to the proximity to a collision, but also with regard to the severity of the resulting collision (similar to accident severity). PET - easy peasy \u00b6 A number of Surrogate Measures of Safety have been developed as objective indicators to classify an interaction as a near miss and to assess its severity (primarily in terms of proximity to a collision). One commonly used indicator is PET (Post Encroachment Time), which is a rather simple safety indicator. It describes the time period between the first road user leaving the common conflict area and the second one entering it. TTC - complex, but meaningful \u00b6 Another widely applied safety indicator in surrogate analysis is Time To Collision (TTC). It describes the remaining time to a collision between two road users at a certain point in time of their interaction and under the assumption that neither of the parties involved will take an evasive action. The lower the TTC, the more severe the conflict between road users. In case of a collision the TTC equals zero. A TTC can only be calculated if the two interacting road users are on a collision course. If the collision course is left due to an evasive action, the TTC for the time from the start of the evasive action is calculated on the basis of the hypotethic trajectories without the evasive action. Such hypotethic trajectories can be approximated by various methods of trajectory prediction. The simplest (but in some cases also the most unrealistic) method for trajectory prediction is to assume continued motion with constant velocity vectors. The TTC is calculated for each moment of interaction. This results in a TTC curve. Interactions and also periods of interactions are often categorised according to whether the TTC is below a threshold value (e.g. 1.5 sec). The Minimum TTC, the Time Exposed TTC and the Time Integrated TTC are used as aggregated indicators. In addition to road safety analysis, TTC is also used for active accident avoidance, especially for decision-making in algorithms of autonomous vehicles. Best practices \u00b6 In order to enable comparisons, e.g. with other traffic facilities, safety indicators like PET and TTC are related to suitable exposure units, such as traffic volume or the total number of interactions in the observation period. For comparisons one also needs statistcal significance of the frequency of conflicts at every individual location. To achieve this one must capture trajectories over a sufficient period of time. In most constellations, conflicts do occur more frequently than accidents that require observation periods of several years. However, recent studis show, that capturing videos over weeks or at least multiple days is the minimum required duration for conflict analyses. One mistake that also should be avoided is to raise the threshold for categorising encounters as conflicts, as this means measuring exposure, but not road safety. It\u00b4s all about validity \u00b6 Exposure based conflict frequencies can then be compared over different road sections or intersections or to known mean values. In addition, in the past, fixed factors for estimating the number of accidents were derived from the number of conflicts, which, however, could often not be confirmed by other researchers. To overcome fixed coefficients, a new approach taking into account variability of conflict severity based on Extreme Value Theory (EVT) is now state of the art, that has also been applied in stock market and flood protection earlier. Conflicts are even more valid with crash data when it comes to relative safety asssessment, e.g. ranking a number of road sites by traffic safety, which should deliver nearly the same results as using crash data from several years. This relative validity is sufficient for many applications in road safety work. Another advantage of the safety analysis of trajectories is that the entire movements of the road users leading to the conflict are known. For example, differences in movement patterns between conflicts and other encounters can be analyzed and suitable measures for increasing road safety can be derived from them. In order to avoid accidents, however, a certain process validity between conflicts and accidents is also required here. How widespread is the method? \u00b6 Several other European countries developed their own dedicated traffic conflict techniques many years ago and conflict analysis already plays an important role in their road safety assessment tools. In some other countries there appear to be concerns about the conflict technique from earlier days, as the severity of the conflicts was assessed by local human observers and was therefore largely subjective. Against the background of Vision Zero (zero traffic fatalities) and the developments in computer technology and AI, we advocate the use of proactive safety assessments based on the trajectories of road users. This can save lives and prevent many accidents. Nonetheless, there is still a need for further research into data collection, indicators, accident prediction and validation. In addition, clarification is required about the strengths and weaknesses of the conflict technology compared to the accident analysis and the need for further proactive safety tools. What\u00b4s next for us? \u00b6 Forthe next years it is planned to integrate the conflict analysis functionalities in OpenTrafficCam, to automate the processes as best as possible and still make them available to all non-programmers in order to enable both basic research and application studies for traffic experts.","title":"Traffic Safety"},{"location":"overview/usecases/trafficsafety/#traffic-safety","text":"","title":"Traffic Safety"},{"location":"overview/usecases/trafficsafety/#the-rest-of-the-iceberg","text":"Traditionally, accident data has primarily been used to analyse the road safety of a traffic facility. However, this is a reactive approach, which means that accident occurrences must first be observed over many years before measures can be taken to improve road safety. In addition, the accident statistics show a certain number of unreported cases and there is only limited information on the sequence of events that led to the accident. Assuming that accidents are only the tip of the iceberg, traffic flow can also be observed and analysed in terms of conflicts or near misses in a complementary or supplementary manner. Near misses or traffic conflicts are defined by Amundsen and Hyd\u00e9n (1977) as \"observable situations in which two or more road users approach each other in space and time to such an extent that there is a risk of collision if their movements remain unchanged\". In a simplified way, the traffic flow can be divided into the following hierarchy of severity: Undisturbed passages - Interactions - Near misses/Conflicts - Accidents. The near misses (similar to accidents) can be divided into further severity levels with regard to the proximity to a collision, but also with regard to the severity of the resulting collision (similar to accident severity).","title":"The rest of the iceberg"},{"location":"overview/usecases/trafficsafety/#pet-easy-peasy","text":"A number of Surrogate Measures of Safety have been developed as objective indicators to classify an interaction as a near miss and to assess its severity (primarily in terms of proximity to a collision). One commonly used indicator is PET (Post Encroachment Time), which is a rather simple safety indicator. It describes the time period between the first road user leaving the common conflict area and the second one entering it.","title":"PET - easy peasy"},{"location":"overview/usecases/trafficsafety/#ttc-complex-but-meaningful","text":"Another widely applied safety indicator in surrogate analysis is Time To Collision (TTC). It describes the remaining time to a collision between two road users at a certain point in time of their interaction and under the assumption that neither of the parties involved will take an evasive action. The lower the TTC, the more severe the conflict between road users. In case of a collision the TTC equals zero. A TTC can only be calculated if the two interacting road users are on a collision course. If the collision course is left due to an evasive action, the TTC for the time from the start of the evasive action is calculated on the basis of the hypotethic trajectories without the evasive action. Such hypotethic trajectories can be approximated by various methods of trajectory prediction. The simplest (but in some cases also the most unrealistic) method for trajectory prediction is to assume continued motion with constant velocity vectors. The TTC is calculated for each moment of interaction. This results in a TTC curve. Interactions and also periods of interactions are often categorised according to whether the TTC is below a threshold value (e.g. 1.5 sec). The Minimum TTC, the Time Exposed TTC and the Time Integrated TTC are used as aggregated indicators. In addition to road safety analysis, TTC is also used for active accident avoidance, especially for decision-making in algorithms of autonomous vehicles.","title":"TTC - complex, but meaningful"},{"location":"overview/usecases/trafficsafety/#best-practices","text":"In order to enable comparisons, e.g. with other traffic facilities, safety indicators like PET and TTC are related to suitable exposure units, such as traffic volume or the total number of interactions in the observation period. For comparisons one also needs statistcal significance of the frequency of conflicts at every individual location. To achieve this one must capture trajectories over a sufficient period of time. In most constellations, conflicts do occur more frequently than accidents that require observation periods of several years. However, recent studis show, that capturing videos over weeks or at least multiple days is the minimum required duration for conflict analyses. One mistake that also should be avoided is to raise the threshold for categorising encounters as conflicts, as this means measuring exposure, but not road safety.","title":"Best practices"},{"location":"overview/usecases/trafficsafety/#it-s-all-about-validity","text":"Exposure based conflict frequencies can then be compared over different road sections or intersections or to known mean values. In addition, in the past, fixed factors for estimating the number of accidents were derived from the number of conflicts, which, however, could often not be confirmed by other researchers. To overcome fixed coefficients, a new approach taking into account variability of conflict severity based on Extreme Value Theory (EVT) is now state of the art, that has also been applied in stock market and flood protection earlier. Conflicts are even more valid with crash data when it comes to relative safety asssessment, e.g. ranking a number of road sites by traffic safety, which should deliver nearly the same results as using crash data from several years. This relative validity is sufficient for many applications in road safety work. Another advantage of the safety analysis of trajectories is that the entire movements of the road users leading to the conflict are known. For example, differences in movement patterns between conflicts and other encounters can be analyzed and suitable measures for increasing road safety can be derived from them. In order to avoid accidents, however, a certain process validity between conflicts and accidents is also required here.","title":"It\u00b4s all about validity"},{"location":"overview/usecases/trafficsafety/#how-widespread-is-the-method","text":"Several other European countries developed their own dedicated traffic conflict techniques many years ago and conflict analysis already plays an important role in their road safety assessment tools. In some other countries there appear to be concerns about the conflict technique from earlier days, as the severity of the conflicts was assessed by local human observers and was therefore largely subjective. Against the background of Vision Zero (zero traffic fatalities) and the developments in computer technology and AI, we advocate the use of proactive safety assessments based on the trajectories of road users. This can save lives and prevent many accidents. Nonetheless, there is still a need for further research into data collection, indicators, accident prediction and validation. In addition, clarification is required about the strengths and weaknesses of the conflict technology compared to the accident analysis and the need for further proactive safety tools.","title":"How widespread is the method?"},{"location":"overview/usecases/trafficsafety/#what-s-next-for-us","text":"Forthe next years it is planned to integrate the conflict analysis functionalities in OpenTrafficCam, to automate the processes as best as possible and still make them available to all non-programmers in order to enable both basic research and application studies for traffic experts.","title":"What\u00b4s next for us?"},{"location":"overview/usecases/vehiclespeeds/","text":"Vehicle Speeds \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Vehicle Speeds"},{"location":"overview/usecases/vehiclespeeds/#vehicle-speeds","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Vehicle Speeds"}]}