{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to OpenTrafficCam","text":"<p>OpenTrafficCam makes analyzing traffic easier, faster and more efficient.</p> <p>Welcome to OpenTrafficCam, the only fully integrated open source workflow for video-based recording and automated analysis of road traffic.</p> <p>This site will cover all the documentation in one place.</p> <p>Check out our GitHub page for downloads and code.</p>"},{"location":"#motivation","title":"Motivation","text":"<p>Thanks to the rapid development of computer hardware and machine learning in the last decade, automatic object detection is state of the art in many business sectors. Road traffic, on the other hand, is still often surveyed manually at great expense. There exist tools that optimize some of the processing steps, but they can hardly be used without programming skills or they cause significant costs. This is why research and planning still have to be conducted with minimal data or sometimes without any at all. This lack of data primarily affects sustainable modes of transportation such as walking and cycling.</p> <p>OpenTrafficCam aims to address this gap for a stronger data base for transportation planning, policy and research. In doing so, we ourselves benefit substantially from other open source projects. Consequently, we also publish large parts of OpenTrafficCam as open source hardware and software under the GNU General Public License v3.0. This also helps in efficiently using limited public resources for road infrastructure planning and research and in avoiding substantial vendor locks. So all transport professionals and anyone else interested are very welcome to try OpenTrafficCam.</p> <p>You want to support?</p> <p>Developing, maintaining and organizing open source requires quite a lot of time and money. But fortunately, there are several ways to support us:</p> <ul> <li>Traffic engineers can help by spreading the word and submitting questions, bugs or feature ideas as issues     in the corresponding GitHub repositories of the modules     OTCamera,     OTVision,     OTAnalytics or     OTLabels.</li> <li>Researchers and developers can support by contributing code     or contacting us for scientific collaboration.</li> <li>Users, such as municipalities or engineering companies, can contact us to fund specific enhancements to meet their use case and thus help all other users.</li> </ul>"},{"location":"#how-it-works","title":"How it works","text":"<p>OpenTrafficCam consists of multiple modules. The core is composed of three:</p> <ol> <li>The OTCamera hardware itself to record videos,</li> <li>OTVision, a collection of algorithms to generate trajectories of objects (road users) based on the videos and</li> <li>OTAnalytics to gather traffic measures based on these trajectories.</li> </ol> <p>Each of the three modules comes with easy installation, a basic user interface and a detailed user documentation. The Open Source version covers the most common use cases of video-based traffic surveying, such as traffic counts and speed measurements.</p> <p>In addition to the three main modules, with OTLabels we will provide a set of labelled images of German road users and vehicles along with algorithms to train object detection models on custom data sets.</p> <p></p> <p>As described on the following pages of this documentation, some manual work, patience and basic skills are required to assemble OTCamera. And in order to regularly extract trajectories from videos using OTVision or train your own custom object detection models using OTLabels, you will need a powerful machine with a strong graphics card.</p> <p>Don\u00b4t want to deal with it? We got your back!</p> <p>We offer various services related to OpenTrafficCam:</p> <p> Selling and renting OTCameras</p> <p> Processing and analyzing your videos</p> <p> Setting up the OpenTrafficCam pipeline in your environment</p> <p> Onboarding and support to get the most out of OpenTrafficCam</p> <p> Conducting whole traffic surveys for you</p> <p> Developing features beyond the current open source version</p> <p> Consulting on traffic analysis, modeling and optimization</p> <p> Contact us for more information</p>"},{"location":"#who-we-are","title":"Who we are","text":"<p>OpenTrafficCam is coordinated and lead developed by the German startup platomo, a team of traffic engineers, software and hardware developers. The core team also includes traffic and photogrammetry researchers from the TU Dresden.</p> <p> </p> <p>OpenTrafficCam is funded by the German Federal Ministry for Digital and Transport as part of the research initiative mFUND.</p> <p> </p>"},{"location":"OTAnalytics/","title":"Features","text":"<p>The brain of OpenTrafficCam.</p> <p>OTAnalytics is a set of self developed algorithms to gather traffic measures based on trajectories of road users.</p>"},{"location":"OTAnalytics/#key-features","title":"Key features","text":"<ul> <li>Generation of traffic flow matrices and node flow diagrams</li> <li>Measurement of time gaps, velocities and accelerations</li> <li>Long-term: Analysis of near-accidents (e.g. TTC, PET)</li> <li>Visualization of traffic data</li> <li>Import of trajectories from other systems (e.g. DataFromSky)</li> <li>Runs on any Windows laptop</li> </ul>"},{"location":"OTAnalytics/#content-of-documentation","title":"Content of documentation","text":"<p>Most of the code will be developed by the OpenTrafficCam contributors (based on python libraries like NumPy, pandas, tkinter and OpenCV).</p>"},{"location":"OTAnalytics/Accuracy/counts/","title":"Traffic counts","text":""},{"location":"OTAnalytics/Accuracy/counts/#why-is-a-high-quality-of-the-counts-so-important","title":"Why is a high quality of the counts so important?","text":"<p>One of the main goals of OpenTrafficCam are automatic traffic counts at road sections and intersections. However, the results of such counts are only suitable for mobility planning if they are of high quality. In the worst case, traffic facilities planned based on inaccurate count data can waste public resources and even endanger the health of their users.</p>"},{"location":"OTAnalytics/Accuracy/counts/#how-to-measure-the-quality-of-the-counts","title":"How to measure the quality of the counts?","text":"<p>Precision and Recall, which are used to evaluate machine learning algorithms, can similarly be used as meaningful metrics to evaluate the quality of automated traffic counts. The precision metric describes the ratio of vehicles counted correctly by OpenTrafficCam to all vehicles counted by OpenTrafficCam including incorrect ones. On the other hand, the recall metric describes the ratio of vehicles correctly counted by OpenTrafficCam to all vehicles passed in reality including those not counted by OpenTrafficCam.</p> <p>An Example:</p> <ul> <li>100 vehicles pass through a junction</li> <li>80 of them are correctly detected by the system (true positive)</li> <li>20 are therefore not correctly detected (false negative)</li> <li>the system indicates that 110 vehicles would have passed the intersection</li> <li>the system has thus detected 30 vehicles that were not present in reality (false positive)</li> </ul> 110 got detected 20 got not detected 100 vehicles passed 80 (true positive) 20 (false negative) 30 are not real 30 (false positive) -- (true negative) <p>The prescision (or accuracy) is equal to true positive / (true positive + false positive). In this case the prescision would be 80 / (80 + 30) = 0.73. The recall (or sensitivity) is equal to true positive / (true positive + false negative). In this case the recall would be 80 / (80 + 20) = 0.80. The higher the two measurements are, the higher the quality of the results are.</p>"},{"location":"OTAnalytics/Accuracy/counts/#how-accurate-can-you-count-using-opentrafficcam","title":"How accurate can you count using OpenTrafficCam?","text":"<p>In his diploma thesis OpenTrafficCam contributor Armin Kollascheck investigated the quality of counting motorized traffic that got generated by OpenTrafficCam using YOLOv5x6 (Ultralytics), and a modified version of the IOU-Tracker (Bochinski et al.) for the following eight realistic traffic scenarios.</p> Three-way intersection Four-way intersection (1) Four-way intersection (2) Four-way intersection (2) at night Four-way roundabout Section (1) Section (2) Section (2) at night <p>Armin evaluated the count data automatically generated with OpenTrafficCam against a manually collected and verified ground truth. The sample sizes for determining the precision and recall of the automated traffic counts are 200 vehicles for each scenario. The ground truth of 200 vehicles was always counted manually. The results are shown in the table below.</p> Traffic facility Lighting Camera angle Precision Recall Three-way intersection Day Steep 0.95 0.85 Four-way intersection (1) Day Steep 1.00 0.94 Four-way intersection (2) Day Steep 0.99 0.95 Four-way intersection (2) Night with street lights Steep 0.98 0.95 Four-way roundabout Day Flat 0.63 0.56 Section (1) Day Steep 1.00 0.91 Section (2) Day Steep 1.00 0.97 Section (2) Night without street lights Steep 0.06 0.12"},{"location":"OTAnalytics/Accuracy/counts/#conclusion","title":"Conclusion","text":"<p>The high values for precision and recall in many scenarios show that OpenTrafficCam already works quite well with the official YOLOv5x6 model and a rather simple tracker. Thus, on different traffic facilities, counting qualities were achieved that probably exceed those of manual on-site counts. And even video recordings at night with street-side lighting seem to be counted correct with OpenTrafficCam.</p> <p>Unsatisfactory results, however, occurred with comparatively flat camera angles or night recordings without street lighting. This shows that the current pipeline is not yet perfect. We aim to further improve OpenTrafficCam to achieve satisfying counting results even under very difficult circumstances.</p>"},{"location":"OTAnalytics/gettingstarted/firstuse/","title":"First Use","text":""},{"location":"OTAnalytics/gettingstarted/firstuse/#load-video","title":"Load Video","text":"<p>OTAnalytics provides a simple and classic Point-and-Click-Userinterface. Your first step should be to load a videofile in the most common formats such as mkv or mp4 by clicking on the \"load video\"-button. The chosen filename will be inserted in the upper left listbox and a canvas with the first videoframe will be created. You can navigate through the video by either by scrolling, sliding or simply playing the video.</p>"},{"location":"OTAnalytics/gettingstarted/firstuse/#load-and-select-detections","title":"Load and select Detections","text":"<p>With click on the \"Load tracks\"-button a dialogue window pops up which asks for the trackfile from OTVision. This file contains framewise information about vehicles such as class, position and size in pixel of detected objects. With this information OTAnalytics can calculate bounding boxes and driven tracks and colors them dependent on vehicleclass. After the import object IDs and object class will be displayed in die middle right listbox. You can display a single track or multiple tracks by selecting the object IDs or you can display all tracks at once.</p>"},{"location":"OTAnalytics/gettingstarted/firstuse/#first-glance","title":"First glance","text":"<p>After importing all necessary files. Its important to check if chosen data correspond to eachother. The easiest way to do this, is to play, slide or scroll through the video. If correct, bounding boxes should overlay vehicles, pedestrians and cyclist. The bounding boxes are annotated with object class and confidence factor. Its also possible to live draw tracks.</p> <p></p>"},{"location":"OTAnalytics/gettingstarted/installation/","title":"Installation","text":""},{"location":"OTAnalytics/gettingstarted/installation/#install-python-39","title":"Install Python 3.9","text":"<p>If not done yet, install the 64-bit version Python 3.9.x via Windows installer from www.python.org/downloads/ as follows:</p> What if I already have another Python version installed? <p>In addition, also install Python 3.9. The last installed Python will automatically be the default Python interpreter of your system.</p> <p>On Windows, it is also possible to change the default Python interpreter by changing the order of the system-wide environment variables (move Python39 and Python39\\Scripts to the top, see animation below).</p> <p>This is necessary e.g. if you have already installed Python 3.9, but another Python version is your default because you installed it in the meantime (e.g. 3.10).</p> <p></p> <p></p> <p></p> <p></p> <p></p>"},{"location":"OTAnalytics/gettingstarted/installation/#install-otanalytics","title":"Install OTAnalytics","text":"<ol> <li> <p>Download and unzip the latest version of OTAnalytics from Github.</p> </li> <li> <p>In the unzipped folder Double-click the \"install.bat\" and wait until the installation of the dependencies is complete.</p> </li> </ol>"},{"location":"OTAnalytics/gettingstarted/installation/#to-run-otanalytics","title":"To run OTAnalytics","text":"<p>... double click the \"OTAnalytics.bat\" to run the Software via graphical user interface.</p>"},{"location":"OTAnalytics/gettingstarted/installation/#if-you-encounter-problems","title":"If you encounter problems","text":"<p>... please open an issue in the OTAnalytics repository on Github.</p> <p>We also welcome code contributions (e.g. fixing bugs or adding features) from other programmers by forking the repository and creating a pull request. Please check the contribute section of this documentation first.</p>"},{"location":"OTAnalytics/gettingstarted/requirements/","title":"Requirements","text":""},{"location":"OTAnalytics/gettingstarted/requirements/#hardware-prerequisites","title":"Hardware prerequisites","text":"<p>Both OTVision and OTAnalytics run on modern 64 bit Windows 10 desktops and laptops (e.g. i5 processor and 8 GB RAM).</p>"},{"location":"OTAnalytics/preparingtracks/movements/","title":"Movements","text":"<p>Movements represent source destination relationships of traffic flow and single objects like cars, trucks or pedestrians. The definition of movements play a big role if your analysis needs to be more detailed and specific.</p>"},{"location":"OTAnalytics/preparingtracks/movements/#create-an-delete-movements","title":"Create an delete Movements","text":"<p>Movements can be defined with a click on the \"new movement\"-button and adding pre defined Sections while being selected. Therefore movements can consist of infinite sections where the first section is the source and the last section stands for the destination. As sections, movements can be saved, renamed, deleted and imported.</p> <p></p>"},{"location":"OTAnalytics/preparingtracks/sections/","title":"Sections","text":"<p>OTAnalytics provides functions to lay down sections for analyzing traffic flow and volume over time or traffic composition. Sections work as sensors which can detect crossing events and associated time and also the direction of objects if driven over multiple sections. Sections can easily be modeled as line or areasections depending on your purpose.</p>"},{"location":"OTAnalytics/preparingtracks/sections/#create-and-delete-sections","title":"Create and delete Sections","text":"<p>For a linesection simply toggle the \"Line\"-button, click- and drag your mouse over the canvas. Press enter to define a sectionname and finish the creationprocess. Areasections work almost similar. Leftclick to define boundaries and rightclick to automatically close the polygon. Created sections will appear in the middle left listbox. Selected section will be highlighted on canvas and can be deleted or renamed. If you want to continue your work you can ex- and later import a flow-file that contains information about sections and movements.</p> <p></p>"},{"location":"OTAnalytics/trafficanalytics/counts/","title":"Traffic Counts","text":"<p>The main purpose of OTAnalytics is to determine traffic flow and counting. By analyzing your data vehicle class will be grouped by gatecrossing events, movements and chosen timeintervall and can be exported as excel-file for further remodeling and use.</p> <p>To present your analysis OTAnalytics provide different easy to adjust printable diagrams.</p>"},{"location":"OTCamera/","title":"Features","text":"<p>The eye of OpenTrafficCam.</p> <p>OTCamera is a mobile camera system for capturing traffic videos.</p> <p>It is not for sale, but for DIY. You don't want to build the camera yourself and preferably don't want to have anything to do with the measurement? Well, just write us.</p> <p></p>"},{"location":"OTCamera/#key-features","title":"Key features","text":"<ul> <li>Based on Raspberry Pi Zero W</li> <li>Waterproof case</li> <li>Operation with buttons and/or smartphone</li> <li>Continuous recording for about one week</li> <li>Privacy compliant recording</li> <li>Under 400 \u20ac per camera system</li> </ul>"},{"location":"OTCamera/#content-of-this-documentation","title":"Content of this documentation","text":"<ul> <li>List of hardware components (and where to purchase them)</li> <li>Software needed for video capturing and camera calibration</li> <li>Instructions for assembling the hardware and installing the software</li> <li>Recommendations for mounting, alignment and settings of the camera</li> </ul> <p>We are providing this information for two different prototypes.</p> <p>The Development Version is intended to actively participate in the development of OTCamera. It is also recommended to set up a development version to adapt the software and hardware to your own requirements. In addition, getting started with this prototype is very easy and can easily be done directly at your desk without the need for additional hardware.</p> <p>For the Field Version you need the ability to solder (simple) components to a blank circuit board. For this you get the most buttons (yay buttons) and the easiest operation in the field. All parts can be purchased from various dealers and are then easily plugged together. Also, the power consumption is the lowest (and the battery life the longest) and you can fully customize the hardware to your own needs.</p>"},{"location":"OTCamera/calibration/calibrate/","title":"How to calibrate OTCamera","text":"<p>OTCamera gives you the opportunity to receive distinctive camera parameters. These include intrinsics, extrinsics and distortion coefficients. By using our web browser based program and multiple images of a calibration pattern, like a chessboard you can calculate a camera matrix and distortion coefficients. You will be able to evaluate your calibration with a reprojection error which is used to quantify how closely you can recreate true projection. In other words: You can undistort pictures, trajectories or any other pixel coordinates.</p> <p>First step</p> <ul> <li>First you need to download a chessboard pattern (you can search for calibration board).</li> <li>The pattern size and dimension will play no big role for now, so feel free to use any chessboard.</li> <li>Attach your printout to a solid object. The image should be as flat as possible.</li> <li>For best results its unavoidable to let your calibration pattern be manufactured by professionals.</li> </ul> <p>Procedure</p> <ul> <li>Start the calibration application on your raspberrypi.</li> <li>Create a new calibration with a self-chosen CamID.</li> <li>Enter the number of chessboardcolumns, -rows and the squaresize in mm.</li> <li>Choose a number of wanted successful calibration pictures and resolution.</li> <li>Start the calibrationprocess.</li> <li>By clicking on \"receive coefficients\" your camera parameters will be stored as json-file on your raspberrypi.</li> <li>Evaluate the reprojection error displayed in the userinterface.</li> </ul> <p></p>"},{"location":"OTCamera/gettingstarted/assembly/","title":"Assembly","text":"<p>Coming soon</p> <p>Unfortunately, there is no content here yet. But we are currently working on completing this website.</p>"},{"location":"OTCamera/gettingstarted/firstuse/","title":"First Use","text":"<p>Coming soon</p> <p>Unfortunately, there is no content here yet. But we are currently working on completing this website.</p>"},{"location":"OTCamera/gettingstarted/installation/","title":"Overview","text":"<p>No matter which version you want to install, you will always use a Raspberry Pi as a base.</p> <p>You will need:</p> <ul> <li>Raspberry Pi 2B / 3B(+) / 4 / Zero W / Zero 2W and power supply</li> <li>We recommend a Zero W or Zero 2W because they draw substantially less power.</li> <li>Micro SD card (a High Endurance version is recommended)</li> <li>SD Card Reader</li> <li>Raspberry Pi Imager.</li> </ul> <p>Warning</p> <p>Even though we have completed the guide ourselves, every PC is different and Raspberry OS can change as well. There are steps described with which you can break your Windows, macOS or Linux on your PC. You should know what you are doing. We are not responsible for any damage that may occur.</p>"},{"location":"OTCamera/gettingstarted/installation/#quick-installer","title":"Quick Installer","text":"<p>We provide a simple quick installer which will do most of the work for you. We're assuming that you'll use an DS3132 RTC module. If you are using a different or no RTC module you should adjust the quick installer.</p> <p>Info</p> <p>If you like, you can also setup the Raspberry Pi and install OTCamera manually. The necessary steps are described on the following pages.</p>"},{"location":"OTCamera/gettingstarted/installation/#prepare-the-sd-card","title":"Prepare the SD-Card","text":"<p>To use the quick installer you'll need to prepare a SD-Card first. If done correctly you should now be able to connect to your Raspberry Pi via SSH.</p>"},{"location":"OTCamera/gettingstarted/installation/#update-and-upgrade-the-system","title":"Update and Upgrade the System","text":"<p>After preparing the SD-Card it is important to update and upgrade the Raspberry Pi before using the quick installer.</p> <p>To do this execute the following command:</p> <pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y &amp;&amp; sudo reboot\n</code></pre>"},{"location":"OTCamera/gettingstarted/installation/#download-and-run-quick-installer","title":"Download and Run Quick Installer","text":"<p>Connect to your Raspberry Pi via SSH again after reboot and download and run the quick installer:</p> <pre><code>wget https://raw.githubusercontent.com/OpenTrafficCam/OTCamera/master/raspi-files/install_otcamera.sh\nsudo bash install_otcamera.sh\n</code></pre> <p>You will be asked for some config values.</p> <p>After the installer complete, simply reboot the Raspberry Pi and the OTCamera software should be up and running.</p> <p>If not, please report your issues using the GitHub issue tracker.</p> <p>Note</p> <p>We are planning to offer a ready-to-use image for the Raspberry Pi, which can be easily installed. The setup will then be even easier. Stay tuned :)</p>"},{"location":"OTCamera/gettingstarted/requirements/","title":"Requirements","text":"<p>You will need some special hardware to build your own OTCamera to record videos.</p> <p>OTCamera is based on a Raspberry Pi Zero W and the official Rapberry Pi Camera Module V2.</p> <p>A specially designed PCB (printed circuit board) is needed to connect all additional parts to the Raspberry Pi. The single parts for the PCB as well as basic soldering skills are required to assemble a OTCamera.</p> <p></p> <p>As housing a common waterproof explorer case is used in addition to a 3d printed inlay which carries all OTCamera parts.</p> <p>To power everthing up, we will use a USB Power Bank.</p> <p>All in all we will need:</p> <ul> <li>Raspberry Pi Zero W</li> <li>Raspbrry Pi Camera Module V2</li> <li>Ribbon cable to connect Camera to Raspberry</li> <li>micro SD card</li> <li>OTCamera PCB</li> <li>Adafruit PowerBoost 1000C</li> <li>1x8 header female and male</li> <li>1x5 header female</li> <li>two 2x3 male header</li> <li>2x4 male header</li> <li>40 Pin GPIO female header</li> <li>2x1 male header</li> <li>7.5 mm 4700 \u00b5F capacitor</li> <li>two 100 k\u2126 resitors</li> <li>10 k\u2126 resistor</li> <li>47 k\u2126 resistor</li> <li>Si-diode</li> <li>1 cell LiPo battery</li> <li>four switches (3d print is optimized for Marquardt 1801)</li> <li>three LEDs (for example Barthelme 6V DC, 9.5mm)</li> <li>some cables and cable shoes</li> </ul> <p>To assemble everything, we will need a drill, a soldering iron and some basics tools as well.</p>"},{"location":"OTCamera/gettingstarted/installation/hwclock/","title":"Hardware Clock","text":"<p>The Raspberry Pi itself doesn't bring a hardware clock. It's assumed that a Pi always has a internet connection to get the current time. Since we want to use OTCamera in the field and thus may not have a Wi-Fi connection to access internet, we'll need a dedicated hardware clock (i.e. real time clock or RTC). It contains a backup battery (a coin cell) to keep track of time. The Pi will use the hardware clock time to set the system time.</p> <p>There are several RTC chips out there. There are even several RTC modules that are specifically designed to use with a Pi. One of the most precise RTC chips is the DS3231. We are using a Adafruit PiRTC - Precise DS3231 Real Time Clock for Raspberry Pi.</p> <p>Adafruit also provides a good (but little bit outdated) tutorial on how to use a RTC on the Pi. The follwing description is based on that tutorial.</p> <p>First, we'll need to enable I2C to communicate with the RTC. Either use <code>sudo raspi-config</code> and navigate to:</p> <ul> <li>Interface Options \u2192 I2C \u2192 Yes</li> </ul> <p>Or just use the command-line version of raspi-config:</p> <pre><code>sudo raspi-config nonint do_i2c 0\n</code></pre> <p>Let's install and use <code>i2c-tools</code> to see if the RTC works:</p> <pre><code>sudo apt install i2c-tools -y\nsudo i2cdetect -y 1\n</code></pre> <p>You should see an output of several lines containing <code>68</code> in one of them.</p> <p>We now need to add a device tree overlay in <code>/boot/config.txt</code> by adding the highlighted line at end of the file:</p> /boot/config.txt (end of file)<pre><code>...\n# (e.g. for USB device mode) or if USB support is not required.\notg_mode=1\n\n[all]\n\n[pi4]\ndtoverlay=vc4-fkms-v3d\n# Run as fast as firmware / board allows\narm_boost=1\n\n[all]\ngpu_mem=128\n\n# OTCamera\ndtoverlay=disable-bt\ndisable_camera_led=1\ndtparam=act_led_trigger=none\ndtparam=act_led_activelow=on\ndtoverlay=i2c-rtc,ds3231\n</code></pre> I'm not using the DS3231 <p>If you are using a different RTC than the DS3231, check out the Adafruit tutorial.</p> <p>In a nutshell: Use <code>dtoverlay=i2c-rtc,ds1307</code> or <code>dtoverlay=i2c-rtc,pcf8523</code> instead.</p> <p>After rebooting the Pi (<code>sudo reboot</code>) you can run <code>sudo i2cdetect -y 1</code> again. Instead of <code>68</code>there should be a <code>UU</code> instead.</p> <p>Now we'll disable the fake hardware clock:</p> <pre><code>sudo apt remove fake-hwclock -y\nsudo update-rc.d -f fake-hwclock remove\nsudo systemctl disable fake-hwclock\n</code></pre> <p>Additionally, we'll need to comment out some lines in <code>/lib/udev/hwclock-set</code>. So run <code>sudo nano /lib/udev/hwclock-set</code> and add <code>#</code> at the beginning of the lines:</p> /lib/udev/hwclock-set<pre><code>#!/bin/sh\n# Reset the System Clock to UTC if the hardware clock from which it\n# was copied by the kernel was in localtime.\n\ndev=$1\n\n#if [ -e /run/systemd/system ] ; then\n#    exit 0\n#fi\n\n#/sbin/hwclock --rtc=$dev --systz\n/sbin/hwclock --rtc=$dev --hctosys\n</code></pre> <p>The Pi should now be able to communicate with the RTC. Let's try it:</p> <pre><code>sudo hwclock -r\n</code></pre> <p>The Pi should now have automatically synced the time. If not, check if the Pi knows the correct time and sync it once:</p> <pre><code>date\nsudo hwclock -w\n</code></pre> <p>Done :)</p> <p>Your OTCamera should now be able to keep track of time without any USB power connected and without access to the internet. We recommend booting your OTCamera once within Wi-Fi range before each recording to update the time, although the DS3231 is fairly accurate.</p>"},{"location":"OTCamera/gettingstarted/installation/otcamera/","title":"Install OTCamera","text":"<p>Now we are ready to install last missing dependencies and setup OTCamera.</p>"},{"location":"OTCamera/gettingstarted/installation/otcamera/#setup-python-and-dependencies","title":"Setup Python and Dependencies","text":"<p>By default, Raspberry OS light doesn't come with PIP and git installed. We will need it to install required packages.</p> <pre><code>sudo apt install python3-pip git -y\n</code></pre> If you are using RaspberryPi OS legacy <p>Since the latest RaspberryPi OS python3 is the default python version. If you are using (not recommended) an older version of RaspberryPi OS, you need to make python3 your default version.</p> <p>Raspberry OS legacy ships with python 2 and python 3. By default python 2 is used. We want to change that to python 3 by adding two single lines to <code>.bashrc</code>.</p> <pre><code>echo \"alias python='/usr/bin/python3'\" &gt;&gt; ~/.bashrc\necho \"alias pip=pip3\" &gt;&gt; ~/.bashrc\n\nsource ~/.bashrc\n\npython --version\npip --version\n</code></pre> <p>Both commands should state, that they are (using) python 3.(x).</p>"},{"location":"OTCamera/gettingstarted/installation/otcamera/#clone-otcamera","title":"Clone OTCamera","text":"<p>We'll need to download OTCamera using git to get all the code we'll need to run OTCamera.</p> <pre><code>git clone --depth 1 https://github.com/OpenTrafficCam/OTCamera.git\n</code></pre> <p>OTCamera requires additional python packages, which need to be installed.</p> <pre><code>cd OTCamera\npip install -r requirements.txt --upgrade\n</code></pre>"},{"location":"OTCamera/gettingstarted/installation/otcamera/#setup-webserver-for-preview","title":"Setup Webserver for Preview","text":"<p>We are currently using nginx (a small webserver) to serve a small HTML file including a preview of the camera view.</p> <p>To install nginx:</p> <pre><code>sudo apt install nginx -y\n</code></pre> <p>We need to configure nginx to serve the OTCamera GUI. Open the nginx config file <code>/etc/nginx/sites-available/default</code> and edit the webserver root.</p> <pre><code>sudo nano /etc/nginx/sites-available/default\n</code></pre> <p>You should see something like this (there may be some comment lines starting with # which you can ignore).</p> /etc/nginx/sites-available/default<pre><code>server {\n        listen 80 default_server;\n        listen [::]:80 default_server;\n\nroot /var/www/html;\nindex index.html index.htm index.nginx-debian.html;\n\n        server_name _;\n\n        location / {\n                try_files $uri $uri/ =404;\n        }\n\n}\n</code></pre> <p>The important line we need to change is highlighted. Replace <code>/var/www/html</code> with the full path to the <code>OTCamera/gui/webfiles</code> folder. If your username is <code>pi</code> it should be <code>/home/pi/OTCamera/webfiles</code>.</p> <p>Restart nginx afterwards to let it know about the new directory:</p> <pre><code>sudo systemctl restart nginx.service\n</code></pre>"},{"location":"OTCamera/gettingstarted/installation/otcamera/#start-otcamera-on-startup","title":"Start OTCamera on startup","text":"<p>Now we want to start OTCamera every time the Raspberry starts. To do so, we will setup and enable a service. Edit <code>otcamera.service</code> inside the OTCamera repository according to your username and path.</p> ./raspi-files/otcamera.service<pre><code>[Unit]\nDescription=This service starts OTCamera and keeps it running\n\n[Service]\nUser=pi\nWorkingDirectory=/home/pi/OTCamera\nRestart=always\nRestartSec=3\nExecStart=/usr/bin/python3 run.py\n\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Now copy the file to <code>/lib/systemd/system</code> and enable it:</p> <pre><code>sudo cp ./raspi-files/otcamera.service /lib/systemd/system\nsudo systemctl daemon-reload\nsystemctl enable otcamera.service\n</code></pre> <p>After rebooting the Raspberry you should be able to access it's Wi-Fi ap and to open the OTCamera status site using the Raspberry's ip address: http://10.10.50.1</p>"},{"location":"OTCamera/gettingstarted/installation/prepare-sd-card/","title":"Prepare the SD Card","text":"<p>Download and install the Raspberry Pi Imager, insert the SD-Card and start the Imager.</p> <p></p> <p>It's recommended to use the Raspberry Pi Imager Advanced Options to do some basic setup. Just press Ctrl+Shift+X on startscreen to access advanced options. In newer Raspberry Pi Imager versions you just need to press the gear symbol.</p> <p>Setup a hostname, enable SSH, either password based or using public-key authentication (recommended!), configure your Wi-Fi credentials and choose the correct locale for Wi-Fi, timezone and keyboard layout. You can also skip the first-run wizard.</p> <p>If you set the default username to anything different than \"pi\" (which is recommended) you'll need to replace \"pi\" in the following documentation with your username. For example: instead of connecting to <code>ssh pi@hostname</code> you'll need <code>ssh username@hostname</code>.</p> How to generate a public key <p>Generate SSH-Keys for password-less connection. On your desktop computer open a command-line terminal (CMD or Powershell on Windows or a bash on Linux) and run</p> <pre><code>ssh-keygen\n</code></pre> <p>to generate a public private key combination. Add the private key to your ssh-agent (you may need to update OpenSSH on Windows).</p> <p></p> <p>Now insert the SD card into your PC. Select Raspberry Pi OS Lite (32-bit) as operating system under \"Raspberry Pi (Other)\". Then select the SD card on which the operating system will be installed.</p> <p>Warning</p> <p>It is possible to select not only the SD card, but also the hard drive or SSD where your operating system or other PC data is stored. Make sure that you really select the right SD card. Most of the time, you can tell by the size.</p> <p>Press the Write button and wait until the Raspberry Pi Imager has completely written and verified your SD card.</p> <p>Tip</p> <p>Sometimes the verification may fail. Try another USB port on your PC or another card reader. If this does not help, maybe the SD card is broken.</p> Setup without Raspberry Pi Imager <ol> <li> <p>Add an empty file named <code>ssh</code> to the boot partition to enable ssh on first boot.</p> </li> <li> <p>Add your Wi-Fi credentials as described in the Raspberry Pi Documentation</p> </li> <li> <p>Connect to the pi using ssh (<code>ssh pi@raspberry</code>)to the Pi using password authentication.</p> <p>Create and edit the needed <code>authorized_keys</code> file.</p> <pre><code>mkdir -p ~/.ssh\nnano ~/.ssh/authorized_keys\n</code></pre> <p>Copy your public key on the host and paste it on the pi, save&amp;close using Ctrl+X - Enter - Y.</p> </li> </ol>"},{"location":"OTCamera/gettingstarted/installation/prepare-sd-card/#next-steps","title":"Next steps","text":"<p>You're now ready to boot your Raspberry Pi to install OTCamera. You can either do it by using our Quick Installer or follwing all steps on the next sites.</p>"},{"location":"OTCamera/gettingstarted/installation/raspberry/","title":"Setup the Raspberry","text":"<p>Now, take the SD-Card and insert it in the Raspberry. You can now boot the Pi on LAN or within your Wi-Fi by connecting it to the power supply. Try to connect to the Pi using a command-line or PowerShell:</p> <pre><code>ssh pi@otcamera01\n</code></pre> <p>If everything is setup correctly, you will be asked to add the host key to the list of known hosts (answer: yes) and you should be connected to your Raspberry Pi.</p> warning: agent returned different signature type ssh-rsa (expected rsa-sha2-512) <p>If you are on Windows you may need to update OpenSSH if you ar getting this error:</p> <pre><code>warning: agent returned different signature type ssh-rsa (expected rsa-sha2-512)\n</code></pre> <p>Warning</p> <p>This guide should run on Windows 10 but you are modifying your system files. Please do not do anything you do not understand! Read the source for more information. We are not liable for any damage you may cause to your system. But we followed the steps ourselves and everything went well.</p> <ol> <li> <p>Download the latest OpenSSH for Windows release here. Depending on your Windows version it is probably <code>OpenSSH-Win64.zip</code>.</p> </li> <li> <p>Open Windows Explorer and navigate to your Download folder. You should see the <code>OpenSSH-Win64.zip</code>. Open an elevated PowerShell via File \u2192 Open Windows PowerShell \u2192 Open Windows PowerShell as administrator.</p> </li> <li> <p>If you copy and paste the following code, you will unzip OpenSSH and overwrite the system's default OpenSSH version.</p> <pre><code># Overwrite windows installed bins\n$openSshBins = (Get-ChildItem 'C:\\WINDOWS\\System32\\OpenSSH\\').Name\nExpand-Archive -Path .\\OpenSSH-Win64.zip -DestinationPath .\ntakeown.exe /a /r /f C:\\Windows\\System32\\OpenSSH\\\nicacls.exe 'C:\\Windows\\System32\\OpenSSH' /grant 'BUILTIN\\Administrators:(OI)(CI)F'\nicacls.exe 'C:\\Windows\\System32\\OpenSSH' /grant 'BUILTIN\\Administrators:F' /t\nStop-Service ssh-agent\n$openSshBins | %{ Copy-Item -Path .\\OpenSSH-Win64\\$_ -Destination C:\\Windows\\System32\\OpenSSH\\ }\nStart-Service ssh-agent\n</code></pre> </li> </ol> <p></p> <p>If you have successfully logged in now, we can configure the Raspberry Pi for OTCamera.</p>"},{"location":"OTCamera/gettingstarted/installation/raspberry/#update","title":"Update","text":"<p>Update the pi by running apt and reboot.</p> <pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y &amp;&amp; sudo reboot\n</code></pre> A new version of configuration file is available <p></p> <p>If you get this message, don't worry. Keep the local version currently installed, since we changed the ssh server configuration using the RPi Imager.</p>"},{"location":"OTCamera/gettingstarted/installation/raspberry/#raspi-config","title":"Raspi Config","text":"<p>Reconnect to your pi (open PowerShell and run <code>ssh pi@otcamera01</code>) and run the raspberry configuration tool.</p> <pre><code>sudo raspi-config\n</code></pre> <p>Change the following settings to appropriate values:</p> <ul> <li>System Options \u2192 Password (if not already done with Raspi Imager choose a new password for security reasons)</li> <li>Interface Options \u2192 I1 Legacy Camera \u2192 yes (since the new camera API is not supported by picamerax)</li> <li>Advanced options \u2192 GL driver \u2192 G1 Legacy (This may take a while, but saves a lot of energy.)</li> </ul> Setup without Raspberry Pi Imager <p>If you did not use the Raspberry Pi Imager, you will need to setup a few more things.</p> <ul> <li>System Options<ul> <li>Hostname</li> </ul> </li> <li>Localization Options<ul> <li>Timezone (Europe/Berlin)</li> <li>WLAN Country (DE)</li> </ul> </li> </ul> <p>Exit the raspi-config selecting \"Finish\" and reboot the Pi afterwards.</p>"},{"location":"OTCamera/gettingstarted/installation/raspberry/#power-saving-options","title":"Power Saving Options","text":"<p>After a reboot we also want to disable the HDMI output for additional power saving. CNX Software made some great power measurements for some Raspberry Pis. We'll need to add a specific line to the file <code>/etc/rc.local</code> in order to deactivate HDMI on every boot.</p> <pre><code>sudo nano /etc/rc.local\n</code></pre> <p>This opens the texteditor nano. We need to insert <code>/usr/bin/tvservice -o</code> in this file as highlighted below. Additionally we'll insert <code>sbin/iw dev wlan0 set power_save off</code> to disable automatic Wi-Fi power saving since we'll deactivate it anyways as soon as we don't need Wi-Fi.</p> /etc/rc.local<pre><code>#!/bin/sh -e\n#\n# rc.local\n#\n# This script is executed at the end of each multiuser runlevel.\n# Make sure that the script will \"exit 0\" on success or any other\n# value on error.\n#\n# In order to enable or disable this script just change the execution\n# bits.\n#\n# By default this script does nothing.\n\n# Print the IP address\n_IP=$(hostname -I) || true\nif [ \"$_IP\" ]; then\nprintf \"My IP address is %s\\n\" \"$_IP\"\nfi\n\n/usr/bin/tvservice -o\n/sbin/iw dev wlan0 set power_save off\n\nexit 0\n</code></pre> <p>Press Ctrl+X and Y and Enter to save the file and exit nano.</p> <p>Additionally we will disable bluetooth and the camera and onboard LED's. Edit <code>/boot/config.txt</code>to do so.</p> <pre><code>sudo nano /boot/config.txt\n</code></pre> <p>The config is quite long. We will add some lines (highlighted) at the end of the file:</p> /boot/config.txt (end of file)<pre><code>...\n# (e.g. for USB device mode) or if USB support is not required.\notg_mode=1\n\n[all]\n\n[pi4]\ndtoverlay=vc4-fkms-v3d\n# Run as fast as firmware / board allows\narm_boost=1\n\n[all]\ngpu_mem=128\n\n# OTCamera\ndtoverlay=disable-bt\ndisable_camera_led=1\ndtparam=act_led_trigger=none\ndtparam=act_led_activelow=on\ndtparam=audio=off\ndisplay_auto_detect=0\ngpio=6,16,18,19=ip\ngpio=16,18,19=pu\ngpio=6=pd\ngpio=5,12,13=op\ngpio=5,12=dl\ngpio=13=dh\n</code></pre> <p>Lines 22 to 27 define the default state of the GPIO pins. If you are using the OTCamera PCB, you want to add those lines. If you are not using it, you may want to adjust settings to your specific setup.</p> <p>Rebooting the Pi activates the new settings.</p> <pre><code>sudo reboot\n</code></pre>"},{"location":"OTCamera/gettingstarted/installation/wifi-ap/","title":"Wi-Fi Accesspoint","text":"<p>In order to access the OTCamera Raspberry Pi in field, we will let the Raspberry create it's own Wi-Fi. First, we'll need to install some packages:</p> <pre><code>sudo apt install hostapd dnsmasq dhcpcd -y\n</code></pre> <p>Now we'll need to configure all three of the newly installed packages. Let's start with hostap.</p> <pre><code>sudo nano /etc/default/hostapd\n</code></pre> <p>We need to modify line 13 to specify a valid <code>hostapd.conf</code>. To do so, insert <code>\"/etc/hostapd/hostapd.conf\"</code> right after <code>DAEMON_CONF=</code> in line 13:</p> /etc/default/hostapd<pre><code># Defaults for hostapd initscript\n#\n# WARNING: The DAEMON_CONF setting has been deprecated and will be removed\n#          in future package releases.\n#\n# See /usr/share/doc/hostapd/README.Debian for information about alternative\n# methods of managing hostapd.\n#\n# Uncomment and set DAEMON_CONF to the absolute path of a hostapd configuration\n# file and hostapd will be started during system boot. An example configuration\n# file can be found at /usr/share/doc/hostapd/examples/hostapd.conf.gz\n#\nDAEMON_CONF=\"/etc/hostapd/hostapd.conf\"\n# Additional daemon options to be appended to hostapd command:-\n#       -d   show more debug messages (-dd for even more)\n#       -K   include key data in debug messages\n#       -t   include timestamps in some debug messages\n#\n# Note that -B (daemon mode) and -P (pidfile) options are automatically\n# configured by the init.d script and must not be added to DAEMON_OPTS.\n#\n#DAEMON_OPTS=\"\"\n</code></pre> <p>Now let's edit this hostapd.conf file to configure our access point:</p> /etc/hostapd/hostapd.conf<pre><code>channel=11\nssid=MyOTCameraWifiNetwork\nwpa_passphrase=reallysafepassword\ninterface=uap0\nhw_mode=g\nmacaddr_acl=0\nauth_algs=1\nwpa=2\nwpa_key_mgmt=WPA-PSK\nwpa_pairwise=TKIP\nrsn_pairwise=CCMP\ncountry_code=DE\n</code></pre> <p>If you are planning to connect OTCamera to your own Wi-Fi (e.g. to transfer files in office or to get internet access to update etc.) you must use the same Wi-Fi channel as your office Wi-Fi network (since the Raspberry has just one antanna). You should edit your office Wi-Fi to always use the same channel to avoid automatic channel selection.</p> <p>The name of the Wi-Fi is specified just after <code>ssid=</code> and the password in line 3.</p> <p>Depending on where you will use OTCamera you should set the according contry code in the last line (for us it's Germany --&gt; DE).</p> <p>Save and exit the file.</p> <p>If you will connect to the OTCamera's Wi-Fi your device will need a valid ip address. dhcpcd and dnsmasq will help us doing by adding some lines (12-14) to the end of <code>/etc/dhcpcd.conf</code>:</p> /etc/dhcpcd.conf<pre><code>...\n# It is possible to fall back to a static IP if DHCP fails:\n# define static profile\n#profile static_eth0\n#static ip_address=192.168.1.23/24\n#static routers=192.168.1.1\n#static domain_name_servers=192.168.1.1\n\n# fallback to static profile on eth0\n#interface eth0\n#fallback static_eth0\ninterface uap0\n        static ip_address=10.10.50.1/24\n        nohook wpa_supplicant\n</code></pre> <p>If your office Wi-Fi uses the same address range you should use another one by, for example, using 51 instead of 50. But you need to remember that address to connect to your OTCamera later on.</p> <p>Finally, let's configure dnsmasq's config (<code>/etc/dnsmasq.conf</code>). It's a quite long file with a lot of explaining comments. We will backup this template and afterwards create a new, empty config:</p> <pre><code>sudo mv /etc/dnsmasq.conf /etc/dnsmasq.conf.backup\nsudo nano /etc/dnsmasq.conf\n</code></pre> <p>Now add the necessary config:</p> /etc/dnsmasq.conf<pre><code>interface=lo,uap0\nno-dhcp-interface=lo,wlan0\nbind-interfaces\nserver=10.10.50.1\ndomain-needed\nbogus-priv\ndhcp-range=10.10.50.10,10.10.50.250,2h\n</code></pre> <p>If you chose a different ip address range in <code>/etc/dhcpcd.conf</code> you'll need to adjust ip addresses here as well.</p> <p>Puh, this was quite a lot configuration edit stuff... But we are almost ready :)</p> <p>We don't want the services to start uncoordinated at bootup. So let's unmask hostapd first and then disable all three services to not start up at boot:</p> <pre><code>systemctl unmask hostapd.service\nsystemctl disable hostapd.service\nsystemctl disable dhcpcd.service\nsystemctl disable dnsmasq.service\n</code></pre> <p>We'll use a script instead to start up services. Make sure you are still inside the OTCamera directory.</p> <pre><code>sudo cp ./raspi-files/usr/local/bin/wifistart /usr/local/bin/wifistart\n</code></pre> <p>Last but not least, let's add this script to <code>/etc/rc.local</code> to start it at boot.</p> /etc/rc.local<pre><code>#!/bin/sh -e\n#\n# rc.local\n#\n# This script is executed at the end of each multiuser runlevel.\n# Make sure that the script will \"exit 0\" on success or any other\n# value on error.\n#\n# In order to enable or disable this script just change the execution\n# bits.\n#\n# By default this script does nothing.\n\n# Print the IP address\n_IP=$(hostname -I) || true\nif [ \"$_IP\" ]; then\nprintf \"My IP address is %s\\n\" \"$_IP\"\nfi\n\n/usr/bin/tvservice -o\n/sbin/iw dev wlan0 set power_save off\n\n/bin/bash /usr/local/bin/wifistart\"\nexit 0\n</code></pre> <p>Yeah, we're done! If you carfully followed this steps and we maintained this manual (...) the Raspberry should still connect to your Wi-Fi and will also create it's own Wi-Fi. It could take a minute or two, so don't worry to early. Let's try:</p> <pre><code>sudo reboot\n</code></pre> <p>All good? We hope so!</p>"},{"location":"OTCamera/recording/getvideos/","title":"Get the Videos","text":"<p>Coming soon</p> <p>Unfortunately, there is no content here yet. But we are currently working on completing this website.</p>"},{"location":"OTCamera/recording/mounting/","title":"Mounting","text":"<p>Coming soon</p> <p>Unfortunately, there is no content here yet. But we are currently working on completing this website.</p>"},{"location":"OTCamera/recording/preparation/","title":"Preparation","text":"<p>Coming soon</p> <p>Unfortunately, there is no content here yet. But we are currently working on completing this website.</p>"},{"location":"OTCamera/recording/record/","title":"Record","text":"<p>Coming soon</p> <p>Unfortunately, there is no content here yet. But we are currently working on completing this website.</p>"},{"location":"OTCamera/recording/safetywarning/","title":"Safety Warning","text":"<p>Coming soon</p> <p>Unfortunately, there is no content here yet. But we are currently working on completing this website.</p>"},{"location":"OTCamera/recording/settings/","title":"Settings","text":"<p>Coming soon</p> <p>Unfortunately, there is no content here yet. But we are currently working on completing this website.</p>"},{"location":"OTLabels/","title":"Features","text":"<p>The backbone of OpenTrafficCam.</p> <p>OTLabels is a workflow for retraining AI models to detect road users in videos based on custom video frames and existing open source software packages (CVAT, YOLO v5).</p>"},{"location":"OTLabels/#key-features","title":"Key features","text":"<ul> <li>Annotation of custom video frames using existing open source software (CVAT)</li> <li>Retrain existing neural network models with YOLO v5 for individual optimization of the detection ability</li> <li>Runs on any Windows laptop. However, a decent GPU with CUDA support is strongly recommended.</li> </ul>"},{"location":"OTLabels/#content-of-documentation","title":"Content of documentation","text":"<ul> <li>Training</li> <li>Validation</li> <li>Models</li> </ul> <p>Most of the code describing the workflow will be developed by the OpenTrafficCam contributors (based on python libraries like NumPy, pandas, tkinter and OpenCV).</p>"},{"location":"OTLabels/gettingstarted/data/","title":"Data","text":"<p>Coming soon</p> <p>Unfortunately, there is no content here yet. But we are currently working on completing this website.</p>"},{"location":"OTLabels/gettingstarted/installation/","title":"Installation","text":"<p>Coming soon</p> <p>Unfortunately, there is no content here yet. But we are currently working on completing this website.</p>"},{"location":"OTLabels/gettingstarted/requirements/","title":"Requirements","text":"<p>Coming soon</p> <p>Unfortunately, there is no content here yet. But we are currently working on completing this website.</p>"},{"location":"OTLabels/models/coco6/","title":"COCO 6-class","text":"<p>Coming soon</p> <p>Unfortunately, there is no content here yet. But we are currently working on completing this website.</p>"},{"location":"OTLabels/training/cvat/","title":"CVAT","text":"<p>CVAT is a free video and image annotation tool for computer vision. We recommend the developers guides for Installation as well as the user's guide.</p> <p>If you want to label your own dataset to retrain models, keep in mind that the format of your new labels needs to be the same as the format of the original dataset. On this page we define our intended organizational and technical annotation workflow using CVAT.</p> <p>Since YOLOv5 is based on the the MS COCO dataset of 2D bounding boxes, you need to apply your own labels in the frames from your sample videos in the same format. We therefore recommend the following steps:</p> <ol> <li>Install and setup an instance of CVAT either on your local computer or on a server (recommended when working in a team).</li> <li>Import the videos in CVAT and select the frames you want to use for labelling.</li> <li>Download the dataset from CVAT using the YOLO v1.1 format.</li> <li>Pre-annotate your frames with the standard YOLOv5x model. The results do probably not meet the desired quality. However, the pre-annotated labels will save you some time since it not necessary to draw all labels from scratch.</li> <li>Upload the pre-annotated frames to CVAT and revise the detected labels.</li> <li>Download the final labels from CVAT and convert them to COCO format (using our cvat_to_coco.py script).</li> </ol> <p>The COCO dataset and consequently the YOLOv5 models can detect 80 classes. However, for the detection of traffic, only six classes are relevant.</p>"},{"location":"OTLabels/training/cvat/#target-classes","title":"Target classes","text":"<p>Target classes are all road user classes from the initial COCO data set that are relevant for our sample videos:</p> <ul> <li>Person</li> <li>Bicycle</li> <li>Motorcycle</li> <li>Car</li> <li>Bus</li> <li>Truck</li> </ul> <p>All vehicles or combinations of vehicles are labeled as \"truck\" if they have</p> <ul> <li>twin tires (except for regular service buses or coaches),</li> <li>a superstructure or a loading area or</li> <li>a trailer (also cars with trailer, the 2D box includes vehicle and trailer).</li> </ul> <p>Here, as in the COCO dataset, both the \"person\" and the \"bicycle\" are labeled separately for bicyclists.</p>"},{"location":"OTLabels/training/cvat/#object-dimensions","title":"Object dimensions","text":"<p>The 2D box boundaries are defined by the outermost parts of the objects visible in that frame of the video.</p> <p>Even if only a part of an object is visible and the class of the object is recognizable for the annotator, it should be labeled.</p>"},{"location":"OTLabels/training/cvat/#projects-tasks-and-jobs","title":"Projects, Tasks and Jobs","text":"<p>We define certain set of videos as a project. Every project has multiple tasks (each consists of one video to annotate). Each task has a status (\"Pending\", \"In Progress\" and \"Completed\") and a progress bar showing the number of completed jobs. Open the task with click on \"Open\". For each task (or video) multiple jobs with a certain amounts of frames from the video are defined. Each job has a status (\"annotation\", \"validation\" and \"completed\"), an \"assignee\" (responsible for annotation) and a \"reviewer\" (responsible for reviewing the assignees annotation). Open the job with click on \"Job #...\".</p>"},{"location":"OTLabels/training/cvat/#import-datafix","title":"Import datafix","text":"<p>Coming soon</p> <p>Unfortunately, there is no content here yet. But we are currently working on completing this site.</p>"},{"location":"OTLabels/training/cvat/#workflow","title":"Workflow","text":"<p>For each job, the project coordinator assigns the roles of the \"assignee\" (= annotator) and the \"reviewer\". The reviewer then starts working on a job by initial labeling of the video frames. The assignee should save his labeling work from time to time and can also stop and later resume working on a job. After finishing the initial labeling, the assignee requests a review (menu - request review, the assigned reviewer should already be named there) and thus sets status of the job to \"validation\". Then the reviewer checks the initial labeling done by the assignee and raises issues when there is a problem or question. After reviewing all video frames of the job, the reviewer submits the review by either accepting and completing the job or rejecting it (which sets the status of the job bach to \"Annotation\"). Now the assignee has to check the issues, update the labels, resolve the issues and again request a review. The review process starts again and if necessary also next annotations, until the reviewer accepts the job and it is marked as \"Completed\". </p> <p>We recommend the following procedure for annotation in CVAT:</p> <ul> <li>Go through all pre-labeled objects on the right</li> <li>Check and delete false positive labels (if there is no object)</li> <li>Check and delete duplicate labels for the same object</li> <li>Check and correct object class for target classes</li> <li>Check and delete objects from other classes if they are classified wrong</li> <li>Zoom in on one quadrant of the image at a time:</li> <li>Check and correct position of object\u00b4s 2D boxes</li> <li>Check if there are objects from the target classes not already pre-labeled, draw new 2D box and annotate object class</li> </ul>"},{"location":"OTLabels/training/cvat/#download-data","title":"Download data","text":"<p>Coming soon</p> <p>Unfortunately, there is no content here yet. But we are currently working on completing this site.</p>"},{"location":"OTLabels/training/preprocessingdata/","title":"Preprocessing Data","text":"<p>Before retraining the YOLOv5 model, you might want to preprocess your labeled data to ensure that your frames are in the right format.</p>"},{"location":"OTLabels/training/preprocessingdata/#convert-cvat-output-to-yolov5","title":"Convert CVAT output to YOLOv5","text":"<p>OTLabels provides the <code>cvat_to_yolo.py</code> script to convert the annotated data to the appropriate format YOLOv5 needs for retraining the model. Before executing the script, you have to setup the configurations for your needs:</p> <ul> <li>Within the script:</li> <li>Set destPath: path where your data is stored (typically ./data/*).</li> <li>Set annFile: path to the COCO-annotation file (if you do not have it yet, see below how to get it).</li> <li>Set catFile: path to a text file containing your CVAT labels (standard: labels_CVAT.txt).</li> <li>Set cvatFile: path to a CVAT output file (containing images and labels, see also section CVAT: Download data) or to a folder containing multiple CVAT output files.</li> <li>Set name: the name of subfolder of destPath/images and destPath/labels to store the data in.</li> <li>labels_CVAT.txt: a text file with two columns separated by actual commas with headers named Cat and CatId containing the name and the ID of your CVAT labels.   For a example, see the labels_CVAT.txt file in the OTLabels repository.   Please note that labels not provided in this file will not be converted and consequently be deleted.</li> </ul> <p>The script performs the following steps:</p> <ol> <li>Unzip the CVAT output file</li> <li>Copy the images to the directory destPath/images/name.</li> <li>Import the label files and convert the CVAT labels to the COCO labels used by YOLOv5.</li> <li>Export the converted label files to the directory destPath/labels/name.</li> </ol>"},{"location":"OTLabels/training/preprocessingdata/#filter-the-labels","title":"Filter the labels","text":"<p>If for some reason you want to filter the labels that are already stored in your data directory, you can use the filter_labels.py script. Before executing the script, you have to setup the configurations for your needs:</p> <ul> <li>Within the script:</li> <li>Set path: path where the data is stored (typically ./data/*).</li> <li>Set name: the name of one or more subfolder of destPath/images and destPath/labels to store the data in.     More than one name must be provided as list.</li> <li>Set annFile: path to the COCO-annotation file (if you do not have it yet, see below how to get it).</li> <li>Set labelsFilter: path to a text file (standard: label_filter.txt) containing the labels you want to keep.</li> <li>label_filter.txt: a text file containing the category names (e.g., \"car\") of the labels you want to keep (without quotes and one category name per row).</li> </ul> <p>Please note that the images will not be copied to the corresponding folder in destPath/images in order to save file space. Instead, a text file with all images still containing the filtered labels is created. This file of filenames can also be referred to by YOLOv5.</p> <p>The script performs the following steps:</p> <ol> <li>Get the category IDs to the corresponding category name.</li> <li>Import the label files.</li> <li>Filter the labels by the provided category names.</li> <li>Export the labels to the directory destPath/labels/name_filtered.</li> <li>Create a text file with all image files in the directory path. Please note that images and label files not including any label after filtering are not exported.</li> </ol>"},{"location":"OTLabels/training/preprocessingdata/#get-coco-annotation-file","title":"Get COCO annotation file","text":"<p>This file is necessary to ensure your labels match the labels of the pretrained YOLOv5 models. Use the get_coco_annotation_files.py script to download one or more COCO annotation files. Configurations to be made:</p> <ul> <li>Within the script:</li> <li>Set path: path where the data is stored (typically ./data/*).</li> <li>Set URLFile: path to the config file that stores the URLs of the annotation files</li> <li>coco_annotation_json_URLs.txt: a text file containing the URLs of the annotation files (without quotes and one URL per row)</li> </ul>"},{"location":"OTLabels/training/preprocessingdata/#get-the-original-coco-dataset","title":"Get the original COCO dataset","text":"<p>In some cases you might want to get the original COCO dataset that was used to train the original YOLOv5 models. Therefore, OTLabels provides the get_coco_data.py script to download the data. Please note that you could also use the get_coco.sh script from the YOLOv5 repository. Before executing the script, you have to setup the configurations for your needs:</p> <ul> <li>Within the script:</li> <li>Set imageURLs: path of a text file (standard: coco_image_URLs.txt) containing the URLs of the image data sets.</li> <li>Set annURL: URL to the labels.</li> <li>Set destPath: path where your data is stored (typically ./data/*).</li> <li>coco_image_URLs.txt: a text file containing the URLs of the images for training, validation and testing (without quotes and one URL per row).</li> </ul>"},{"location":"OTLabels/training/retrainmodel/","title":"Retraining a Model","text":"<p>Although the pretrained YOLOv5 models (especially the bigger ones like YOLOv5l or YOLOv5xl) return acceptable detection results, we experienced some shortcomings for certain object classes which are not well represented and/or distributed in the COCO dataset used for training the YOLOv5 models.</p> <ol> <li>Especially county specific differences were identified, since the COCO dataset contains mainly pictures taken in North-American areas (US trucks vs. European trucks).</li> <li>Also, less represented object classes (e.g., bicycles) might cause worse detection rate or even false detections and hence, to an uneven detection accuracy between the object classes.</li> <li>This might lead to biases in the final counts (e.g., every car is detected, but only every second motorcycle).</li> </ol> <p>Further, one might condense the 80 classes of the pretrained YOLOv5 models to the relevant six classes (pedestrian, bicycle, car, bus, truck, motorcycle) for detecting moving traffic to reduce noise from non-relevant classes.</p>"},{"location":"OTLabels/training/retrainmodel/#first-step-setting-up-the-config-files","title":"First step: setting up the config files","text":"<p>For retraining, there are two relevant config files that needs to be set up.</p>"},{"location":"OTLabels/training/retrainmodel/#data-structure-and-labels","title":"Data structure and labels","text":"<p>YOLOv5 needs a config file in yaml format that contains information about file locations and the labels. This config file is usually stored in the data folder in the OTLabels directory.</p>"},{"location":"OTLabels/training/retrainmodel/#data_structure_labelsyaml","title":"data_structure_labels.yaml","text":"<pre><code># COCO 2017 dataset http://cocodataset.org\n# Train command: python train.py --data coco.yaml\n# Default dataset location is next to /yolov5:\n#   /parent_folder\n#     /coco\n#     /yolov5\n\n# download command/URL (optional)\n# download: bash data/scripts/get_coco.sh\n\n# train and val data as \n# 1) directory: path/images/, \n# 2) file: path/images.txt, or \n# 3) list: [path1/images/, path2/images/]\n\ntrain: ../OTLabels/data/path_to_structure_file_training.txt\nval: ../OTLabels/data/coco/path_to_structure_file_validation.txt\ntest: ../OTLabels/data/coco/path_to_structure_file_test.txt\n\n# number of classes\nnc: 6\n\n# class names\nnames: [\"person\", \"bicycle\", \"car\", \"motorcycle\", \"bus\", \"truck\"]\n</code></pre>"},{"location":"OTLabels/training/retrainmodel/#model-structure","title":"Model structure","text":"<p>Further, the configuration file containing the model structure (also in yaml format) needs to be set up or adapted. These configuration files are usually stored in the models folder within the yolov5 directory. Natively, YOLOv5 comes with one configuration file for each model (e.g., yolov5s.yaml). We strongly recommend to keep the model structure itself as it is and only adapt the number of classes, since the retraining process is based on the trained standard weights, which rely on the original structure.</p>"},{"location":"OTLabels/training/retrainmodel/#model_structureyaml","title":"model_structure.yaml","text":"<pre><code># parameters\nnc: 6  # number of classes\n...\n</code></pre>"},{"location":"OTLabels/training/retrainmodel/#second-step-connecting-to-wandb","title":"Second step: connecting to wandb","text":"<p>If you want to have your training process logged and visualized, YOLOv5 comes with the option to connect to weights and biases (wandb). For further instructions, please visit the wandb issue on GitHub. Otherwise you can chose the option not to use wandb when asked.</p>"},{"location":"OTLabels/training/retrainmodel/#third-step-retraining-the-model","title":"Third step: retraining the model","text":"<p>Now, you are ready to start with the retraining of your models. To start the process, simply execute the train.py with the desired options.</p> <p><code>python yolov5/train.py --img **img_size** --batch **batch_size** --epochs **n_epochs** --data path_to/**data_structure_labels**.yaml --weights yolov5/weights/**yolov5_weights** --cfg path_to/**model_structure**.yaml</code></p> <p>For further information about the whole retraining process with YOLOv5, please see the original documentation on GitHub.</p>"},{"location":"OTLabels/validation/gettingstarted/","title":"Getting Started","text":"<p>This section provides a guide on how to install and use OTValidate.</p>"},{"location":"OTLabels/validation/gettingstarted/#installation","title":"Installation","text":"WindowsLinux/ macOS IntelApple M1 <p>Note</p> <p>Installation instructions for Windows are following soon.</p> <p>Install OTValidate by cloning the repository  with git:</p> <pre><code>git clone https://github.com/OpenTrafficCam/OTValidate.git\ncd OTValidate\npip install -r requirements.txt\npip install .\n</code></pre> <p>Or install by using the <code>Makefile</code>:</p> <pre><code>git clone https://github.com/OpenTrafficCam/OTValidate.git\ncd OTValidate\nmake install\n</code></pre> <p>The installation for machines running on Apple's M1 chip is not as straightforward. There are two ways to install <code>OTValidate</code> on an M1 Mac. As a prerequisite the package manager Homebrew is required.</p> <ol> <li> <p>By executing these commands in the following order:</p> <pre><code>git clone https://github.com/OpenTrafficCam/OTValidate.git\ncd OTValidate\nbrew install openblas\nOPENBLAS=$(brew --prefix openblas) CFLAGS=\"-falign-functions=8 ${CFLAGS}\" pip install scipy==1.7.2\npip install -r requirements.txt\npip install .\n</code></pre> </li> <li> <p>By using the <code>Makefile</code></p> <pre><code>git clone https://github.com/OpenTrafficCam/OTValidate.git\ncd OTValidate\nmake install_m1\n</code></pre> </li> </ol>"},{"location":"OTLabels/validation/gettingstarted/#prerequisites","title":"Prerequisites","text":""},{"location":"OTLabels/validation/gettingstarted/#image-annotation-data","title":"Image Annotation Data","text":"<p>The folder containing the ground truth annotations of the images need to be in the YOLO format:</p> <pre><code>annotation_data\n\u2502   obj.data\n\u2502   obj.names\n\u2502   train.txt\n\u2502\n\u2514\u2500\u2500\u2500obj_train_data\n    \u2502   frame_01.png\n    \u2502   frame_01.txt\n    \u2502   frame_02.png\n    \u2502   frame_02.txt\n    \u2502   ...\n    \u2502\n</code></pre>"},{"location":"OTLabels/validation/gettingstarted/#usage","title":"Usage","text":""},{"location":"OTLabels/validation/gettingstarted/#analyse-object-detection-performance","title":"Analyse Object Detection Performance","text":""},{"location":"OTLabels/validation/gettingstarted/#quickstart-guide","title":"Quickstart Guide","text":"<pre><code>from OTValidate  import evaluate_detection_performance\n\n# path to the directory containing the annotated dataset in otdet format\ngt_data = \"path/to/data1\"\n\n# model weights\nmodel1 = \"path/to/model_weights1.pt\"\nmodel2 = \"path/to/model_weights2.pt\"\nmodel3 = \"path/to/model_weights3.pt\"\n</code></pre> <p>Use the <code>evaluate_detection_performance</code> function to calculate a set of object detection metrics of the respective models:</p> <pre><code>evaluate_detection_performance(\n    path_to_model_weights=[model1, model2, model3],\n    yolo_path=yolo_path,\n    otdet_gt_dir=gt_data,\n    is_gt_xyxy_format=False, # whether the ground truth's bounding box is in xyxy or xywh format\n    normalized=True,\n)\n</code></pre>"},{"location":"OTLabels/validation/gettingstarted/#results","title":"Results","text":"<p>The evaluation results of the models will be saved in the directories containing the annotation data. An <code>out</code> directory containing all the results will be created there.</p>"},{"location":"OTLabels/validation/metrics/","title":"Performance Metrics","text":"<p>In this section we briefly go over the different metrics to evaluate our models on how they fare in the tasks of object detection and object tracking. Furthermore, we will look into the different metrics used to evaluate the results in terms of traffic analysis.</p>"},{"location":"OTLabels/validation/metrics/#object-detection","title":"Object Detection","text":"<p>In the task of object detection a model is considered to be good if it is able to detect and classify an object correctly. In this section we are going to have a look in the different object detection performance metrics.</p>"},{"location":"OTLabels/validation/metrics/#intersection-over-union-iou","title":"Intersection Over Union (IOU)","text":"<p>We can tell if a predicted bounding box matches a ground truth bounding box by calculating and looking at the IOU of the two bounding boxes. As Padilla et al. explained in the paper1, \"a perfect match is considered when the area and location of the predicted and ground-truth boxes are the same\". Therefore, the IOU is calculated by determining the area of the intersection of the two bounding boxes and dividing it by the area of the union of the two bounding boxes as shown in:</p> <p></p> <p> Illustration adapted from the paper \"Analysis of Object Detection Metrics with a Companion Open-Source Toolkit\" </p> <p>Thus, two bounding boxes are considered a perfect match if the IOU = 1. Meaning the predicted and ground truth bounding boxes share the same location and the same size.</p> <p>On the other hand, the IOU = 0 when there is no intersection between the predicted and the ground truth bounding box.</p> <p>Usually an IOU threshold is defined in order to decide whether a predicted and ground truth bounding box are considered a match.</p>"},{"location":"OTLabels/validation/metrics/#true-positives-false-positives-false-negatives","title":"True Positives, False Positives, False Negatives","text":"<p>This section will explain what true positives, false positives, false negatives and true negatives are in the task of object detection. Thus, we will look at their definitions as defined by Padilla et al. 1:</p> <p>A True Positive is a correct detection of a ground-truth bounding box. An incorrect detection of a non-existing object or a misplaced detection of an existing object is a False Positive. An undetected ground-truth bounding box is named False Negative.</p>"},{"location":"OTLabels/validation/metrics/#precision","title":"Precision","text":"<p>Padilla et al. 1 explain precision as \"the ability of a model to identify only relevant objects. It is the percentage of correct positive predictions.\"</p> <p>Precision is calculated as:</p> \\[ Precision = \\frac{TP}{TP + FP} \\]"},{"location":"OTLabels/validation/metrics/#recall","title":"Recall","text":"<p>Padilla et al. 1 explain recall as \"the ability of a model to find all relevant cases (all ground-truth bounding boxes). It is the percentage of correct positive predictions among all given ground truths.\"</p> <p>Recall is calculated as:</p> \\[ Recall = \\frac{TP}{TP + FN} \\]"},{"location":"OTLabels/validation/metrics/#average-precision-ap","title":"Average Precision (AP)","text":"<p>As Padilla et al. explained that an object detection model \"can be considered good if, when the confidence threshold decreases, its precision remains high as its recall increases\"[ 1 ]. Taking this into account a model with a large area under a precision-recall curve indicates a high precision and a high recall. Therefore, the average precision \"is a metric based on the area under a [precision-recall curve]\" 1 with a selected IOU threshold. Thus the following notation for example, AP@50 denotes the average precision with IOU threshold at 50%.</p>"},{"location":"OTLabels/validation/metrics/#mean-average-precision-map","title":"Mean Average Precision (mAP)","text":"<p>We need to keep in mind that the average precision needs to be calculated for each class individually. Hence, the mean average precision \"is a metric that is able to represent the exactness of the detections among all classes\" 1.</p> <p>The mAP is calculated as follows:</p> \\[ mAP = \\frac{1}{C} \\sum_{i=1}^{C} AP_i \\] <p>where C is the total number of classes and \\(AP_i\\) is the average precision of class \\(i\\) 1.</p>"},{"location":"OTLabels/validation/metrics/#tide-metrics","title":"TIDE Metrics","text":"<p>Bolya et al. created TIDE a General Toolbox for Identifying Object Detection Errors2. As Bolya et al. explain in their paper2 \"mAP succinctly summarizes the performance of a model in one number\". Thus, the mAP performance metric does not give us any insight on what and how the different error types influence its score, that is the mAP score. The aim of TIDE is exactly that, to give us this insight on how the different error types affect the mAP score and as Bolya et al. 2 stated giving us \"a comprehensive analysis of each model's strengths and weaknesses\".</p> <p>TIDE defines six main error types as follows:</p> <p>Info</p> <p>The following descriptions of the error types are directly taken from the TIDE source code</p> <ol> <li> <p>Classification Error: Error caused when a prediction would have been marked positive if it had the correct class.</p> </li> <li> <p>Localization Error: Error caused when a prediction would have been marked positive if it was localized better.</p> </li> <li> <p>Both Cls and Loc Error: This detection didn't fall into any of the other error categories.</p> </li> <li> <p>Duplicate Detection Error: Error caused when a prediction would have been marked positive if the GT wasn't already in use by another detection.</p> </li> <li> <p>Background Error: Error caused when this detection should have been classified as background (IoU &lt; 0.1).</p> </li> <li> <p>Missed Ground Truth Error: Represents GT missed by the model. Doesn't include GT corrected elsewhere in the model.</p> </li> </ol>"},{"location":"OTLabels/validation/metrics/#confusion-matrix","title":"Confusion Matrix","text":"<p>The confusion matrix gives us a visual insight on how our object detection model performs in the classification task.</p> <p>Let us have a look first at the confusion matrix of a binary classification problem:</p> <p></p> <p> Illustration adapted from the paper \"Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation\" by David M. W. Powers[^3] </p> <p>The rows of the above confusion matrix represent the predicted class whereas the columns represent the ground truth class. Thus a prediction can be categorized as follows:</p> <ol> <li> <p>A prediction that has been predicted as positive class and that is found to be an actual/real positive class in the ground truth, is counted as a true positive.</p> </li> <li> <p>A prediction that has been predicted as negative class and is found to be an actual/real negative class is counted as a true negative.</p> </li> <li> <p>A prediction that has been predicted as positive and is found to be not an actual/real positive class is counted as a false positive.</p> </li> <li> <p>A prediction that has been predicted as negative and is found to be not an actual/real negative class is counted as a false negative.</p> </li> </ol> <p>Hence, the confusion matrix gives us a clear visualization of how many of our predictions were classified correctly or incorrectly.</p> <p>The confusion matrix of multi classification problem looks a little bit different:</p> <p></p> <p>As the image above implies, we now have multiple classes. For this example we want to classify the class <code>car</code>, <code>person</code> and <code>truck</code>. The green color coded tiles denote the true positive predictions.</p> <p>Let's take for example the row denoted with the class <code>car</code>. Here is how it is to be interpreted: Out of the 8 cars that have been predicted:</p> <ul> <li>three were correctly classified as a <code>car</code>,</li> <li>none were incorrectly classified as a <code>person</code> and</li> <li>5 were incorrectly classified as <code>truck</code></li> </ul>"},{"location":"OTLabels/validation/metrics/#object-tracking","title":"Object Tracking","text":"<p>Coming soon</p> <p>Unfortunately, there is no content here yet. But we are currently working on completing this website.</p>"},{"location":"OTLabels/validation/metrics/#traffic-measures","title":"Traffic Measures","text":"<p>To see, how well OpenTrafficCam performs see OTAnalytics.</p>"},{"location":"OTLabels/validation/metrics/#references","title":"References","text":"<ol> <li> <p>Padilla, R., Passos, W. L., Dias, T. L., Netto, S. L., &amp; da Silva, E. A. (2021). A comparative analysis of object detection metrics with a companion open-source toolkit. Electronics, 10(3), 279. https://doi.org/10.3390/electronics10030279 \u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Bolya, D., Foley, S., Hays, J., &amp; Hoffman, J. (2020). Tide: A general toolbox for identifying object detection errors. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part III 16 (pp. 558-573). Springer International Publishing. https://dbolya.github.io/tide/paper.pdf \u21a9\u21a9\u21a9</p> </li> <li> <p>Powers, D. M. (2020). Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation. arXiv preprint arXiv:2010.16061.\u00a0\u21a9</p> </li> </ol>"},{"location":"OTLabels/validation/modelvalidation/","title":"Model Validation","text":"<p>In this section we will compare the different YOLO models on how good they fare in the object detection task. The YOLO models to be evaluated are YOLOv5s, YOLOv5m, YOLOv5l and YOLOv5x. The models are evaluated on a custom dataset consisting of custom video recordings. Thus, we want to investigate which of the four YOLO models might be best suited in the detection of traffic objects.</p>"},{"location":"OTLabels/validation/modelvalidation/#dataset","title":"Dataset","text":"<p>As mentioned above, the dataset consists of different video recordings capturing different traffic scenes. Thus, the video recordings are turned into datasets that will be used to evaluate the object detection models. It is important to note that not every single frame is selected, but only every \\(n\\)-th frame where \\(n \\in \\mathbb{N}\\) and \\(n\\) is arbitrarily chosen by the user.</p> <p>Here is how the scenes are looking like:</p> Scenes <p>As seen above, there are two instances where scenes were also recorded at night. This makes it interesting to see how the YOLO models perform in detecting traffic objects in low light conditions.</p> <p>The class labels to be considered for our evaluation of the YOLO models are:</p> <ul> <li>person</li> <li>bicycle</li> <li>car</li> <li>motorcycle</li> <li>bus</li> <li>truck</li> </ul>"},{"location":"OTLabels/validation/modelvalidation/#evaluation-procedure","title":"Evaluation Procedure","text":"<p>The YOLO models are evaluated with the help of OTValidate. As a prerequisite, what OTValidate needs in order to start the evaluation are the YOLO models and the labeled ground truth data. In our case the ground truth labeled data is our custom dataset, which needs to be in the CVAT YOLO format. As for the YOLO model, a custom trained or an existing model can be loaded into OTValidate.</p> <p>OTValidate then uses each model to predict the ground truth images. Afterwards, the prediction results and ground truth data are used to calculate the different object detection metrics for model comparison. In our case, we will use the mAP and the TIDE metrics.</p> <p>Info</p> <p>OTValidate takes a list of class labels as a parameter. All predictions and ground truth data are then filtered according to that list of class labels. Meaning, all predictions or ground truth data, whose predicted or labeled class are not contained in the list, are discarded and therefore not regarded in the evaluation process. This is especially useful if the ground truth data contains class labels that the model can't predict. </p>"},{"location":"OTLabels/validation/modelvalidation/#evaluation","title":"Evaluation","text":"<p>In this section we will discuss the evaluation results of each model and put them under comparison. Specifically, we will evaluate the models on each dataset depicted by the scenes as shown above in the table and on all the ground truth data. The models confidence and IOU threshold are set to 0.25 and 0.5 respectively. Meaning, all detections that have an IOU lower than 0.5 are not regarded as possible detections and all detections with a confidence lower than 0.25 are discarded.</p>"},{"location":"OTLabels/validation/modelvalidation/#evaluation-on-all-data","title":"Evaluation on all data","text":"<p>This diagram depicts each models mAP at different IOU thresholds:</p> <p></p> <p>We can clearly see that the YOLOv5s model's mAP is the lowest compared to the other models. Meaning, by only looking at the mAP metric the YOLOv5s, YOLOv5m, and the YOLOv5x models are to be considered.</p> <p>Nevertheless, let us also have a look at the TIDE metrics to get an insight on the types of errors made by the models:</p> TIDE Metrics <p>What immediately stands out are the Missed Ground Truth Error(Miss) and the Classification Error(Cls). Meaning, the models were not able to detect many ground truth bounding boxes or be able to classify them correctly. Thus, the models were able to detect the majority of the bounding boxes, but had problems predicting the correct classes.</p> <p>The YOLOv5s model had the highest classification error rate out of the four models. Although, the missed ground truth error rate is also the highest, it does not differ much from the other three models.</p> <p>It becomes apparent that the bigger models perform better at detecting bounding boxes than the smaller ones. But there is also a point where the models' performance, namely the YOLOv5m, YOLOv5l and YOLOv5x, don't differ much at all. Implying that there is not much of a trade-off in choosing the YOLOv5m or YOLOv5l over the YOLOv5x.</p> <p>Still, let us try to find out why the  Missed Ground Truth Error appears to be a problem for all four models by evaluating them on the data of each scene.</p>"},{"location":"OTLabels/validation/modelvalidation/#evaluation-on-each-dataset-of-a-scene","title":"Evaluation on each dataset of a scene","text":""},{"location":"OTLabels/validation/modelvalidation/#scene-1-2","title":"Scene 1 &amp; 2","text":"Scene 1 Scene 2 <p>Scene 1 captures a three way junction. Compared to the combined dataset, the models' mAP value doesn't differ that much. The missed ground truth error of all four models is above 20%. Our assumption as to why the missed ground truth error is so high might be due to moving objects appearing in the distance. The models might not be able to detect these objects as they appear to be very small in the picture, as marked in red bounding boxes in the following image:</p> <p></p> <p>It is also possible that the timestamp covers up parts of objects making the models not be able to detect them.</p> <p>Scene 2 is similar to scene 1 in respect to the metrics calculated on all four models. The models' mAP evaluated on the dataset of scene 2 is higher than those of scene 1 when looking at the YOLOv5's <code>m</code>, <code>l</code> and <code>x</code> models. Scene 2's TIDE errors behave also very similar to those of scene 1. Also, small objects that have been annotated with small bounding boxes appear in images of the dataset of Scene 2. This could also be related to the high Missed Ground Truth Error.</p> <p></p>"},{"location":"OTLabels/validation/modelvalidation/#scene-3","title":"Scene 3","text":"<p>Scene 3 captures a four-way junction. There are two datasets capturing scene 2 at daylight and nighttime respectively.</p> Scene 3 At Night <p>Comparing these two datasets, it is not surprising that the models' mAP evaluated on the dataset taken at daylight fares much better to the one evaluated at nighttime. The low light condition makes it really hard for the models to detect any of the objects. The biggest drop in performance in terms of the mAP metric is seen from the YOLOv5s model.</p> <p>But then, why is the Missed Ground Truth Error (Miss) much lower and the Classification Error (Cls) that much higher of the nighttime dataset of Scene 3 compared to the one taken at daylight? Let's have a closer look at the two datasets first:</p> Scene 3 at daylight Scene 3 at nighttime <p>Overall, the amount of ground truth bounding boxes in scene 3 taken at daylight is more than twice as big as the one taken at nighttime. This implies that there is less traffic at nighttime, which makes sense. Thus, there is not much to detect at night resulting in a lower Missed Ground Truth Error. On the other hand, the classification error for all models is above 40% with the exception of the YOLOv5l model whose classification error is at approximately 24%. In addition to that the Localization Error (Loc) is very low. Meaning, the model is able to detect objects very well, but has trouble assigning them to the correct class in badly lit areas. Here is an example where a car is barely recognizable:</p> <p></p> <p>Speaking of scene 3 at daylight, the MissedGround Truth Error is relatively high across all models which is due to the same reason as explained in Scene 1. Traffic objects whose bounding boxes are really small might be detected by the models resulting in misses.</p> <p>But it is also important to take into account that there is the possibility of not moving objects appearing in every image at the same spot in the dataset. Such case can be seen in scene 3 at nighttime where cars have been parked on the sidewalk in the following image:</p> <p></p> <p>Thus, such case could influence the values of the calculated metrics for better or worse.</p>"},{"location":"OTLabels/validation/modelvalidation/#scene-4","title":"Scene 4","text":"<p>Scene 4 captures a rural road.</p> Scene 4 <p>The Missed Ground Truth Error (Miss) is pretty high for the YOLOv5s and the YOLOv5x models with an error value over 50% compared to the YOLOv5m and YOLOv5l models. At this moment, we could not come up with an explanation of why the YOLOv5x model's missed ground truth error is that much higher than its <code>m</code> and <code>l</code> counterpart. But there is an explanation why the missed ground truth error is high across all models. The reason lies in the dataset of scene 4 itself:</p> Scene 4 - images with cut off objects <p>The images above contain objects, which are surrounded by red bounding boxes, that are only partly in their respective image. Therefore, a model might not have been able to detect these objects due to a low confidence score and thus resulting in misses. These type of images appear often in the dataset. Another thing to keep in mind is that there is not much traffic on this rural road. Most images in the dataset do not contain bounding box annotations.</p> <p></p> <p>There are a total of 110 labeled objects in a dataset consisting of 401 images. As a result, even a small number of cut off objects could have a great impact on the metrics.</p>"},{"location":"OTLabels/validation/modelvalidation/#scene-5","title":"Scene 5","text":"<p>Scene 5 captures a rural road.</p> Scene 5 At Night <p>Scene 5 is captured at two different times, namely at day and night. The only difference to scene 3 is that scene 5 captures a rural road instead of a four way junction. Unsurprisingly, the models performed tremendously better on the dataset taken during the day compared to the one taken at night when looking at the mAP diagram.</p> <p>It is not surprising that all models evaluated on the nighttime dataset have a high Missed Ground Truth Error (Miss) which is due to the worse light conditions. What is standing out is the fact that the models evaluated on the day dataset also have a high miss error rate. To get behind the possible reason for this we should have quick peek in the ground truth dataset:</p> <p>Objects or vehicles emerging from the road in the upper left corner of the image make it harder for the models to detect and assign them to the correct class. This is reflected in the missed ground truth error and the localization error. Thus the further the objects move to the right area of the image, the better they are detected by the models.</p>"},{"location":"OTLabels/validation/modelvalidation/#conclusion-and-future-work","title":"Conclusion and Future Work","text":"<p>We have evaluated the <code>YOLOv5s</code>, <code>YOLOv5m</code>, <code>YOLOv5x</code>, and the <code>YOLOv5l</code> object detection models on a custom dataset consisting of videos capturing different scenes of traffic junctions and roads. Looking at mAP and TIDE metrics gave us insight to how the YOLOv5 models performed on our custom dataset and we can come to the conclusion that a <code>YOLOv5m</code>, <code>YOLOv5l</code> and <code>YOLOv5x</code> models performed far better than the <code>YOLOv5s</code>. But there is not a big increase in performance in terms of object detection upon choosing the <code>YOLOv5x</code> over the <code>YOLOv5m</code> or <code>YOLOv5l</code> model.</p> <p>Another important aspect to look into in the future is how much time the models under discussion take to detect the images. Depending on the use case and resources at hand, choosing the <code>YOLOv5x</code> model which might need much more time to finish the detection might not be suitable and thus taking the <code>YOLOv5m</code> or <code>YOLOv5l</code> model might be the better choice.</p> <p>What we are also currently working on is to train our own models using the YOLOv5 models as our foundation on a custom dataset. Thus it would be interesting to see if there would be a significant increase in performance by using custom trained models. If there is a significant increase in performance, the next question to ask if it makes sense to invest time in training a custom model.</p>"},{"location":"OTLabels/validation/overview/","title":"Overview","text":"<p>Validation gives insight on how well a model or software performs. In the case OpenTrafficCam, we want to evaluate and compare different object detection models on how well they perform in object detection and object tracking. Furthermore, we want to analyse the results of OTAnalytics on a set of traffic performance metrics.</p> <p>For this purpose we are currently developing OTValidate which allows the user to analyse and compare the models' performances on object detection and tracking by calculating metrics corresponding to the specific task at hand. As mentioned above OTValidate will also provide tools to analyse the results of OTAnalytics in regards to traffic performance.</p>"},{"location":"OTLabels/validation/overview/#otvalidate","title":"OTValidate","text":"<p>The image below gives a good overview on the structure of <code>OTValidate</code>:</p> <p></p> <p>For the object detection task <code>OTValidate</code> needs two input files namely an <code>.otdet</code> file and the ground truth label data for the object detection task. Alternatively, a custom or an existing model can be given over as an input instead of an <code>.otdet</code> file.</p>"},{"location":"OTVision/","title":"Features","text":"<p>The heart of OpenTrafficCam.</p> <p>OTVision is a collection of algorithms to generate trajectories of road users from traffic videos. Most of the algorithms have been developed in previous open source projects. We modify them and link them with our own developments into a working pipeline.</p> <p>The current pipeline consists of three core functionalities of OTVision: convert, detect and track.</p> <pre><code>graph LR\n    subgraph OTVision[\"OTVision:\"]\n        direction LR\n        conv(&lt;b&gt;convert&lt;/b&gt;\\n\\n...raw video\\nfiles to mp4)\n        det(&lt;b&gt;detect&lt;/b&gt;\\n\\n...road users\\nin single\\nframes)\n        tr(&lt;b&gt;track&lt;/b&gt;\\n\\n...detected road\\nusers over\\nmultiple frames)\n        conv --&gt; det --&gt; tr\n    end\n    tr .-&gt; traj[/.ottrk\\nTrajectories/]\n    vf[/.mp4\\nVideo Files/] .-&gt; det\n    rvf[/.h264\\nVideo Files/] .-&gt; conv</code></pre>"},{"location":"OTVision/#key-features","title":"Key features","text":"<ul> <li>Can be used without programming knowledge</li> <li>Conversion of .h264 video files to other formats     (using ffmpeg)</li> <li>Detection (joint localization and classification) of road users using     state-of-the-art AI object detection models in single video frames     (currently using YOLOv5 by Ultralytics)</li> <li>Tracking of detected road users over multiple frames     (currently using the     IOU-Tracker by Bochinski et al.)     and over multiple videos.</li> <li>Result: Trajectories in pixel coordinates</li> </ul>"},{"location":"OTVision/advanced_usage/configuration/","title":"Configuration File","text":"<p>If you\u00b4re getting tired of providing all those parameters to the CLI, you can also specify them in a configuration <code>yaml</code> file and pass it\u00b4s path to the CLI instead.</p> <p>By default, OTVision refers to the <code>user_config.otvision.yaml</code> in the root directory.</p> <p>You can either</p> <ul> <li>modify this file (then you don\u00b4t have to specify it\u00b4s path in the CLI) or</li> <li>build your own file and save it somewhere else (then you have to specify it\u00b4s path     in the CLI).</li> </ul> <p>You can specify parameters for the sub-tasks that are provided by OTVision (<code>convert</code>, <code>detect</code>, <code>track</code>) in separate configuration files or in a single file. The scripts for the respective sub-tasks only read the parameters they need.</p> <p>Warning</p> <p>Any parameter passed to the CLI will overwrite the respective parameter from the config file.</p> <p>In case of any problems, we suggest looking at yaml.info first when you customize your configuration files.</p>"},{"location":"OTVision/gettingstarted/firstuse/","title":"First Use","text":"<p>OTVision can be executed on a command-line (e.g., a Terminal on macOS and Linux or the Command Prompt on Windows).</p>"},{"location":"OTVision/gettingstarted/firstuse/#the-command-line-interface","title":"The Command-Line Interface","text":"<p>We provide a command-line interface (CLI) to run the OTVision pipeline steps. This documentation is written for using the Command Prompt on Windows or the integrated terminals in Linux or macOS.</p>"},{"location":"OTVision/gettingstarted/firstuse/#navigate-to-the-otvision-root-directory","title":"Navigate to the OTVision root directory","text":"<p>Open a Terminal and navigate to the OTVision root folder.</p> <pre><code>cd \"path/to/OTVision\"\n</code></pre> <p>Where is the OTVision root directory?</p> <p>It's the folder you downloaded und unzipped.</p> <p>Maybe your OTVision root directory is called <code>OTVision-main</code> after unzipping, if you downloaded it from Github. This is the correct directory.</p> <p>Inside the OTVision root directory, there is another directory called <code>OTVision</code> (this child directory is the wrong directory).</p>"},{"location":"OTVision/gettingstarted/firstuse/#activate-virtual-environment","title":"Activate virtual environment","text":"<p>Before using OTVision, you have to activate the virtual environment that was created by running the installation scripts:</p> Windows command promptLinux / macOS terminal <p>Open a Command Prompt an run:</p> <pre><code>venv\\Scripts\\activate\n</code></pre> <pre><code>source venv/bin/activate\n</code></pre> <p>The virtual environment should be activated, indicated by the <code>(venv)</code> in braces in front of your current working directory in the terminal.</p>"},{"location":"OTVision/gettingstarted/firstuse/#build-your-command","title":"Build your command","text":"<p>Every command consists of three parts:</p> <ol> <li>Invoke the Python interpreter with <code>python</code>.</li> <li>Specify the pipeline step you want to run (<code>convert.py</code>, <code>detect.py</code> or <code>track.py</code>).</li> <li> <p>Specify parameters for your script:</p> <p>For basic use, you only have to specify one parameter: The path(s) to the data you want to process. You can specify a file or folder path (in quotation marks) after the <code>-p</code> (or <code>--paths</code>) argument.</p> Some hints about specifying the paths <p>You can just drag a file or folder and drop them into the terminal.</p> <p>If you provide a path to a folder, every file within the folder will be processed.</p> <p>You can also provide multiple paths straight after one another (each in quotation marks).</p> <p>All other parameters are optional. They can also be set via arguments in the CLI (see \"Usage\" section) or via a separate configuration file.</p> </li> </ol> <p>Have a look at the basic examples provided below.</p>"},{"location":"OTVision/gettingstarted/firstuse/#convert","title":"Convert","text":"<p>In case you have raw <code>.h264</code> videos (e.g. from OTCamera), you need to convert them to a supported video format (see convert.py) first. Therefore, we provide the <code>convert.py</code> script.</p> <p>To convert <code>.h264</code> videos, run the following command after activating the venv:</p> <pre><code>python convert.py -p \"path/to/your/h264 files\"\n</code></pre> <p>where <code>path/to/h264 files</code> is either the path to a single h264 video file or a folder containing multiple h264 video files.</p> <p>Each converted video will by default be saved as a <code>.mp4</code> file in the same folder with the same name as the input <code>.h264</code> file.</p>"},{"location":"OTVision/gettingstarted/firstuse/#detect","title":"Detect","text":"<p>If you have converted your video files to one of the accepted file formats (<code>.avi</code>,<code>.mkv</code>,<code>.mov</code>,<code>.mp4</code>) or you already have your video files in one of those formats, you are ready to detect road users in each single frame of the video(s). Therefore, we provide the <code>detect.py</code> script.</p> <p>To detect video files, run the following command after activating your venv:</p> <pre><code>python detect.py -p \"path/to/your/video files\"\n</code></pre> <p>where <code>path/to/video files</code> is either the path to a single video file or a folder containing multiple video files.</p> <p>For each video file, the detected objects will be written to a separate <code>.otdet</code> file in the same folder with the same name as the input video file.</p>"},{"location":"OTVision/gettingstarted/firstuse/#track","title":"Track","text":"<p>With the detected road users in one or more <code>.otdet</code> files, you are ready to track the road users over multiple frames of the video (or even over multiple consecutive videos). Therefore, we provide the <code>track.py</code> script.</p> <p>To track <code>.otdet</code> files, run the following command after activating your venv:</p> <pre><code>python track.py -p \"path/to/your/otdet files\"\n</code></pre> <p>where <code>path/to/otdet files</code> is either the path to a single <code>.otdet</code> file or a folder containing your <code>.otdet</code> files. If you provide a folder, every <code>.otdet</code> file within the folder will be tracked.</p> <p>For each video file, the tracks will be written to a separate <code>.ottrk</code> file in the same folder as the input <code>.otdet</code> file.</p>"},{"location":"OTVision/gettingstarted/installation/","title":"Installation","text":"<p>In this section, we provide instructions how to install OTVision on the most common Operating Systems.</p> <p>Before installing OTVision, make sure your system meets all requirements.</p>"},{"location":"OTVision/gettingstarted/installation/#install-otvision","title":"Install OTVision","text":"<p>We provide install scripts for the most common operating systems.</p> <p>Download and unzip the latest version of OTVision from GitHub or clone the OTVision repository.</p> WindowsLinux / macOS <p>Inside the unzipped folder open the <code>install.cmd</code> and wait until the installation of the dependencies is complete.</p> <p>In a terminal, navigate to the OTVision folder and run the installer.</p> <pre><code>./install.sh\n</code></pre> <p>The installation of the dependencies could take a moment.</p> What is installed here? <p>The <code>install</code> script will create and activate a virtual environment (venv) and install the Python packages specified in the requirements.txt via pip from the Python Package Index.</p> <p>If you want to contribute code, additional requirements should to be installed in this virtual environment. Clone the OTVision repository  from github. Run the <code>install_dev.sh</code> in your OTVision folder and wait until the installation of the dependencies is complete. Find more information here.</p>"},{"location":"OTVision/gettingstarted/installation/#nvidia-cuda-optional","title":"Nvidia CUDA (optional)","text":"<p>If you have a Windows or Linux PC with a Nvidia graphics card and already installed CUDA, you chose the release with the suffix <code>-cuda</code>. It contains the requirements to use CUDA. If you want to contribute to OTVision and use CUDA, you have to perform additional steps in your Terminal/Command Prompt:</p>"},{"location":"OTVision/gettingstarted/installation/#check-cuda-version","title":"Check CUDA version","text":"<p>Check if CUDA is recognized and available.</p> <pre><code>nvcc --version\n</code></pre> <p>Navigate to the OTVision root directory.</p> <pre><code>cd \"path/to/OTVision\"\n</code></pre> <p>Where is the OTVision root directory?</p> <p>It's the folder you downloaded und unzipped.</p> <p>Maybe your OTVision root directory is called <code>OTVision-main</code> after unzipping, if you downloaded it from Github. This is the correct directory.</p> <p>Inside the OTVision root directory, there is another directory called <code>OTVision</code> (this child directory is the wrong directory).</p>"},{"location":"OTVision/gettingstarted/installation/#activate-virtual-environment","title":"Activate virtual environment","text":"<p>Activate the virtual environment that was created by running the installation scripts.</p> Windows command promptLinux / macOS terminal <p>Open a Command Prompt an run:</p> <pre><code>venv\\Scripts\\activate\n</code></pre> <pre><code>source venv/bin/activate\n</code></pre> <p>The virtual environment should be activated, indicated by the <code>(venv)</code> in braces in front of your current working directory in the terminal.</p>"},{"location":"OTVision/gettingstarted/installation/#install-torch-and-torchvision-for-cuda","title":"Install torch and torchvision for CUDA","text":"<p>Depending on your operating system (Windows or Linux) and your CUDA version you can select, copy and run the install command from the PyTorch site under \"INSTALL PYTORCH\" (choose Build=\"Stable\", Package=\"pip\" and Language=\"Python\").</p> <p>E.g., for CUDA 11.7 and the latest stable PyTorch Build, the command is:</p> <pre><code>pip3 install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu117\n</code></pre>"},{"location":"OTVision/gettingstarted/installation/#if-you-encounter-problems","title":"If you encounter problems","text":"<p>Maybe you also have to install Microsoft Visual C++ 14.0 or greater from the Visual Studio Build Tools.</p> <p>In case of further problems please open an issue in the OTVision repository on GitHub or contact us. We are happy to know about you experience.</p> <p>We also welcome code contributions (e.g., fixing bugs or adding features) from other programmers by forking the repository and creating a pull request. Please check the contribute section of this documentation first.</p>"},{"location":"OTVision/gettingstarted/requirements/","title":"Requirements","text":"<p>OTVision is designed to run on most modern operating systems (Windows, Linux and Mac). However, a few prerequisites are required before installing OTVision itself.</p>"},{"location":"OTVision/gettingstarted/requirements/#hardware-prerequisites","title":"Hardware prerequisites","text":"<p>OTVision runs on modern desktops and laptops (e.g. Intel i5+ of the last few generations, AMD Zen chips or Apple Silicon processors and 8 GB RAM).</p> <p>If you want to detect road users in videos with OTVision on a regular basis, we strongly recommend purchasing a powerful desktop computer or workstation (&gt;= 8+ cores, &gt;= 16 GB RAM) with a Nvidia graphics card (&gt;= GeForce 10XX Series, better is usually faster). Make sure that the Nvidia drivers as well as the NVidia CUDA Toolkit are installed and up-to-date to get the best performance.</p> <p>Apple Neural Engine</p> <p>Accelerated detection using the Apple Neural Engine is not yet supported. Detection on Apple silicon chips is currently performed using CPU only.</p>"},{"location":"OTVision/gettingstarted/requirements/#software-prerequisites","title":"Software prerequisites","text":""},{"location":"OTVision/gettingstarted/requirements/#python-310","title":"Python 3.10","text":"<p>OTVision is based on Open Source Python packages. Thus, it runs on Windows, macOS and Linux systems after setting up the environment correctly. Consequently, Python needs to be installed on your system. Currently OTVision runs best on Python 3.10. An instruction how to do that on a Windows system can be found below.</p> What if I want to use another Python version? <p>Python 3.10 is the version we are currently testing against and providing installation scripts for. Other Python versions might also work, but are not currently tested. In any case, you will have to install the requirements manually or adapt the installation scripts accordingly.</p> WindowsLinuxmacOS <p>If not done yet, install the latest 64-bit version of Python 3.10 via Windows installer (www.python.org/downloads).</p> <p>Please make sure to check the following boxes during installation:</p> <ul> <li> Add Python to PATH</li> <li> Install pip</li> <li> All advanced options</li> </ul> What if I already have another Python version installed? <p>In addition, also install Python 3.10. On most operating systems you can choose the python version to use by using <code>python-3.10</code>oder <code>python3.10</code> styled commands.</p> <p>On Windows, it is also possible to change the default Python interpreter by changing the order of the system-wide environment variables (move Python310 and Python310\\Scripts to the top, see animation below).</p> <p>This is necessary e.g. if you have already installed Python 3.10, but another Python version is your default because you installed it (e.g. 3.11).</p> <p></p> <p>To check your Python installation, run the the follwing commands in a cmd terminal:</p> <pre><code>python --version\n</code></pre> <p>If <code>Python 3.10.x</code> is returned, you are good!</p> <p>Depending on the Linux distribution you are using, Python 3.10 might not be the pre-installed version. You can check if (and which sub-version of) Python 3.10 is installed by running the following command in the terminal:</p> <pre><code>python3.10 --version\n</code></pre> <p>If necessary, install Python 3.10 manually using additional sources.  Since the sources might differ depending on the Linux distribution you are using, we kindly ask you to use a search engine for a detailed instruction on how to install Python 3.10 manually for your distribution. </p> <p>In any case, please make sure that you also have the python virtual environment package <code>python3.10-venv</code> installed.</p> <p>Depending on the macOS version you are using, Python 3.10 might not be the pre-installed version. You can check if (and which sub-version of) Python 3.10 is installed by running the following command in the terminal:</p> <pre><code>python3.10 --version\n</code></pre> <p>If necessary, install Python 3.10 as described below.</p>"},{"location":"OTVision/gettingstarted/requirements/#homebrew","title":"Homebrew","text":"<p>We recommend installing Python using Homebrew: </p> <pre><code>brew install python@3.10\n</code></pre>"},{"location":"OTVision/gettingstarted/requirements/#manual-installation","title":"Manual installation","text":"<p>Alternatively, you can download a macOS installer for python 3.10 from python.org.</p>"},{"location":"OTVision/gettingstarted/requirements/#check-installation","title":"Check installation","text":"<p>Again, run this command in the terminal:</p> <pre><code>python3.10 --version\n</code></pre>"},{"location":"OTVision/gettingstarted/requirements/#ffmpeg","title":"ffmpeg","text":"<p>If you want to use videos recorded by OTCamera with OTVision, you need to convert the videos to .mp4 files first. For the conversion, we use the Open Source software ffmpeg.</p> <p>Before using the <code>convert.py</code> script, make sure that ffmpeg is installed and available systemwide:</p> WindowsLinuxmacOS <p>To install ffmpeg on your Windows system, please perform the following steps:</p> <ol> <li>Download the file <code>ffmpeg-git-full.7z</code> from     gyan.dev.  </li> <li>Unzip this file by using any file archiver such as 7zip in a folder of     your choice (e.g., <code>C:\\ffmpeg</code>).</li> <li> <p>Now, open a Command Prompt with administrator privileges and set the environment path variable for ffmpeg:</p> <pre><code>setx /m PATH \"path_to_your_ffmpeg_folder\\bin;%PATH%\"\n</code></pre> <p>where <code>path_to_your_ffmpeg_folder</code> represents the folder that you have ffmpeg     unzipped in.</p> <p>If you unzipped to <code>C:\\ffmpeg</code>, for example: </p> <pre><code>setx /m PATH \"C:\\ffmpeg\\bin;%PATH%\"\n</code></pre> </li> <li> <p>Restart your computer and verify the installation by running </p> <pre><code>ffmpeg -version\n</code></pre> </li> </ol>"},{"location":"OTVision/gettingstarted/requirements/#ubuntu-repositories","title":"Ubuntu repositories","text":"<p>If you use Ubuntu, you can install ffmpeg using the official Ubuntu repositories.</p> <pre><code>sudo apt install ffmpeg\n</code></pre>"},{"location":"OTVision/gettingstarted/requirements/#manual-installation_1","title":"Manual installation","text":"<p>For the manual installation of ffmpeg on Linux or if you use another  distribution, please refer to the instructions on the developers website.</p>"},{"location":"OTVision/gettingstarted/requirements/#homebrew_1","title":"Homebrew","text":"<p>For the installation using Homebrew, use the following command in the terminal:</p> <pre><code>brew install ffmpeg\n</code></pre>"},{"location":"OTVision/gettingstarted/requirements/#manual-install","title":"Manual install","text":"<p>For the manual installation of ffmpeg on macOS, please refer to the instructions on the developers website.</p>"},{"location":"OTVision/gettingstarted/requirements/#nvidia-cuda-optional","title":"Nvidia CUDA (optional)","text":"<p>If you intend to use OTVision on a Windows or Linux PC with a modern Nvidia graphics card, download and install the latest version of the NVIDIA Cuda Toolkit that is supported by PyTorch on your platform.</p>"},{"location":"OTVision/usage/convert/","title":"Convert","text":""},{"location":"OTVision/usage/convert/#synopsis","title":"Synopsis","text":"<pre><code>python  convert.py  [-p paths] [-c config]\n[--fps_from_filename] [--input_fps]\n[--delete_input] [-o overwrite]\n</code></pre>"},{"location":"OTVision/usage/convert/#description","title":"Description","text":"<p>In case you have raw <code>.h264</code> videos (e.g. from OTCamera), you need to convert the videos to a supported video format.</p> <p>As <code>.h264</code> does not include metadata, the frame rate of the video has to be either read from the filename or specified by the user.</p> <p>We suggest converting to <code>.mp4</code> (the default output file type).</p>"},{"location":"OTVision/usage/convert/#parameters","title":"Parameters","text":""},{"location":"OTVision/usage/convert/#paths-required","title":"paths (required)","text":"<p><code>-p \"path/to/h264 files\" \"path/to/other h264 files\"</code></p> <p>or</p> <p><code>--paths \"path/to/h264 files\" \"path/to/other h264 files\"</code></p> <p>One or multiple paths to <code>.h264</code> files or folders containing <code>.h264</code> video files.</p> <p>This parameter is required to run the <code>convert.py</code> script. It has to be specified either using the CLI or in the configuration yaml file.</p>"},{"location":"OTVision/usage/convert/#config","title":"config","text":"<p><code>-c \"path/to/config file\"</code></p> <p>or</p> <p><code>--config \"path/to/config file\"</code></p> <p>Path to a custom user configuration yaml file. Other parameters (including <code>paths</code>) are parsed from this configuration file.</p> <p>This parameter is optional. By default, the <code>user_config.otvision.yaml</code> in the root directory is read. This also overrides the default values of all parameters. If the <code>user_config.otvision.yaml</code> in the root directory is missing and no other configuration file is specified, the default values of the parameters as described below are used.</p> <p>Warning</p> <p>Any parameter passed to the CLI will overwrite the respective parameter from the config file.</p>"},{"location":"OTVision/usage/convert/#fps_from_filename","title":"fps_from_filename","text":"<p><code>--fps_from_filename</code> to parse the video frame rates from the input <code>.h264</code> filenames.</p> <ul> <li>In this case the frame rate of each input <code>.h264</code> file has to be specified in the filename using the following pattern: <code>_FR&lt;fps&gt;_</code> (where <code>fps</code> is the frame rate)</li> <li>An example would be <code>_FR20_</code> in the following filename:     <code>OTCamera01_FR20_2023-01-01_12-15-00.h264</code></li> <li>In this case, <code>input_fps</code> will be ignored.</li> </ul> <p>This parameter is used by default if <code>--no-fps_from_filename</code> and <code>--input_fps</code> are not specified.</p>"},{"location":"OTVision/usage/convert/#no_fps_from_filename","title":"no_fps_from_filename","text":"<p>Use <code>--no-fps_from_filename</code> to prevent parsing the video frame rates from the input <code>.h264</code> filenames.</p> <ul> <li>In this case, an <code>input_fps</code> has to be specified.</li> </ul>"},{"location":"OTVision/usage/convert/#input_fps","title":"input_fps","text":"<p><code>--input_fps &lt;int&gt;</code> to set the frame rate for all <code>.h264</code> files.</p> <ul> <li><code>input_fps</code> should be an integer value above zero.</li> <li>E.g. if the input <code>.h264</code> have been recorded at 20 frames per second, specify this parameter as follows: <code>--input_fps 20</code></li> </ul> <p>If <code>--fps_from_filename</code> is used, <code>input_fps</code> will be ignored.</p> <p>This parameter is optional and defaults to <code>20</code>.</p>"},{"location":"OTVision/usage/convert/#overwrite","title":"overwrite","text":"<p><code>-o</code> or <code>--overwrite</code> to overwrite existing <code>.mp4</code> files.</p> <p><code>-no-o</code> or <code>--no-overwrite</code> to prevent overwriting existing <code>.mp4</code> files.</p> <p>This parameter is optional and defaults to <code>--overwrite</code>.</p>"},{"location":"OTVision/usage/convert/#delete-input","title":"delete input","text":"<p><code>--delete_input</code> to delete input <code>.h264</code> files after conversion to <code>.mp4</code> to save disk space.</p> <p><code>--no-delete_input</code> to keep input <code>.h264</code> files after conversion to <code>.mp4</code>.</p> <p>This parameter is optional and defaults to <code>--no-delete_input</code>.</p>"},{"location":"OTVision/usage/detect/","title":"Detect","text":""},{"location":"OTVision/usage/detect/#synopsis","title":"Synopsis","text":"<pre><code>python  detect.py   [-p paths] [-c config] [-w weights]\n                    [--conf] [--iou] [--chunksize] [--half] [--force]\n                    [-o overwrite]\n</code></pre>"},{"location":"OTVision/usage/detect/#description","title":"Description","text":"<p>If you have converted your video files to one of the supported file formats (<code>.avi</code>,<code>.mkv</code>,<code>.mov</code>,<code>.mp4</code>) or you already have such video files, you are ready to start the detection of the road users in each video frame.</p>"},{"location":"OTVision/usage/detect/#parameters","title":"Parameters","text":""},{"location":"OTVision/usage/detect/#paths-required","title":"paths (required)","text":"<p>Filename convention</p> <p>To prevent to set the start date and time in the config file for each indivdual video file, the current version of OTVision reads the start date and time from the video file names.</p> <p>Video files recorded by OTCamera already contain the start date and time in the file name. </p> <p>In case you use OTVision to process video files recorded by other camera systems, please make sure that the file name of these files contain the start date and time in the following format: <code>YYYY-MM-TT_hh-mm-ss</code></p> <p><code>-p \"path/to/video files\" \"path/to/other video files\"</code></p> <p>or</p> <p><code>--paths \"path/to/video files\" \"path/to/other video files\"</code></p> <p>One or multiple paths to video files or folders containing video files.</p> <p>This parameter is required to run <code>detect.py</code>. It has to be specified either using the CLI or in the configuration yaml file.</p>"},{"location":"OTVision/usage/detect/#config","title":"config","text":"<p><code>-c \"path/to/config file\"</code></p> <p>or</p> <p><code>--config \"path/to/config file\"</code></p> <p>Path to a custom user configuration yaml file. Other parameters (including <code>paths</code>) are parsed from this configuration file.</p> <p>This parameter is optional. By default, the <code>user_config.otvision.yaml</code> in the root directory is read. This also overrides the default values of all parameters. If the <code>user_config.otvision.yaml</code> in the root directory is missing and no other configuration file is specified, the default values of the parameters as described below are used.</p> <p>Warning</p> <p>Any parameter passed to the CLI will overwrite the respective parameter from the config file.</p>"},{"location":"OTVision/usage/detect/#weights","title":"weights","text":"<p><code>-w &lt;weights&gt;</code> or <code>--weights &lt;weights&gt;</code></p> <p>Name of weights from PyTorch hub or path to weights file.</p> <p>This parameter is optional and defaults to <code>YOLOv5s.pt</code>.</p>"},{"location":"OTVision/usage/detect/#conf","title":"conf","text":"<p><code>--conf &lt;float&gt;</code></p> <p>The YOLOv5 model confidence threshold. Should be a float value between zero and one.</p> <p>The confidence threshold is the minimum confidence for a detection to be considered a true detection (otherwise this detection will be ignored entirely).</p> <p>This parameter is optional and defaults to <code>0.25</code>.</p>"},{"location":"OTVision/usage/detect/#iou","title":"iou","text":"<p><code>--iou &lt;float&gt;</code></p> <p>The YOLOv5 model IOU threshold. Should be a float value between zero and one.</p> <p>The IOU threshold is the overlap threshold for areas of bounding boxes used in non-maximum suppression to avoid duplicate detections.</p> <p>This parameter is optional and defaults to <code>0.45</code>.</p>"},{"location":"OTVision/usage/detect/#chunksize","title":"chunksize","text":"<p><code>--chunksize &lt;int&gt;</code></p> <p>The number of frames of a video to be detected by YOLOv5 in one iteration. Should be an integer above zero.</p> <p>This parameter is optional and defaults to <code>1</code>.</p>"},{"location":"OTVision/usage/detect/#half","title":"half","text":"<p><code>--half</code> to use half precision (FP16) to speed up detection.</p> <p><code>--no-half</code> to not use half precision.</p> <p>This parameter is optional and defaults to <code>--no-half</code>.</p> <p>Half precision only works for running detection on a GPU!</p>"},{"location":"OTVision/usage/detect/#force","title":"force","text":"<p><code>--force</code> to force a reload of a YOLOv5 standard model from PyTorch hub instead of using a cached model from previous detection runs.</p> <p><code>--no-force</code> to prevent forcing this reload.</p> <p>This parameter is optional and defaults to <code>--no-force</code>.</p>"},{"location":"OTVision/usage/detect/#overwrite","title":"overwrite","text":"<p><code>-o</code> or <code>--overwrite</code> to overwrite existing <code>.otdet</code> files.</p> <p><code>-no-o</code> or <code>--no-overwrite</code> to prevent overwriting existing <code>.otdet</code> files.</p> <p>This parameter is optional and defaults to <code>--overwrite</code>.</p>"},{"location":"OTVision/usage/track/","title":"Track","text":""},{"location":"OTVision/usage/track/#synopsis","title":"Synopsis","text":"<pre><code>python  track.py    [-p paths] [-c config]\n                    [--sigma_l] [--sigma_h] [--sigma_iou] [--t_min] [--t_miss_max]\n                    [-o overwrite]\n</code></pre>"},{"location":"OTVision/usage/track/#description","title":"Description","text":"<p>With the detected road users in one or more <code>.otdet</code> files, you are ready to start tracking road users over consecutive frames and even consecutive videos.</p>"},{"location":"OTVision/usage/track/#parameters","title":"Parameters","text":""},{"location":"OTVision/usage/track/#paths","title":"paths","text":"<p><code>-p \"path/to/otdet files\" \"path/to/other otdet files\"</code></p> <p>or</p> <p><code>--paths \"path/to/otdet files\" \"path/to/other otdet files\"</code></p> <p>One or multiple paths to <code>.otdet</code> files or folders containing <code>.otdet</code> files.</p> <p>This parameter is required to run the <code>track.py</code> script. It has to be specified either using the CLI or in the configuration yaml file.</p>"},{"location":"OTVision/usage/track/#config","title":"config","text":"<p><code>-c \"path/to/config file\"</code></p> <p>or</p> <p><code>--config \"path/to/config file\"</code></p> <p>Path to a custom user configuration yaml file. Other parameters (including <code>paths</code>) are parsed from this configuration file.</p> <p>This parameter is optional. By default, the <code>user_config.otvision.yaml</code> in the root directory is read. This also overrides the default values of all parameters. If the <code>user_config.otvision.yaml</code> in the root directory is missing and no other configuration file is specified, the default values of the parameters as described below are used.</p> <p>Warning</p> <p>Any parameter passed to the CLI will overwrite the respective parameter from the config file.</p>"},{"location":"OTVision/usage/track/#sigma_l","title":"sigma_l","text":"<p><code>--sigma_l &lt;float&gt;</code></p> <p>The lower confidence threshold for the IOU tracker. Detections with confidences below <code>sigma_l</code> are not considered for tracking at all.</p> <p>This parameter is optional and defaults to <code>0.27</code>.</p>"},{"location":"OTVision/usage/track/#sigma_h","title":"sigma_h","text":"<p><code>--sigma_h &lt;float&gt;</code></p> <p>The upper confidence threshold for the IOU tracker. Tracks are only considered as valid if they contain at least one detection with a confidence above sigma_h.</p> <p>This parameter is optional and defaults to <code>0.42</code>.</p>"},{"location":"OTVision/usage/track/#sigma_iou","title":"sigma_iou","text":"<p><code>--sigma_iou &lt;float&gt;</code></p> <p>Intersection-Over-Union threshold for the IOU tracker. Two detections in subsequent frames are considered to belong to the same track if their IOU value exceeds sigma_iou and this is the highest IOU of all possible combinations of detections.</p> <p>This parameter is optional and defaults to <code>0.38</code>.</p>"},{"location":"OTVision/usage/track/#t_min","title":"t_min","text":"<p><code>--t_min &lt;int&gt;</code></p> <p>Minimum number of detections to count as a valid track. All tracks with less detections will be dismissed.</p> <p>This parameter is optional and defaults to <code>5</code>.</p>"},{"location":"OTVision/usage/track/#t_miss_max","title":"t_miss_max","text":"<p><code>--t_miss_max &lt;int&gt;</code></p> <p>Maximum number of missed detections before continuing a track. If more detections are missing, the track will not be continued.</p> <p>This parameter is optional and defaults to <code>51</code>.</p>"},{"location":"OTVision/usage/track/#overwrite","title":"overwrite","text":"<p><code>-o</code> or <code>--overwrite</code> to overwrite existing <code>.ottrk</code> files.</p> <p><code>-no-o</code> or <code>--no-overwrite</code> to prevent overwriting existing <code>.ottrk</code> files.</p> <p>This parameter is optional and defaults to <code>--overwrite</code>.</p>"},{"location":"contribute/","title":"Getting Started","text":"<p>Become a part of OpenTrafficCam</p> <p>We are happy if you contribute your own code to OpenTrafficCam. This can be bugfixes in the current code, or even completely new stuff. It is best to start at GitHub to get a first overview.</p> <p>We are also happy to receive reports about the experience using OpenTrafficCam. If you find a bug in the software or the documentation, please report it as an issue in the corresponding repository on GitHub or via email.</p> <p>From time to time we are looking to expand the core team behind OpenTrafficCam. So if you are an enthusiastic developer or engineer who has programming experience and would like to work at the intersection of mobility analysis and planning, software and hardware, feel free to get in touch :)</p> <p>If you have any questions, feel free to send us an email.</p>"},{"location":"contribute/coding/","title":"Coding (Python)","text":"<p>Coding standards are not about right and wrong or good and bad. They are about uniformity, so people can easily read and contribute to the code regardless who worked on the software before.</p> <p>For now, nearly all code is Python so we just agreed on coding conventions for the Python programming language. We basically follow the PEP 8 Style Guide for Python Code. A good summary and some additions can be found in the Hitchhikers Guide or at RealPython.com. Below we defined what we consider the most important standards.</p>"},{"location":"contribute/coding/#naming-conventions","title":"Naming Conventions","text":"<p>All names should be as short as possible but as long as necessary to understand them.</p>"},{"location":"contribute/coding/#general","title":"General","text":"<p>The following table from RealPython.com summarizes the PEP8 naming conventions:</p> Type Naming Convention Examples Function Use a lowercase word or words. Separate words by underscores to improve readability. <code>function, my_function</code> Variable Use a lowercase single letter, word, or words. Separate words with underscores to improve readability. <code>x, var, my_variable</code> Class Start each word with a capital letter. Do not separate words with underscores. This style is called camel case. <code>Model, MyClass</code> Method Use a lowercase word or words. Separate words with underscores to improve readability. <code>class_method, method</code> Constant Use an uppercase single letter, word, or words. Separate words with underscores to improve readability. <code>CONSTANT, MY_CONSTANT, MY_LONG_CONSTANT</code> Module Use a short, lowercase word or words. Separate words with underscores to improve readability. <code>module.py, my_module.py</code> Package Use a short, lowercase word or words. Do not separate words with underscores. <code>package, mypackage</code>"},{"location":"contribute/coding/#files-folder-dirs","title":"Files, Folder, Dirs","text":"Naming Convention Examples dir D:\\tmp\\ filename testfile suffix _detections file type .csv file D:\\tmp\\testfile_detections.csv path \"D:\\tmp\\\" or \"D:\\tmp\\testfile_detections.csv\""},{"location":"contribute/coding/#file-extensions","title":"File extensions","text":"<p>Files get an extension according to their content.</p> Extension Description .otdet detections as bounding boxes .ottrk trajectories in pixel (and UTM) coordinates .otrfpts reference points to convert pixel to UTM coordinates"},{"location":"contribute/coding/#code-documentation","title":"Code documentation","text":"<p>Each module, function, class or method should be described in a docstring (Google style)</p>"},{"location":"contribute/coding/#docstrings-for-modules","title":"Docstrings for Modules","text":"<p>Each file should start with the license snippet followed by a docstring describing the contents and usage of the module:</p> <pre><code>\"\"\"A one line summary of the module or program, terminated by a period.\n\nLeave one blank line.  The rest of this docstring should contain an\noverall description of the module or program.  Optionally, it may also\ncontain a brief description of exported classes and functions and/or usage\nexamples.\n\n  Typical usage example:\n\n  foo = ClassFoo()\n  bar = foo.FunctionBar()\n\"\"\"\n</code></pre>"},{"location":"contribute/coding/#docstrings-for-functions","title":"Docstrings for Functions","text":"<p>Each function must be described by a docstring:</p> <pre><code>def hello_world(message: str = \"Hello World\", author: str = \"Santa Claus\"):\n\"\"\"Deliver a message from an author to the world.\n\n    \"author says message\"\n\n    Args:\n        message (str, optional): Message to deliver. Defaults to \"Hello World\".\n        author (str, optional): Author name. Defaults to \"Santa Claus\".\n\n    Returns:\n        str: the message said\n    \"\"\"\n    msg = author + \" says \" + message\n    print(msg)\n\n    return msg\n</code></pre> <p>Note</p> <p>If you are using VS Code, you may want to use the Python Docstring Generator extension.</p>"},{"location":"contribute/coding/#comments","title":"Comments","text":"<p>If it is necessary to explain your code in between, use single line or block comments for some (or all) code that follows:</p> <pre><code># This is an example for a single line comment\n\n# This is an \n# example for a\n# block comment\n</code></pre> <p>Try to avoid inline comments.</p>"},{"location":"contribute/coding/#dependencies","title":"Dependencies","text":"<p>We try to develop OpenTrafficCam using as few dependencies as possible. For basic functionality we prefer packages that come with the Python standard distribution (like pathlib or Tkinter). However, OpenTrafficCam would not be possible without code from third party libraries, especially when it comes to videos, images, data analysis, neural networks or web development. These are the libraries we intend to use for specific functionalities throughout the whole framework (we are grateful to the authors):</p> Functionality Library Paths pathlib (distributed with Python since 3.4) Videos, Images OpenCV Neural networks PyTorch, torchvision Data handling pandas, NumPy Shape handling GeoPandas, Shapely Plotting, graphing Plotly User interface Tkinter Web Dash (dashboards)"},{"location":"contribute/coding/#lintingautoformatting","title":"Linting/Autoformatting","text":"<p>To match PEP8 specs and write pretty code we use flake8 to check against structural and style errors and black for autoformatting. Additionally the package flake8-docstrings can be used to lint the docstrings.</p> <p>The following settings are set in the <code>.flake8</code> file:</p> <pre><code>[flake8]\nmax-line-length = 88\ndocstring-convention=google\n</code></pre>"},{"location":"contribute/documentation/","title":"Documentation","text":"<p>You can help correcting, improving and extending this documentation by clicking on the \"edit\" button on top right of each page that brings you directly to the corresponding file in the documentation repository on Github. You can edit the page using the rather simple Markdown language. Then you commit your changes by choosing a meaningful commit meassage and the \"Create a new branch for this commit and start a pull request\" option. After internal review and discussion (if necessary) we will merge your pull request into the documentation repository and then it will be also visible on the site.</p> <p>If you want to create new pages, have a look at the README file of the documentation repository. There you will also find a description of how this site is rendered and how inserting a new page into the navigation works.</p> <p>The documentation site of OpenTrafficCam is generated by MkDocs using the awesome material theme. The documentation is fully written in Markdown.</p>"},{"location":"contribute/github/","title":"GitHub","text":"<p>All code of OpenTrafficCam is hosted on GitHub. For each core module a separate repository exists:</p> <ul> <li>OTCamera</li> <li>OTVision</li> <li>OTAnalytics</li> <li>OTLabels</li> </ul> <p>Following the Feature Branch Workflow each repository has a master branch, which at all times should contain a stable version of the software. All coding should be committed to separate feature branches. Once you completed coding on a feature, issue or bugfix, you create a pull request for the respective feature branch (preferably labeled using the GitHub labeling system). Your contributed code will then be reviewed by the main developers, discussed (if necessary) and merged into the master branch.</p> <p>Even without coding you can contribute to the project: If you encounter any bug or got any idea for enhancement, feel free to open an issue in the corresponding repository. Every issue should at least have two labels assigned: One label indicating the maturaty of the issue (question, bug, feature or idea) and one assigning the repository-specific subpackage or section (black colored labels).</p> <p>All pull request will be checked by GitHub's super-linter.</p>"},{"location":"contribute/gui/","title":"GUI","text":"<p>For now we decided on using PysimpleGUI to build the OpenTrafficCam user interface, as it is lightweight and easy to start with. Whenever possible, we try to separate the gui code from all other code. All the gui modules have a place in the \"gui\" subpackage of the respective OT repository:</p> <ul> <li>OTCamera/OTCamera/gui</li> <li>OTVision/OTVision/gui</li> <li>OTAnalytics/OTAnalytics/gui</li> </ul> <p>For the future we can also imagine using a web framework for user interaction. This would also help to enable remote access to local GPU machines or high performance computing centers.</p>"},{"location":"contribute/otcamera/","title":"OTCamera Dev Version","text":"<p>The Raspberry Pi Zero is wonderful to use outside (it consumes little power, is cheap, ...). But the development of OTCamera is more fun on a big Pi. Everything is much faster.</p> <p>You will need</p> <ul> <li>a prepared Raspberry Pi 2B/3B(+)/4 (the 2 GB works well)</li> <li>Camera module (no USB webcam)</li> </ul>"},{"location":"contribute/otcamera/#setup-vs-code-remote-development-extension","title":"Setup VS Code Remote Development Extension","text":"<p>Install the Remote-SSH extension on your desktop using the marketplace.</p> <p>Add the Pi as remote host. Connect to the Pi using the Remote-SSH extension (rightclick on the ne host - \"Connect to Host in New Window\"). When asked for the operating system of the host, choose \"Linux\". VS code will download and install the necessary dependencies on the Pi.</p> <p>Open the extension manager in the new windows an install all necessary extensions.</p> <p>Install a virtual environment and all requirements.</p> <pre><code>sudo apt install python3-venv -y\npython -m venv venv --upgrade-deps\npip install -r requirements.txt -r requirements-dev.txt -U\n</code></pre>"},{"location":"contribute/otcamera/#setup-git-and-github","title":"Setup Git and GitHub","text":"<p>If not already done install git using apt.</p> <pre><code>sudo apt install git -y\n</code></pre> <p>To setup your git commit name and email, login to your GitHub account and copy your private commit email.</p> <p>On the pi run</p> <pre><code>git config --global user.name \"Your Name\"\ngit config --global user.email \"123456+username@users.noreply.github.com\"\n</code></pre> <p>The easiest way to setup your GitHub credentials is to use vs code. In the file explorer panel click \"Clone Repository\" and choose \"Clone from GitHub\". Depending on your desktop computer settings, a browser tab will open to login into your GitHub account. Afterwards, you can search for \"OpenTrafficCam/OTCamera\" inside the vs code command prompt and select a folder to pull (choose /home/pi by default).</p>"},{"location":"contribute/otcamera/#install-screen-to-run-otcamera-in-background","title":"Install Screen to Run OTCamera in Background","text":"<p>Quote</p> <p>Screen is a full-screen window manager that multiplexes a physical terminal between several processes, typically interactive shells.</p> <p>To install screen on the developer pi just run</p> <pre><code>sudo apt install screen -y\n</code></pre> <p>You can now start <code>screen</code> and use it as an usual terminal. You can run the <code>python OTCamera</code> to start the camera in an active screen session. Hit Ctrl+A and then D to detach the current session. Screen will still run in background including the OTCamera. If you start screen including the <code>-r</code> option you can reconnect to this terminal session:</p> <pre><code>screen -r\n</code></pre> <p>You can safely disconnect you ssh connection to the pi, while the screen session is still running.</p>"},{"location":"contribute/otcamera/#ready-to-develop","title":"Ready to Develop","text":"<p>You should now be ready to pull the OTCamera repository and start developing.</p>"},{"location":"contribute/vscode/","title":"VS Code","text":"<p>We are developing OpenTrafficCam using Visual Studio Code using the following extensions:</p> <ul> <li>GitHub Pull Requests and Issues</li> <li>markdownlint</li> <li>Pylance</li> <li>Python</li> <li>Python Docstring Generator</li> <li>yaml Language Support</li> </ul>"},{"location":"contribute/vscode/#pylance-settings","title":"Pylance Settings","text":"<p>To solve import errors in the repository, we need to add an extra path to the pylance config.</p> <p>For example in the workspace settings of OTCamera add:</p> <pre><code>\"python.analysis.extraPaths\": [\n\"./OTCamera\"\n]\n</code></pre> <p>Or set it using the Settings-UI (Workspace -&gt; Pylance -&gt; Python - Analysis: Extra Path -&gt; add './OTCamera')</p>"},{"location":"contribute/vscode/#snippets","title":"Snippets","text":"<p>To comfortably insert code use snippets. You can open the configuration file for a specific language to add or modify the user snippets: <code>Ctrl</code> + <code>Shift</code> + <code>p</code> and <code>Preferences: Configure User Snippets</code>.</p> <p>You can insert the user snippets by start typing the \"prefix\" value. For example start typing <code>gpl_</code> and autocompletection should recognize the user snippets stated below.</p>"},{"location":"contribute/vscode/#gpl-license-information","title":"GPL License Information","text":"<p>Add the following snippet to <code>vscodedata/user/snippets/python.json</code></p> <pre><code>\"gpl_license_header\": {\n\"prefix\": \"gpl_add_head\",\n\"body\":[\n\"$LINE_COMMENT ${1:Program Name and Function}\",\n\"$LINE_COMMENT Copyright (C) $CURRENT_YEAR OpenTrafficCam Contributors\",\n\"$LINE_COMMENT &lt;https://github.com/OpenTrafficCam\",\n\"$LINE_COMMENT &lt;team@opentrafficcam.org&gt;\",\n\"$LINE_COMMENT\",\n\"$LINE_COMMENT This program is free software: you can redistribute it and/or modify\",\n\"$LINE_COMMENT it under the terms of the GNU General Public License as published by\",\n\"$LINE_COMMENT the Free Software Foundation, either version 3 of the License, or\",\n\"$LINE_COMMENT (at your option) any later version.\",\n\"$LINE_COMMENT\",\n\"$LINE_COMMENT This program is distributed in the hope that it will be useful,\",\n\"$LINE_COMMENT but WITHOUT ANY WARRANTY; without even the implied warranty of\",\n\"$LINE_COMMENT MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\",\n\"$LINE_COMMENT GNU General Public License for more details.\",\n\"$LINE_COMMENT\",\n\"$LINE_COMMENT You should have received a copy of the GNU General Public License\",\n\"$LINE_COMMENT along with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\",\n\"\",\n],\n\"description\": \"Add GPLv3 license information in source code.\"\n}\n</code></pre> <p>Add a short description and use tab to jump to the end of the snippet.</p>"},{"location":"contribute/vscode/#mkdocs-configuration-support","title":"MKDocs Configuration Support","text":"<p>In order to minimize friction and maximize productivity, Material for MkDocs provides its own schema.json1 for mkdocs.yml. If your editor supports YAML schema validation, it's definitely recommended to set it up.</p> <p>Source</p>"},{"location":"overview/dataprivacy/","title":"Data Privacy (Germany)","text":"<p>German law only</p> <p>The following FAQ on data protection are intentionally only available in German, as they refer exclusively to German law. They also do not apply to other countries with German as an official language (e.g. Austria, Switzerland).</p> <p>Die nachfolgenden FAQ zum Datenschutz sind mit Absicht nur in deutscher Sprache verf\u00fcgbar, da sie sich ausschlie\u00dflich auf deutsches Recht beziehen.  Sie gelten daher auch nicht f\u00fcr andere L\u00e4nder mit Deutsch als Amtssprache (z. B. \u00d6sterreich, Schweiz).</p>"},{"location":"overview/dataprivacy/#vorwort","title":"Vorwort","text":"<p>In einem Workshop im Rahmen der durch das BMVI gef\u00f6rderten mFund-Machbarkeitsstudie zu OpenTrafficCam haben wir gemeinsam mit anderen Verkehrsfachleuten h\u00e4ufige Fragen zum Datenschutz bei der Videoerhebung des Stra\u00dfenverkehrs gesammelt. Diese Fragen sind nachfolgend aufgelistet.</p> <p>Die Beantwortung der folgenden Fragen erfolgte in mehreren Interviews mit dem Datenschutzbeauftragten der TU Dresden. Sie dienen als initiale Informationsquelle zur Einhaltung des Datenschutzes bei der Videoaufzeichnung in Forschungsprojekten. Sie ersetzen keine Rechtsberatung und k\u00f6nnen verst\u00e4ndlicherweise nicht jedes einzelne Verfahren sowie die rechtlichen Spezifika der Bundesl\u00e4nder ber\u00fccksichtigen.</p> <p>Deshalb ist es auf jeden Fall erforderlich, die/den jeweils zust\u00e4ndigen Datenschutzbeauftragten fr\u00fchzeitig in das konkrete Projekt einzubeziehen. Als ein Ergebnis der Gespr\u00e4che hat sich auch gezeigt, dass zur Kl\u00e4rung detaillierter und \u00fcber den Forschungsbereich hinausgehender Fragen des Datenschutzes bei der automatisierten, videobasierten Verkehrserfassung eine dezidierte Besch\u00e4ftigung einer spezialisierten Anwaltskanzlei zielf\u00fchrend sein kann. Zuk\u00fcnftig ist eine Erweiterung der FAQ um Inhalte zum Datenschutz bei der Videoerhebung in nichtwissenschaftlichen Projekten geplant.</p>"},{"location":"overview/dataprivacy/#faq-zum-datenschutz-bei-videobasierter-verkehrserfassung-in-forschungsprojekten","title":"FAQ zum Datenschutz bei videobasierter Verkehrserfassung in Forschungsprojekten","text":""},{"location":"overview/dataprivacy/#grundsatzliches","title":"Grunds\u00e4tzliches","text":"Wer ist in einem Projekt zust\u00e4ndig f\u00fcr den Datenschutz und tr\u00e4gt die Verantwortung? Die Auftragnehmenden oder die Auftraggebenden? <p>Bei Forschungsf\u00f6rderung: Die erhebende Institution ist verantwortlich.  Hier ist der Forschungsparagraph des jeweiligen Landesdatenschutzgesetzes relevant, der relativ viele Freiheitsgrade beinhaltet. Dementsprechend ist die Freiheit von Forschung und Lehre besonders sch\u00fctzenswert und ist mit dem Recht auf Schutz personenbezogener Daten abzuw\u00e4gen. Grundsatz dabei ist immer die weitgehende Minimierung der Eingriffe in das Pers\u00f6nlichkeitsrecht. </p> <p>Bei Forschungsauftr\u00e4gen: Hier kommt es darauf an, in wessen Interesse die Datenerhebung liegt.</p> <p>Wenn die Forschungsmethodik durch die Auftraggebenden vorgegeben wird oder die Forschungsfrage nicht mit alternativen Erhebungsdesigns beantwortet werden kann (z.B. falls Videobeobachtungen in geringer Aufl\u00f6sung ohne die Erfassung personenbezogener Daten daf\u00fcr nicht ausreichen), liegt die Verantwortung aus rechtlicher Sicht allein beim Auftraggeber. Dies gilt auch, wenn die Auftragnehmenden die (zu diesem Zeitpunkt bereits anonymisierten) Daten \u00fcber den Projektgegenstand hinaus publizieren (es z\u00e4hlt der initiale Zweck der Datenerfassung). In diesem Fall sind die Auftraggebenden nicht nur verpflichtet, nach einem Datenschutzkonzept zu fragen. Sie sind auch dazu verpflichtet, Vorgaben f\u00fcr den Datenschutz in der Ausschreibung zu formulieren und Angebote darauf zu pr\u00fcfen, dass diese eigenen Vorgaben des Datenschutzes eingehalten werden.</p> <p>Wenn die Ausschreibung und die zu beantwortenden Forschungsfragen es zulassen, dass die Auftragnehmenden das Erhebungsdesign frei w\u00e4hlen, gilt die gemeinsame Verantwortlichkeit der Auftraggebenden und Auftragnehmenden gem\u00e4\u00df Art. 26 DSGVO (\u201cgemeinsames Interesse an der Datenauswertung\u201d / Gemeinsame Verantwortlichkeit).</p> Ist dar\u00fcber hinaus eine Instanz (z. B. Stra\u00dfenbaulasttr\u00e4ger) befugt, eine Messung aus Gr\u00fcnden des Datenschutzes zu verbieten? <p>Nein, weder der Stra\u00dfenbaulasttr\u00e4ger, noch eine Gebietsk\u00f6rperschaft oder der entsprechende Landesdatenschutzbeauftragte d\u00fcrfen die Datenerhebung auf \u00f6ffentlichen Fl\u00e4chen aus Gr\u00fcnden des Datenschutzes erlauben oder verbieten. Die Einhaltung des Datenschutzes liegt wie bereits beschrieben entweder in der Verantwortung der Forschungseinrichtung oder des Auftraggebers.</p> <p>Achtung!</p> <p>Neben dem Datenschutz k\u00f6nnten jedoch der Stra\u00dfenbaulasttr\u00e4ger bzw. die Gebietsk\u00f6rperschaft aufgrund ihrer Verkehrssicherungspflicht Einw\u00e4nde haben oder aufgrund der Montage von Kameras an Einbauten oder B\u00e4umen in deren Zust\u00e4ndigkeit.</p> Welche sind dabei die relevanten Rechtsvorschriften und wo/f\u00fcr wen gelten sie? Gibt es bereichsspezifische Regelungen? <p>Grunds\u00e4tzlich gilt zun\u00e4chst immer die europ\u00e4ische DSGVO, die jedoch gewisse Punkte offenl\u00e4sst, die wiederum von den Gesetzen nachgeordneter Gebietsk\u00f6rperschaften geregelt werden.</p> <p>F\u00fcr die Privatwirtschaft und Institutionen des Bundes gilt dabei spezialgesetzlich das BDSG.</p> <p>F\u00fcr \u00f6ffentliche Einrichtungen der Bundesl\u00e4nder (also die meisten Hochschulen), der Landkreise und kreisfreien St\u00e4dte gilt das jeweilige Landesdatenschutzgesetz, welches sich in der Regel bzgl. Forschung nicht stark vom BDSG unterscheidet.</p> <p>Der Geltungsbereich bereichsspezifischer Regelungen wird also durch die rechtliche Zuordnung der Institution zu einer Gebietsk\u00f6rperschaft bestimmt und nicht durch den Ort der Datenerhebung. Wenn mehrere Institutionen gemeinsam f\u00fcr den Datenschutz verantwortlich sind, gilt f\u00fcr jede das f\u00fcr sie relevante Gesetz bzw. die Gesetze.</p> Gibt es Unterschiede je nach Erhebungszweck bzw. Zweck des Projekts? <p>Es gibt Unterschiede zwischen Datenerhebungen zum Zweck der Forschung und anderen Zwecken wie Wirtschaft oder Verwaltung (z.B. Wahrung der \u00f6ffentlichen Sicherheit) sowie in der Verantwortung der Beteiligten. Ansonsten sind keine weiteren Unterschiede nach dem Erhebungszweck bekannt.</p> Wie \"dehnbar\" ist das Datenschutzrecht, wie unterschiedlich kann es ausgelegt werden? <p>Es existieren tats\u00e4chlich viele Grauzonen aufgrund der Nichtregulierung spezifischer Einzelf\u00e4lle (zu denen auch die videobasierte, automatisierte Verkehrserfassung f\u00fcr die Forschung und die Planung z\u00e4hlt), die im Falle rechtlicher Streitigkeiten separat bewertet werden m\u00fcssen. </p> <p>Wenn f\u00fcr rechtliche Fragestellungen keine dezidierten Rechtsnormen vorliegen, kann sich ein Gericht einerseits auf Pr\u00e4zedenzurteile berufen. Zur videobasierten Verkehrserfassung sind bisher jedoch nur wenige Urteile bekannt (siehe Frage \"Gibt es Pr\u00e4zedenz-Urteile zur videobasierten Verkehrserfassung?\").</p> <p>Andererseits ist eine rechtliche Anerkennung niedergeschriebener Branchenstandards m\u00f6glich, die in Zusammenarbeit zwischen den datenerhebenden Institutionen und den Aufsichtsbeh\u00f6rden erstellt wurden. Ein Beispiel daf\u00fcr ist der Arbeitskreis \"Datenschutz\" f\u00fcr Gesundheitsdaten, der in Abstimmung mit den Aufsichtsbeh\u00f6rden ein Rahmenkonzept entwickelt hat, auf das sich im Falle rechtlicher Streitigkeiten berufen werden kann. Zur Schaffung einer gewissen Rechtssicherheit bei den Details der videobasierten Verkehrserhebung f\u00fcr die Forschung und Planung kann ein \u00e4quivalentes Vorgehen sinnvoll sein.</p> Gibt es Pr\u00e4zedenz-Urteile zur videobasierten Verkehrserfassung? <p>Urteile zur Verkehrserfassung per Video zum Zweck der allgemeinen Verkehrsdatenerhebung f\u00fcr oder zur Unterst\u00fctzung von Forschung oder Verwaltungst\u00e4tigkeiten sind uns nicht bekannt. Folgende Urteile zum Datenschutz bei der Bilderfassung im Stra\u00dfenverkehr sind dar\u00fcber hinaus bekannt:</p> OLG Frankfurt, 06.11.2019 - 2 Ss - OWi 942/19 <p>Das OLG Frankfurt hat 2019 entschieden, dass private Dienstleister keine Verkehrs\u00fcberwachungen durf\u00fchren d\u00fcrfen und entsprechende Bu\u00dfgeldbescheide gesetzeswidrig sind.</p> BGH 15.05.2018 \u2013 VI ZR 233/17 <p>Der Bundesgerichtshof hat 2018 das sogenannte Dashcam-Urteil gef\u00e4llt, nachdem es f\u00fcr private Personen datenschutzrechtlich unzul\u00e4ssig ist, personenbezogene Merkmale mit einer Kamera aus einem fahrenden Auto \u201cpermanent und anlasslos\u201d aufzuzeichnen. Kameras mit Ringspeicher, die das Geschehen nur im Falle eines Unfalls permanent speichern, w\u00e4ren demnach zul\u00e4ssig. Im entsprechenden Fall hat das Gericht die permanente Videoaufzeichnung dennoch als Beweismittel zugelassen.</p> Muss eine Verkehrskamera als solche erkennbar und ausgewiesen sein oder darf man \"verdeckt\" filmen? <p>Falls keine personenbezogenen Daten erfasst werden, ist keine Information der Verkehrsbeteiligten notwendig.</p> <p>Falls personenbezogene Daten erfasst werden, ist grunds\u00e4tzlich eine Einverst\u00e4ndniserkl\u00e4rung der jeweiligen Personen einzuholen. Da dies bei Videobeobachtungen im Verkehrsraum meist nicht m\u00f6glich ist, soll stattdessen eine gute erkennbare Information der Verkehrsteilnehmenden erfolgen (z.B. \u00fcber ein auch f\u00fcr Kfz-F\u00fchrende gut lesbares Schild \u201cVerkehrserhebung\u201d) und Detailinformationen zur Erhebung an bzw. in der N\u00e4he der Kamera (mind. erhebende Institution und Kontaktinformationen, weitere Details z.B. auf einer Website, die per QR-Code erreichbar ist).</p> <p>Falls personenbezogene Daten erfasst werden und im Rahmen von Forschungsprojekten eine Information der Verkehrsteilnehmenden \u00fcber die Verkehrserhebung die Verl\u00e4sslichkeit der angestrebten Ergebnisse beeintr\u00e4chtigen w\u00fcrde (z.B. bei der Untersuchung regelkonformen oder sicherheitsrelevanten Verkehrsverhaltens), kann in Einzelf\u00e4llen darauf verzichtet werden. Grundlage hierf\u00fcr ist der Forschungsparagraph des jeweiligen Landesdatenschutzgesetzes, nach dem die Freiheit von Forschung und Lehre besonders sch\u00fctzenswert ist und das Pers\u00f6nlichkeitsrecht \u00fcberwiegen kann. Dieses Vorgehen sollte jedoch ausf\u00fchrlich begr\u00fcndet und von Beginn an dokumentiert werden, um etwaigen Klagen durch Verkehrsteilnehmende vor Gericht standzuhalten. Eine Abstimmung mit der zust\u00e4ndigen Stelle f\u00fcr Informationssicherheit wird dringend empfohlen.</p>"},{"location":"overview/dataprivacy/#vermeidung-der-erfassung-personenbezogener-daten","title":"Vermeidung der Erfassung personenbezogener Daten","text":"Wie sind personenbezogene Daten definiert? Z\u00e4hlen dazu neben Gesicht und Nummernschild auch die Gr\u00f6\u00dfe der Person, die Art ihres Gangs, die Art und Farbe ihrer Kleidung oder auff\u00e4llige Frisuren? <p>Laut Art. 4 Abs. 1 DSGVO sind alle Namen, Kennnummern, Standortdaten und Merkmale von nat\u00fcrlichen Personen als \u201cpersonenbezogene Daten\u201d definiert, die R\u00fcckschl\u00fcsse auf deren Identit\u00e4t erlauben. Es gilt als grunds\u00e4tzlich anerkannt, dass im Bereich des Stra\u00dfenverkehrs das Kfz-Kennzeichen und das Gesicht einer Person als personenbezogene Daten gelten. In Einzelf\u00e4llen k\u00f6nnte auch die Kombination anderer Merkmale wie Gr\u00f6\u00dfe, Kleidung und Frisur mit weiteren Informationen (z.B. dem Standort) die Identifikation einer Person erm\u00f6glichen. Hier liegt die Beweispflicht jedoch bei potentiellen Klagenden und die Identifikation der Person im Bild/Video muss beispielhaft durch unabh\u00e4ngige Personen nachgewiesen werden (d.h. es reicht nicht aus, wenn die Person selbst oder eine ihr bekannte Person sie im Bild/Video identifizieren kann). Es sind Klagen und Urteile zur Identifikation von Personen auf Fotographien des \u00f6ffentlichen Raums bekannt, in denen f\u00fcr die Kl\u00e4ger entschieden wurde \u2013 diese sind jedoch nicht einfach auf Videoerhebungen im Stra\u00dfenverkehr zum Zwecke der allgemeinen Verkehrsdatenerfassung \u00fcbertragbar.</p> Welche Entfernung zur Kamera z\u00e4hlt f\u00fcr die Datenschutzkonformit\u00e4t? <p>Die Kamera ist grunds\u00e4tzlich so zu konfigurieren, dass nur der relevante Untersuchungsbereich erfasst wird. Wenn dies technisch nicht m\u00f6glich ist, dann muss im Nachgang eine Sichtung und L\u00f6schung erfolgen. Dies gilt z. B. auch f\u00fcr F\u00e4lle, in denen eine Person sich aktiv auf H\u00f6he der Kamera begibt, um diese aus der N\u00e4he zu betrachten und dadurch selbst deutlich erkennbar wird. Sichtung und L\u00f6schung k\u00f6nnen auch automatisiert im Nachgang erfolgen (technische Ma\u00dfnahme, falls z.B. Objekte automatisiert detektiert werden und ein Grenzwert f\u00fcr die Gr\u00f6\u00dfe des erfassten Objekts \u00fcberschritten wird).</p> Reicht es aus, in den Videos sichtbare Gesichter und Kennzeichen zu verpixeln? Wann und wie muss das geschehen? <p>Grunds\u00e4tzlich sollte wenn m\u00f6glich bereits durch niedrige Aufl\u00f6sung und De-Fokussierung die Erfassung von Kennzeichen und Gesichtern vermieden werden.</p> <p>Wenn dies f\u00fcr bestimmte, nicht interessierende Bildbereiche nicht m\u00f6glich ist, kann als erste Alternative eine Nicht-Erfassung (z. B. Schw\u00e4rzung) dieser Bildbereiche bereits im Kamerasystem gepr\u00fcft und technisch umgesetzt werden.</p> <p>Wenn auch dies nicht m\u00f6glich ist, k\u00f6nnen die entsprechenden Bildbereiche direkt auf der Kamera live und permanent verpixelt und somit anonymisiert werden. Diese Methode setzt eine automatisierte Detektion des Kennzeichens bzw. Gesichts voraus. Wenn daraus keine weiteren Daten abgeleitet und gespeichert oder \u00fcbermittelt werden (z.B. das Kennzeichen als Klartext), gilt diese Methode als anerkannt, da sie dem Stand der Technik entspricht. Vor dem Einsatz dieser Methode wird jedoch dringend empfohlen, die Konformit\u00e4t mit der entsprechend geltenden Rechtsgrundlage zu pr\u00fcfen.</p> <p>Das zeitversetzte, nachtr\u00e4gliche (manuelle oder automatische) Verpixeln von Kennzeichen oder Gesichtern bei der Sichtung/Auswertung des Videomaterials sollte auf Einzelf\u00e4lle beschr\u00e4nkt bleiben (siehe Frage \"Welche Entfernung zur Kamera z\u00e4hlt f\u00fcr die Datenschutzkonformit\u00e4t?\"). Sollten die zuvor genannten Ma\u00dfnahmen nicht zielf\u00fchrend sein und sind dennoch geh\u00e4uft Kennzeichen oder Gesichter in den Videos zu vermuten, kann nicht mehr von einer anonymen Erhebung (Vermeidung der Erhebung personenbezogener Daten) gesprochen werden. In diesem Fall m\u00fcssen die nachfolgend beschriebenen Rahmenbedingungen f\u00fcr die Erhebung personenbezogener Daten eingehalten werden.</p>"},{"location":"overview/dataprivacy/#ablauf-in-fallen-in-denen-die-erfassung-personenbezogener-daten-nicht-vermieden-werden-kann","title":"Ablauf in F\u00e4llen, in denen die Erfassung personenbezogener Daten nicht vermieden werden kann","text":"Unter welchen Umst\u00e4nden d\u00fcrfen personenbezogene Daten erhoben werden? <p>Grunds\u00e4tzlich ist zu pr\u00fcfen, ob f\u00fcr den Erhebungszweck auch eine Videobeobachtung ohne die Erfassung personenbezogener Daten (bzw. eine Methode g\u00e4nzlich ohne Videobeobachtung) m\u00f6glich ist. Wenn dies nicht m\u00f6glich ist und das Interesse der Allgemeinheit an den Ergebnissen der Videobeobachtung die Interessen der einzelnen Personen (deren Datenschutz) \u00fcberwiegt, ist eine videobasierte Erfassung personenbezogener Daten grunds\u00e4tzlich denkbar.</p> <p>Dabei muss die Zustimmung der betroffenen Personen eingeholt werden oder zumindest eine Information der betroffenen Personen erfolgen (siehe Frage \u201cMuss eine Verkehrskamera als solche erkennbar und ausgewiesen sein oder darf man \"verdeckt\" filmen?\u201d).</p> Was beinhaltet ein Datenschutzkonzept und in welchen F\u00e4llen braucht man es? Wer muss dieses \"absegnen\"? <p>Die Erstellung eines Datenschutzkonzepts ist nicht in allen F\u00e4llen n\u00f6tig, in denen die Erhebung personenbezogener Daten geplant ist. Es ist jedoch sinnvoll, ein solches Konzept zu erstellen, da eine Rechenschaftspflicht besteht, d.h. eine Dokumentation zu f\u00fchren ist. In der DSGVO und in den Landesdatenschutzgesetzen ist f\u00fcr viele Aspekte auch grunds\u00e4tzlich geregelt, ob und wie personenbezogene Daten erhoben werden d\u00fcrfen. Wenn die Konformit\u00e4t mit diesen Rechtsgrundlagen eingehalten wird, reicht es aus, dies im Nachgang belegen zu k\u00f6nnen. Falls diesbez\u00fcglich Unklarheiten existieren, empfiehlt sich die R\u00fccksprache mit Datenschutzfachleuten (z.B. den zust\u00e4ndigen Beauftragten f\u00fcr Informationssicherheit und dem/der Datenschutzbeauftragten). In Zusammenarbeit mit diesen kann au\u00dferdem in F\u00e4llen, in denen vereinzelte Grunds\u00e4tze der Verarbeitung personenbezogener Daten nicht eingehalten werden k\u00f6nnen, eine sogenannte \u201cDatenschutzfolgenabsch\u00e4tzung\u201d erarbeitet werden. In dieser werden Erhebungszweck und -design, der Umgang mit den Daten sowie Gr\u00fcnde f\u00fcr die vereinzelten Abweichungen von den Rechtsgrundlagen detailliert erl\u00e4utert.</p> Sind die Daten bereits w\u00e4hrend der Messung vor Diebstahl zu sch\u00fctzen? Ist es datenschutzkonform m\u00f6glich, einen externen Zugang auf die Kamera w\u00e4hrend der Messung herzustellen? <p>Wenn personenbezogenen Daten erfasst werden, muss bereits w\u00e4hrend der Erfassung technisch sichergestellt werden, dass unbefugte Personen keinen Zugriff auf diese Daten haben. Daher sollten die Daten auf Kameras im Verkehrsraum nach dem aktuellen Stand der Technik gesch\u00fctzt werden. Momentan k\u00f6nnte dies durch ein sicheres Passwort erfolgen. Falls personenbezogene Daten \u00fcbermittelt werden, sind verschl\u00fcsselte Verbindungen zu empfehlen (z. B. per SSH, HTTPS, VPN).</p> Unter welchen Umst\u00e4nden darf man die Videos speichern und aufheben? D\u00fcrfen die Videos f\u00fcr einen anderen als f\u00fcr den urspr\u00fcnglichen Zweck verwendet werden? <p>Personenbezogene Daten m\u00fcssen so gespeichert werden, dass ausschlie\u00dflich befugte Personen Zugriff zu diesen haben. Au\u00dferdem d\u00fcrfen sie grunds\u00e4tzlich nur so lange gespeichert werden bis deren spezifischer Erhebungszweck erf\u00fcllt wurde (also z.B. die spezifische Forschungsfrage innerhalb eines Projekts beantwortet wurde). Anschlie\u00dfend m\u00fcssen die Daten gel\u00f6scht oder um die Eigenschaften reduziert werden, die einen Personenbezug erm\u00f6glichen (bei Videos z.B. Verringerung der Aufl\u00f6sung). Personenbezogene Daten d\u00fcrfen nur dann auch nach Erf\u00fcllung des spezifischen Erhebungszwecks vorgehalten werden, wenn die betroffenen Personen dem ausdr\u00fccklich zugestimmt haben. Vor der Verwendung gespeicherter personenbezogener Daten f\u00fcr weitere Zwecke ist jedoch erneut die Konformit\u00e4t des Zwecks sowohl mit den Rechtsgrundlagen als auch mit dem Wortlaut der Einwilligung der betroffenen Personen zu pr\u00fcfen.</p> Darf ich die Verkehrsvideos weitergeben? Was muss ich dabei beachten? (Stichworte \"Nachnutzbarkeit\" und \"Datensparsamkeit\") <p>Wenn es sich um personenbezogene Daten handelt und der \u00f6ffentliche F\u00f6rder- oder Auftraggeber f\u00fcr den Datenschutz verantwortlich ist, liegt diese Entscheidung bei ihm (siehe Frage \"Wer ist in einem Projekt zust\u00e4ndig f\u00fcr den Datenschutz und tr\u00e4gt die Verantwortung? Die Auftragnehmenden oder die Auftraggebenden?\").</p> <p>Wenn der F\u00f6rder- oder Auftragnehmer selbst f\u00fcr den Datenschutz verantwortlich ist und die Anfrage von \u00f6ffentlichen Stellen (z.B. Gebietsk\u00f6rperschaft, Beh\u00f6rde, Universit\u00e4t) kommt, ist die anfragende \u00f6ffentliche  Stelle auch f\u00fcr die \u00dcbermittlung verantwortlich. Das bedeutet, dass die anfragende Stelle sicherstellen muss, dass die \u00dcbermittlung der Daten gerechtfertigt ist.</p> <p>Einer \u00dcbermittlung an eine \u00f6ffentliche Stelle steht also nichts im Wege. Dies gilt auch, falls die \u00f6ffentliche Stelle die Daten f\u00fcr einen anderen als den urspr\u00fcnglichen Erhebungszweck verarbeiten m\u00f6chte (diese \u00f6ffentliche Stelle muss dann selbst pr\u00fcfen, ob dies rechtskonform ist). Vor einer \u00dcbermittlung an eine nicht\u00f6ffentliche Stelle (z.B. an ein Ingenieurb\u00fcro) empfehlen wir dringend die Konsultation von Fachleuten f\u00fcr Datenschutz. Die \u00dcbermittlung anonymisierter Verkehrsvideos stellt aus datenschutzrechtlicher Sicht nat\u00fcrlich kein Problem dar und kann aus dieser Hinsicht an alle Stellen erfolgen.</p>"},{"location":"overview/usecases/trafficcounts/","title":"Traffic Counts","text":"<p>Coming soon</p> <p>Unfortunately, there is no content here yet. But we are currently working on completing this website.</p>"},{"location":"overview/usecases/trafficsafety/","title":"Traffic Safety","text":""},{"location":"overview/usecases/trafficsafety/#the-rest-of-the-iceberg","title":"The rest of the iceberg","text":"<p>Traditionally, accident data has primarily been used to analyse the road safety of a traffic facility. However, this is a reactive approach, which means that accident occurrences must first be observed over many years before measures can be taken to improve road safety. In addition, the accident statistics show a certain number of unreported cases and there is only limited information on the sequence of events that led to the accident.</p> <p>Assuming that accidents are only the tip of the iceberg, traffic flow can also be observed and analysed in terms of conflicts or near misses in a complementary or supplementary manner. Near misses or traffic conflicts are defined by Amundsen and Hyd\u00e9n (1977) as \"observable situations in which two or more road users approach each other in space and time to such an extent that there is a risk of collision if their movements remain unchanged\". In a simplified way, the traffic flow can be divided into the following hierarchy of severity: Undisturbed passages - Interactions - Near misses/Conflicts - Accidents. The near misses (similar to accidents) can be divided into further severity levels with regard to the proximity to a collision, but also with regard to the severity of the resulting collision (similar to accident severity).</p> <p></p>"},{"location":"overview/usecases/trafficsafety/#pet-easy-peasy","title":"PET - easy peasy","text":"<p>A number of Surrogate Measures of Safety have been developed as objective indicators to classify an interaction as a near miss and to assess its severity (primarily in terms of proximity to a collision). One commonly used indicator is PET (Post Encroachment Time), which is a rather simple safety indicator. It describes the time period between the first road user leaving the common conflict area and the second one entering it.</p> <p></p>"},{"location":"overview/usecases/trafficsafety/#ttc-complex-but-meaningful","title":"TTC - complex, but meaningful","text":"<p>Another widely applied safety indicator in surrogate analysis is Time To Collision (TTC). It describes the remaining time to a collision between two road users at a certain point in time of their interaction and under the assumption that neither of the parties involved will take an evasive action. The lower the TTC, the more severe the conflict between road users. In case of a collision the TTC equals zero.</p> <p>A TTC can only be calculated if the two interacting road users are on a collision course. If the collision course is left due to an evasive action, the TTC for the time from the start of the evasive action is calculated on the basis of the hypotethic trajectories without the evasive action. Such hypotethic trajectories can be approximated by various methods of trajectory prediction. The simplest (but in some cases also the most unrealistic) method for trajectory prediction is to assume continued motion with constant velocity vectors.</p> <p>The TTC is calculated for each moment of interaction. This results in a TTC curve. Interactions and also periods of interactions are often categorised according to whether the TTC is below a threshold value (e.g. 1.5 sec). The Minimum TTC, the Time Exposed TTC and the Time Integrated TTC are used as aggregated indicators.</p> <p>In addition to road safety analysis, TTC is also used for active accident avoidance, especially for decision-making in algorithms of autonomous vehicles.</p> <p></p>"},{"location":"overview/usecases/trafficsafety/#best-practices","title":"Best practices","text":"<p>In order to enable comparisons, e.g. with other traffic facilities, safety indicators like PET and TTC are related to suitable exposure units, such as traffic volume or the total number of interactions in the observation period. For comparisons one also needs statistcal significance of the frequency of conflicts at every individual location. To achieve this one must capture trajectories over a sufficient period of time. In most constellations, conflicts do occur more frequently than accidents that require observation periods of several years. However, recent studis show, that capturing videos over weeks or at least multiple days is the minimum required duration for conflict analyses. One mistake that also should be avoided is to raise the threshold for categorising encounters as conflicts, as this means measuring exposure, but not road safety.</p>"},{"location":"overview/usecases/trafficsafety/#it-s-all-about-validity","title":"It\u00b4s all about validity","text":"<p>Exposure based conflict frequencies can then be compared over different road sections or intersections or to known mean values. In addition, in the past, fixed factors for estimating the number of accidents were derived from the number of conflicts, which, however, could often not be confirmed by other researchers. To overcome fixed coefficients, a new approach taking into account variability of conflict severity based on Extreme Value Theory (EVT) is now state of the art, that has also been applied in stock market and flood protection earlier.</p> <p>Conflicts are even more valid with crash data when it comes to relative safety asssessment, e.g. ranking a number of road sites by traffic safety, which should deliver nearly the same results as using crash data from several years. This relative validity is sufficient for many applications in road safety work.</p> <p>Another advantage of the safety analysis of trajectories is that the entire movements of the road users leading to the conflict are known. For example, differences in movement patterns between conflicts and other encounters can be analyzed and suitable measures for increasing road safety can be derived from them. In order to avoid accidents, however, a certain process validity between conflicts and accidents is also required here.</p>"},{"location":"overview/usecases/trafficsafety/#how-widespread-is-the-method","title":"How widespread is the method?","text":"<p>Several other European countries developed their own dedicated traffic conflict techniques many years ago and conflict analysis already plays an important role in their road safety assessment tools. In some other countries there appear to be concerns about the conflict technique from earlier days, as the severity of the conflicts was assessed by local human observers and was therefore largely subjective. Against the background of Vision Zero (zero traffic fatalities) and the developments in computer technology and AI, we advocate the use of proactive safety assessments based on the trajectories of road users. This can save lives and prevent many accidents. Nonetheless, there is still a need for further research into data collection, indicators, accident prediction and validation. In addition, clarification is required about the strengths and weaknesses of the conflict technology compared to the accident analysis and the need for further proactive safety tools.</p>"},{"location":"overview/usecases/trafficsafety/#what-s-next-for-us","title":"What\u00b4s next for us?","text":"<p>Forthe next years it is planned to integrate the conflict analysis functionalities in OpenTrafficCam, to automate the processes as best as possible and still make them available to all non-programmers in order to enable both basic research and application studies for traffic experts.</p>"},{"location":"overview/usecases/vehiclespeeds/","title":"Vehicle Speeds","text":"<p>Coming soon</p> <p>Unfortunately, there is no content here yet. But we are currently working on completing this website.</p>"}]}