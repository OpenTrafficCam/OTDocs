{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to OpenTrafficCam \u00b6 Open Traffic Cam makes traffic detection easier, faster and more efficient. Check out our github page for more information. This website will cover all the documentation in one place.","title":"Welcome"},{"location":"#welcome-to-opentrafficcam","text":"Open Traffic Cam makes traffic detection easier, faster and more efficient. Check out our github page for more information. This website will cover all the documentation in one place.","title":"Welcome to OpenTrafficCam"},{"location":"OTAnalytics/","text":"In a Nutshell \u00b6 Giving the data a meaning. OTAnalytics is a set of self developed algorithms to gather traffic measures based on trajectories of road users. Key features: Generation of traffic flow matrices and node flow diagrams Measurement of time gaps, velocities and accelerations Long-term: Analysis of near-accidents (e.g. TTC, PET) Visualization of traffic data Import of trajectories from other systems (e.g. DataFromSky) Runs on any Windows laptop Most of the code will be developed by the OpenTrafficCam contributors (based on python libraries like NumPy , pandas , tkinter and OpenCV ).","title":"In a Nutshell"},{"location":"OTAnalytics/#in-a-nutshell","text":"Giving the data a meaning. OTAnalytics is a set of self developed algorithms to gather traffic measures based on trajectories of road users. Key features: Generation of traffic flow matrices and node flow diagrams Measurement of time gaps, velocities and accelerations Long-term: Analysis of near-accidents (e.g. TTC, PET) Visualization of traffic data Import of trajectories from other systems (e.g. DataFromSky) Runs on any Windows laptop Most of the code will be developed by the OpenTrafficCam contributors (based on python libraries like NumPy , pandas , tkinter and OpenCV ).","title":"In a Nutshell"},{"location":"OTCamera/","text":"In a Nutshell \u00b6 The heart of Open Traffic . OTCamera is a prototype of a mobile camera system for capturing traffic videos. Key features: Based on Raspberry Pi Zero W Waterproof case Operation with buttons and/or smartphone Continuous recording over one week Privacy compliant recording Under 400 \u20ac per camera system We provide: List of hardware components (and where to purchase them) Software needed for video capturing and camera calibration Instructions for assembling the hardware and installing the software Recommendations for mounting, alignment and settings of the camera","title":"In a Nutshell"},{"location":"OTCamera/#in-a-nutshell","text":"The heart of Open Traffic . OTCamera is a prototype of a mobile camera system for capturing traffic videos. Key features: Based on Raspberry Pi Zero W Waterproof case Operation with buttons and/or smartphone Continuous recording over one week Privacy compliant recording Under 400 \u20ac per camera system We provide: List of hardware components (and where to purchase them) Software needed for video capturing and camera calibration Instructions for assembling the hardware and installing the software Recommendations for mounting, alignment and settings of the camera","title":"In a Nutshell"},{"location":"OTCamera/install/gettingstarted/","text":"Getting Started \u00b6 No matter which version you want to install, you will always use a Raspberry Pi as a base. This guide describes the basic steps you need to perform for each version. You will need: Raspberry Pi 2B/3B(+)/4/Zero W and power supply Micro SD card (a High Endurance version is recommended) SD Card Reader Raspberry Pi Imager . Warning Even though we have completed the guide ourselves, every PC is different and Raspberry OS can change as well. There are steps described with which you can break your Windows on the PC. You should know what you are doing. We are not responsible for any damage that may occur. Prepare the SD Card \u00b6 Download and install the Raspberry Pi Imager , insert the SD-Card and start the Imager. It's recommended to use the Raspberry Pi Imager Advanced Options to do some basic setup. Just press Ctrl + Shift + X on startscreen to access advanced options. Setup a hostname, enable SSH, either password based or using public-key authentication (recommended!), configure your Wifi credentials and choose the correct locale for Wifi, timezone and keyboard layout. You can also skip the first-run wizard. How to generate a public key Generate SSH-Keys for password-less connection. On your desktop computer open a command line terminal (CMD or Powershell on Windows or a bash on Linux) and run ssh-keygen to generate a public private key combination. Add the private key to your ssh-agent (you may need to update OpenSSH on Windows ). Now insert the SD card into the PC. Select Raspberry Pi OS Lite (32-bit) as operating system under \"Raspberry Pi (Other)\". Then select the SD card on which the operating system will be installed. Warning It is possible to select not only the SD card, but also the hard drive or SSD where your operating system or other PC data is stored. Make sure that you really select the right SD card. Most of the time, you can tell by the size. Press the Write button and wait until the Raspberry Pi Imager has completely written and verified your SD card. Tip Sometimes the verification may fail. Try another USB port on your PC or another card reader. If this does not help, maybe the SD card is broken. Setup without Raspberry Pi Imager Add an empty file named ssh to the boot partition to enable ssh on first boot . Add your Wifi credentials as described in the Raspberry Pi Documentation Connect to the pi using ssh ( ssh pi@raspberry )to the Pi using password authentication. Create and edit the needed authorized_keys file. mkdir -p ~/.ssh nano ~/.ssh/authorized_keys Copy your public key on the host and paste it on the pi, save&close using Ctrl+X - Enter - Y. Setup the Raspberry \u00b6 Now, take the SD-Card and insert it in the Raspberry. You can now boot the Pi on LAN or within your Wifi by connecting it to the power supply. Try to connect to the Pi using a command line or PowerShell: ssh pi @otcamera01 If everything is setup correctly, you will be asked to add the host key to the list of known hosts (answer: yes) and you should be connected to your Raspberry Pi. warning: agent returned different signature type ssh-rsa (expected rsa-sha2-512) If you are on Windows you may need to update OpenSSH if you ar getting this error: warning : agent returned different signature type ssh-rsa ( expected rsa-sha2 - 512 ) Warning This guide should run on Windows 10 but you are modifing your system files. Please do not do anything you do not understand! Read the source for more information. We are not liable for any damage you may cause to your system. But we followed the steps ourselves and everything went well. Download the latest OpenSSH for Windows release here . Depending on your Windows version it is probably OpenSSH-Win64.zip . Open Windows Explorer and navigate to your Download folder. You should see the OpenSSH-Win64.zip . Open an elevated PowerShell via File \u2192 Open Windows PowerShell \u2192 Open Windows PowerShell as administrator. If you copy and paste the following code, you will unzip OpenSSH and overwrite the system's default OpenSSH version. # Overwrite windows installed bins $openSshBins = ( Get-ChildItem 'C:\\WINDOWS\\System32\\OpenSSH\\' ). Name Expand-Archive -Path .\\ OpenSSH-Win64 . zip -DestinationPath . takeown . exe / a / r / f C : \\ Windows \\ System32 \\ OpenSSH \\ icacls . exe 'C:\\Windows\\System32\\OpenSSH' / grant 'BUILTIN\\Administrators:(OI)(CI)F' icacls . exe 'C:\\Windows\\System32\\OpenSSH' / grant 'BUILTIN\\Administrators:F' / t Stop-Service ssh-agent $openSshBins | %{ Copy-Item -Path .\\ OpenSSH-Win64 \\ $_ -Destination C : \\ Windows \\ System32 \\ OpenSSH \\ } Start-Service ssh-agent If you have successfully logged in now, we can configure the Raspberry Pi for the OpenTrafficCam. Update the pi by running apt and reboot. sudo apt update && sudo apt upgrade -y sudo reboot Reconnect to your pi (open PowerShell and run ssh pi@otcamera01 ) and run the raspberry configuration tool. sudo raspi-config Change the following settings to appropriate values: System Options \u2192 Password Interface Options \u2192 Camera \u2192 enable Setup without Raspberry Pi Imager If you did not use the Raspberry Pi Imager, you will need to setup a few more things. System Options Hostname Interface Options Camera (enable) Localization Options Timezone (Europe/Berlin) WLAN Country (DE) Reboot the Pi afterwards ( sudo reboot ).","title":"Getting Started"},{"location":"OTCamera/install/gettingstarted/#getting-started","text":"No matter which version you want to install, you will always use a Raspberry Pi as a base. This guide describes the basic steps you need to perform for each version. You will need: Raspberry Pi 2B/3B(+)/4/Zero W and power supply Micro SD card (a High Endurance version is recommended) SD Card Reader Raspberry Pi Imager . Warning Even though we have completed the guide ourselves, every PC is different and Raspberry OS can change as well. There are steps described with which you can break your Windows on the PC. You should know what you are doing. We are not responsible for any damage that may occur.","title":"Getting Started"},{"location":"OTCamera/install/gettingstarted/#prepare-the-sd-card","text":"Download and install the Raspberry Pi Imager , insert the SD-Card and start the Imager. It's recommended to use the Raspberry Pi Imager Advanced Options to do some basic setup. Just press Ctrl + Shift + X on startscreen to access advanced options. Setup a hostname, enable SSH, either password based or using public-key authentication (recommended!), configure your Wifi credentials and choose the correct locale for Wifi, timezone and keyboard layout. You can also skip the first-run wizard. How to generate a public key Generate SSH-Keys for password-less connection. On your desktop computer open a command line terminal (CMD or Powershell on Windows or a bash on Linux) and run ssh-keygen to generate a public private key combination. Add the private key to your ssh-agent (you may need to update OpenSSH on Windows ). Now insert the SD card into the PC. Select Raspberry Pi OS Lite (32-bit) as operating system under \"Raspberry Pi (Other)\". Then select the SD card on which the operating system will be installed. Warning It is possible to select not only the SD card, but also the hard drive or SSD where your operating system or other PC data is stored. Make sure that you really select the right SD card. Most of the time, you can tell by the size. Press the Write button and wait until the Raspberry Pi Imager has completely written and verified your SD card. Tip Sometimes the verification may fail. Try another USB port on your PC or another card reader. If this does not help, maybe the SD card is broken. Setup without Raspberry Pi Imager Add an empty file named ssh to the boot partition to enable ssh on first boot . Add your Wifi credentials as described in the Raspberry Pi Documentation Connect to the pi using ssh ( ssh pi@raspberry )to the Pi using password authentication. Create and edit the needed authorized_keys file. mkdir -p ~/.ssh nano ~/.ssh/authorized_keys Copy your public key on the host and paste it on the pi, save&close using Ctrl+X - Enter - Y.","title":"Prepare the SD Card"},{"location":"OTCamera/install/gettingstarted/#setup-the-raspberry","text":"Now, take the SD-Card and insert it in the Raspberry. You can now boot the Pi on LAN or within your Wifi by connecting it to the power supply. Try to connect to the Pi using a command line or PowerShell: ssh pi @otcamera01 If everything is setup correctly, you will be asked to add the host key to the list of known hosts (answer: yes) and you should be connected to your Raspberry Pi. warning: agent returned different signature type ssh-rsa (expected rsa-sha2-512) If you are on Windows you may need to update OpenSSH if you ar getting this error: warning : agent returned different signature type ssh-rsa ( expected rsa-sha2 - 512 ) Warning This guide should run on Windows 10 but you are modifing your system files. Please do not do anything you do not understand! Read the source for more information. We are not liable for any damage you may cause to your system. But we followed the steps ourselves and everything went well. Download the latest OpenSSH for Windows release here . Depending on your Windows version it is probably OpenSSH-Win64.zip . Open Windows Explorer and navigate to your Download folder. You should see the OpenSSH-Win64.zip . Open an elevated PowerShell via File \u2192 Open Windows PowerShell \u2192 Open Windows PowerShell as administrator. If you copy and paste the following code, you will unzip OpenSSH and overwrite the system's default OpenSSH version. # Overwrite windows installed bins $openSshBins = ( Get-ChildItem 'C:\\WINDOWS\\System32\\OpenSSH\\' ). Name Expand-Archive -Path .\\ OpenSSH-Win64 . zip -DestinationPath . takeown . exe / a / r / f C : \\ Windows \\ System32 \\ OpenSSH \\ icacls . exe 'C:\\Windows\\System32\\OpenSSH' / grant 'BUILTIN\\Administrators:(OI)(CI)F' icacls . exe 'C:\\Windows\\System32\\OpenSSH' / grant 'BUILTIN\\Administrators:F' / t Stop-Service ssh-agent $openSshBins | %{ Copy-Item -Path .\\ OpenSSH-Win64 \\ $_ -Destination C : \\ Windows \\ System32 \\ OpenSSH \\ } Start-Service ssh-agent If you have successfully logged in now, we can configure the Raspberry Pi for the OpenTrafficCam. Update the pi by running apt and reboot. sudo apt update && sudo apt upgrade -y sudo reboot Reconnect to your pi (open PowerShell and run ssh pi@otcamera01 ) and run the raspberry configuration tool. sudo raspi-config Change the following settings to appropriate values: System Options \u2192 Password Interface Options \u2192 Camera \u2192 enable Setup without Raspberry Pi Imager If you did not use the Raspberry Pi Imager, you will need to setup a few more things. System Options Hostname Interface Options Camera (enable) Localization Options Timezone (Europe/Berlin) WLAN Country (DE) Reboot the Pi afterwards ( sudo reboot ).","title":"Setup the Raspberry"},{"location":"OTLabels/","text":"In a Nutshell \u00b6 Better detection performance. OTLabels is a workflow for the retraining of AI models to detect road users in videos based on existing open source software packages ( CVAT , YOLO v5 ). Key features: Annotation of vehicles and persons using existing open source software ( CVAT ) Retrain existing neural network models with YOLO v5 for individual optimization of the detection ability Long-term: Real time detection Runs on any Windows laptop. However, a decent GPU with CUDA support is strongly recommended. Most of the code describing the workflow will be developed by the OpenTrafficCam contributors (based on python libraries like NumPy , pandas , tkinter and OpenCV ).","title":"In a Nutshell"},{"location":"OTLabels/#in-a-nutshell","text":"Better detection performance. OTLabels is a workflow for the retraining of AI models to detect road users in videos based on existing open source software packages ( CVAT , YOLO v5 ). Key features: Annotation of vehicles and persons using existing open source software ( CVAT ) Retrain existing neural network models with YOLO v5 for individual optimization of the detection ability Long-term: Real time detection Runs on any Windows laptop. However, a decent GPU with CUDA support is strongly recommended. Most of the code describing the workflow will be developed by the OpenTrafficCam contributors (based on python libraries like NumPy , pandas , tkinter and OpenCV ).","title":"In a Nutshell"},{"location":"OTLabels/annotation/","text":"Annotation \u00b6 Why is annotation necessary? \u00b6 What is annotation? \u00b6 Annotation steps \u00b6 Which tools can I use to annotate my images or videos? \u00b6 Which labelling method should I use? \u00b6","title":"Annotation"},{"location":"OTLabels/annotation/#annotation","text":"","title":"Annotation"},{"location":"OTLabels/annotation/#why-is-annotation-necessary","text":"","title":"Why is annotation necessary?"},{"location":"OTLabels/annotation/#what-is-annotation","text":"","title":"What is annotation?"},{"location":"OTLabels/annotation/#annotation-steps","text":"","title":"Annotation steps"},{"location":"OTLabels/annotation/#which-tools-can-i-use-to-annotate-my-images-or-videos","text":"","title":"Which tools can I use to annotate my images or videos?"},{"location":"OTLabels/annotation/#which-labelling-method-should-i-use","text":"","title":"Which labelling method should I use?"},{"location":"OTLabels/annotation/extend-coco/","text":"Extend COCO \u00b6 If you want to extend datasets to retrain models, the format of your new labels should be conform to those of the original dataset. We want to extend the MS COCO dataset of 2D bounding boxes with our own labels in frames from our sample videos. For this purpose, annotators are provided with pre-labeled detections using YOLOv5x. In these, they will check classification and positional accuracy of 2D boxes just for a defined set of object classes. On this page we define our intended organizational and technical annotation workflow using CVAT. Annotation Workflow in CVAT \u00b6 Projects, Tasks and Jobs \u00b6 We define certain set of videos as a project . Every project has multiple tasks (each consists of one video to annotate). Each task has a status (\"Pending\", \"In Progress\" and \"Completed\") and a progress bar showing the number of completed jobs. Open the task with klick on \"Open\". For each task (or video) multiple jobs with a certain amounts of frames from the video are defined. Each job has a status (\"annotation\", \"validation\" and \"completed\"), an \"assignee\" (responsible for annotation) and a \"reviewer\" (responsible for reviewing the assignees annotation). Open the job with klick on \"Job #...\". Macro Workflow \u00b6 For each job, the project coordinator assigns the roles of the \"assignee\" (= annotator) and the \"reviewer\". The reviewer then starts working on a job by initial labeling (see Annotation ) of the video frames. The assignee should save his labeling work from time to time and can also stop and later repeat working on a job. After finishing the initial labeling, the assignee requests a review (menu - request review, the assigned reviewer should already be named there) and thus sets status of the job to \"validation\".Then the reviewer checks the initial labeling done by the assignee and raises issues when there is a problem or question. After reviewing all video frames of the job, the reviewer submits the review (see see Review by either accepting and completing the job or rejecting it (which sets the status of the job bach to \"Annotation\"). Now the assignee has to check the issues, update the labels, resolve the issus and again request a review. The review process starts again and if necessary also next annotations, until the reviewer accepts the job and it is marked as \"Completed\". Micro Workflow \u00b6 Annotation \u00b6 The following procedure is defined for annotation in CVAT: Go through all pre-labeled objects on the right Check and delete false positive labels (if there is no object) Check and delete duplicate labels for the same object Check and correct object class for target classes Check and delete objects from other classes if they are classified wrong Zoom in on one quadrant of the image at a time: Check and correct position of object\u00b4s 2D boxes Check if there are objects from the target classes not already pre-labeled, draw new 2D box and annotate object class Review \u00b6 LATER Project specific instructions \u00b6 Target classes \u00b6 Target classes are all road user classes from the initial COCO data set that are relevant for our sample videos: Person Bicycle Motorcycle Car Bus Truck All vehicles or combinations of vehicles are labeled as \"truck\" if they have twin tires (except for regular service buses or coaches), a superstructure or a loading area or a trailer (also cars with trailer, the 2D box includes vehicle and trailer). Here, as in the COCO dataset, both the \"person\" and the \"bicycle\" are labeled separately for bicyclists. Object dimensions \u00b6 The 2D box boundaries are defined by the outermost parts of the objects visible in that frame of the video. Even if only a part of an object is visible and the class of the object is recognizable for the annotator, it should be labeled. Links \u00b6 Labeling is done under label.opentrafficcam.org . Accounts and jobs for labeling and review are administrated by the OpenTrafficCam core developers . Annotators will also find assistance in the CVAT user guide .","title":"Extend COCO"},{"location":"OTLabels/annotation/extend-coco/#extend-coco","text":"If you want to extend datasets to retrain models, the format of your new labels should be conform to those of the original dataset. We want to extend the MS COCO dataset of 2D bounding boxes with our own labels in frames from our sample videos. For this purpose, annotators are provided with pre-labeled detections using YOLOv5x. In these, they will check classification and positional accuracy of 2D boxes just for a defined set of object classes. On this page we define our intended organizational and technical annotation workflow using CVAT.","title":"Extend COCO"},{"location":"OTLabels/annotation/extend-coco/#annotation-workflow-in-cvat","text":"","title":"Annotation Workflow in CVAT"},{"location":"OTLabels/annotation/extend-coco/#projects-tasks-and-jobs","text":"We define certain set of videos as a project . Every project has multiple tasks (each consists of one video to annotate). Each task has a status (\"Pending\", \"In Progress\" and \"Completed\") and a progress bar showing the number of completed jobs. Open the task with klick on \"Open\". For each task (or video) multiple jobs with a certain amounts of frames from the video are defined. Each job has a status (\"annotation\", \"validation\" and \"completed\"), an \"assignee\" (responsible for annotation) and a \"reviewer\" (responsible for reviewing the assignees annotation). Open the job with klick on \"Job #...\".","title":"Projects, Tasks and Jobs"},{"location":"OTLabels/annotation/extend-coco/#macro-workflow","text":"For each job, the project coordinator assigns the roles of the \"assignee\" (= annotator) and the \"reviewer\". The reviewer then starts working on a job by initial labeling (see Annotation ) of the video frames. The assignee should save his labeling work from time to time and can also stop and later repeat working on a job. After finishing the initial labeling, the assignee requests a review (menu - request review, the assigned reviewer should already be named there) and thus sets status of the job to \"validation\".Then the reviewer checks the initial labeling done by the assignee and raises issues when there is a problem or question. After reviewing all video frames of the job, the reviewer submits the review (see see Review by either accepting and completing the job or rejecting it (which sets the status of the job bach to \"Annotation\"). Now the assignee has to check the issues, update the labels, resolve the issus and again request a review. The review process starts again and if necessary also next annotations, until the reviewer accepts the job and it is marked as \"Completed\".","title":"Macro Workflow"},{"location":"OTLabels/annotation/extend-coco/#micro-workflow","text":"","title":"Micro Workflow"},{"location":"OTLabels/annotation/extend-coco/#project-specific-instructions","text":"","title":"Project specific instructions"},{"location":"OTLabels/annotation/extend-coco/#target-classes","text":"Target classes are all road user classes from the initial COCO data set that are relevant for our sample videos: Person Bicycle Motorcycle Car Bus Truck All vehicles or combinations of vehicles are labeled as \"truck\" if they have twin tires (except for regular service buses or coaches), a superstructure or a loading area or a trailer (also cars with trailer, the 2D box includes vehicle and trailer). Here, as in the COCO dataset, both the \"person\" and the \"bicycle\" are labeled separately for bicyclists.","title":"Target classes"},{"location":"OTLabels/annotation/extend-coco/#object-dimensions","text":"The 2D box boundaries are defined by the outermost parts of the objects visible in that frame of the video. Even if only a part of an object is visible and the class of the object is recognizable for the annotator, it should be labeled.","title":"Object dimensions"},{"location":"OTLabels/annotation/extend-coco/#links","text":"Labeling is done under label.opentrafficcam.org . Accounts and jobs for labeling and review are administrated by the OpenTrafficCam core developers . Annotators will also find assistance in the CVAT user guide .","title":"Links"},{"location":"OTLabels/training/","text":"Train the model \u00b6 How reliable are object recognition models? \u00b6 I expect special vehicles in my counting. Are the AI models capable of detecting them as well? \u00b6 How can I improve the object detection of a existing model? \u00b6 Retraining Steps \u00b6 Training \u00b6 Test \u00b6 Validation \u00b6","title":"Train the model"},{"location":"OTLabels/training/#train-the-model","text":"","title":"Train the model"},{"location":"OTLabels/training/#how-reliable-are-object-recognition-models","text":"","title":"How reliable are object recognition models?"},{"location":"OTLabels/training/#i-expect-special-vehicles-in-my-counting-are-the-ai-models-capable-of-detecting-them-as-well","text":"","title":"I expect special vehicles in my counting. Are the AI models capable of detecting them as well?"},{"location":"OTLabels/training/#how-can-i-improve-the-object-detection-of-a-existing-model","text":"","title":"How can I improve the object detection of a existing model?"},{"location":"OTLabels/training/#retraining-steps","text":"","title":"Retraining Steps"},{"location":"OTLabels/training/#training","text":"","title":"Training"},{"location":"OTLabels/training/#test","text":"","title":"Test"},{"location":"OTLabels/training/#validation","text":"","title":"Validation"},{"location":"OTVision/","text":"In a Nutshell \u00b6 Turn videos into data. OTVision , is a collection of algorithms to generate trajectories of road users in traffic videos. Most of the algorithms have been developed in previous open source projects. We modify and link them to a working pipeline. Key features: Automated video analysis Detection using neural networks Can be used without programming knowledge Result: Trajectories in image and world coordinates Necessary: Windows computer with powerful graphics card (approx. 2.500 \u20ac) Pipeline of algorithms: Conversion of h264 video files to other formats ( ffmpeg ) Detection (joint localization and classification) of road users in single frames ( YOLOv5 by Jocher/Ultralytics ) Tracking of detected road users between single frames ( IOU-Tracker by Bochinski et al. ) Correction of trajectories for lens distortion ( OpenCV-Library ) Transformation of trajectories to world coordinates ( OpenCV-Library )","title":"In a Nutshell"},{"location":"OTVision/#in-a-nutshell","text":"Turn videos into data. OTVision , is a collection of algorithms to generate trajectories of road users in traffic videos. Most of the algorithms have been developed in previous open source projects. We modify and link them to a working pipeline. Key features: Automated video analysis Detection using neural networks Can be used without programming knowledge Result: Trajectories in image and world coordinates Necessary: Windows computer with powerful graphics card (approx. 2.500 \u20ac) Pipeline of algorithms: Conversion of h264 video files to other formats ( ffmpeg ) Detection (joint localization and classification) of road users in single frames ( YOLOv5 by Jocher/Ultralytics ) Tracking of detected road users between single frames ( IOU-Tracker by Bochinski et al. ) Correction of trajectories for lens distortion ( OpenCV-Library ) Transformation of trajectories to world coordinates ( OpenCV-Library )","title":"In a Nutshell"},{"location":"contribute/coding/","text":"Coding (Python) \u00b6 Coding standards are not about right and wrong or good and bad. They are about uniformity, so people can easily read and contribute to the code regardless who worked on the software before. For now, nearly all code is Python so we just agreed on coding conventions for the Python programming language. We basically follow the PEP 8 Style Guide for Python Code . A good summary and some additions can be found in the Hitchhikers Guide or at RealPython.com . Below we defined what we consider the most important standards. Naming Conventions \u00b6 All names should be as short as possible but as long as necessary to understand them. General \u00b6 The following table from RealPython.com summarizes the PEP8 naming conventions: Type Naming Convention Examples Function Use a lowercase word or words. Separate words by underscores to improve readability. function, my_function Variable Use a lowercase single letter, word, or words. Separate words with underscores to improve readability. x, var, my_variable Class Start each word with a capital letter. Do not separate words with underscores. This style is called camel case. Model, MyClass Method Use a lowercase word or words. Separate words with underscores to improve readability. class_method, method Constant Use an uppercase single letter, word, or words. Separate words with underscores to improve readability. CONSTANT, MY_CONSTANT, MY_LONG_CONSTANT Module Use a short, lowercase word or words. Separate words with underscores to improve readability. module.py, my_module.py Package Use a short, lowercase word or words. Do not separate words with underscores. package, mypackage Files, Folder, Dirs \u00b6 Naming Convention Examples dir D:\\tmp\\ filename testfile suffix _detections filetype .csv file D:\\tmp\\testfile_detections.csv path \"D:\\tmp\\\" or \"D:\\tmp\\testfile_detections.csv\" Suffix \u00b6 Files get a suffix according to their content. Suffix Description _detections detections as bounding boxes _tracks-px trajectories in pixel coordinates _tracks-corr trajectories in pixel coordinates corrected for lens distortion _tracks-utm trajectories transformed to UTM coordinates _refpts reference points to convert pixel to UTM coordinates Code documentation \u00b6 Each module, function, class or method should be described in a docstring ( Google style ) Docstrings for Modules \u00b6 Each file should start with the license snippet followed by a docstring describing the contents and usage of the module: \"\"\"A one line summary of the module or program, terminated by a period. Leave one blank line. The rest of this docstring should contain an overall description of the module or program. Optionally, it may also contain a brief description of exported classes and functions and/or usage examples. Typical usage example: foo = ClassFoo() bar = foo.FunctionBar() \"\"\" Docstrings for Functions \u00b6 Each function must be described by a docstring: def hello_world ( message : str = \"Hello World\" , author : str = \"Santa Claus\" ): \"\"\"Deliver a message from an author to the world. \"author says message\" Args: message (str, optional): Message to deliver. Defaults to \"Hello World\". author (str, optional): Author name. Defaults to \"Santa Claus\". Returns: str: the message said \"\"\" msg = author + \" says \" + message print ( msg ) return msg Note If you are using VS Code, you may want to use the Python Docstring Generator extension. Comments \u00b6 If it is necessary to explain your code in between, use single line or block comments for some (or all) code that follows: # This is an example for a single line comment # This is an # example for a # block comment Try to avoid inline comments. Dependencies \u00b6 We try to develop OpenTrafficCam using as few dependencies as possible. For basic functionality we prefer packages that come with the Python standard distribution (like pathlib). However, OpenTrafficCam would not be possible without code from third party libraries, especially when it comes to videos, images, data analysis, neural networks or GUI. These are the libraries we intend to use for specific functionalities throughout the whole framework (we are grateful to the authors): Functionality Library Paths pathlib (distributed with Python since 3.4) Videos, Images OpenCV Neural networks PyTorch , torchvision Data handling pandas , NumPy Shape handling GeoPandas , Shapely Plotting, graphing Plotly User interface PySimpleGui Web PySimpleGUIWeb (camera system), Dash (dashboards) Linting/Autoformatting \u00b6 To match PEP8 specs and write pretty code we use flake8 to check against structural and style errors and black for autoformatting. Additionally the package flake8-docstrings can be used to lint the docstrings. The following settings are set in the .flake8 file: [flake8] max-line-length = 88 docstring-convention = google","title":"Coding (Python)"},{"location":"contribute/coding/#coding-python","text":"Coding standards are not about right and wrong or good and bad. They are about uniformity, so people can easily read and contribute to the code regardless who worked on the software before. For now, nearly all code is Python so we just agreed on coding conventions for the Python programming language. We basically follow the PEP 8 Style Guide for Python Code . A good summary and some additions can be found in the Hitchhikers Guide or at RealPython.com . Below we defined what we consider the most important standards.","title":"Coding (Python)"},{"location":"contribute/coding/#naming-conventions","text":"All names should be as short as possible but as long as necessary to understand them.","title":"Naming Conventions"},{"location":"contribute/coding/#general","text":"The following table from RealPython.com summarizes the PEP8 naming conventions: Type Naming Convention Examples Function Use a lowercase word or words. Separate words by underscores to improve readability. function, my_function Variable Use a lowercase single letter, word, or words. Separate words with underscores to improve readability. x, var, my_variable Class Start each word with a capital letter. Do not separate words with underscores. This style is called camel case. Model, MyClass Method Use a lowercase word or words. Separate words with underscores to improve readability. class_method, method Constant Use an uppercase single letter, word, or words. Separate words with underscores to improve readability. CONSTANT, MY_CONSTANT, MY_LONG_CONSTANT Module Use a short, lowercase word or words. Separate words with underscores to improve readability. module.py, my_module.py Package Use a short, lowercase word or words. Do not separate words with underscores. package, mypackage","title":"General"},{"location":"contribute/coding/#files-folder-dirs","text":"Naming Convention Examples dir D:\\tmp\\ filename testfile suffix _detections filetype .csv file D:\\tmp\\testfile_detections.csv path \"D:\\tmp\\\" or \"D:\\tmp\\testfile_detections.csv\"","title":"Files, Folder, Dirs"},{"location":"contribute/coding/#suffix","text":"Files get a suffix according to their content. Suffix Description _detections detections as bounding boxes _tracks-px trajectories in pixel coordinates _tracks-corr trajectories in pixel coordinates corrected for lens distortion _tracks-utm trajectories transformed to UTM coordinates _refpts reference points to convert pixel to UTM coordinates","title":"Suffix"},{"location":"contribute/coding/#code-documentation","text":"Each module, function, class or method should be described in a docstring ( Google style )","title":"Code documentation"},{"location":"contribute/coding/#docstrings-for-modules","text":"Each file should start with the license snippet followed by a docstring describing the contents and usage of the module: \"\"\"A one line summary of the module or program, terminated by a period. Leave one blank line. The rest of this docstring should contain an overall description of the module or program. Optionally, it may also contain a brief description of exported classes and functions and/or usage examples. Typical usage example: foo = ClassFoo() bar = foo.FunctionBar() \"\"\"","title":"Docstrings for Modules"},{"location":"contribute/coding/#docstrings-for-functions","text":"Each function must be described by a docstring: def hello_world ( message : str = \"Hello World\" , author : str = \"Santa Claus\" ): \"\"\"Deliver a message from an author to the world. \"author says message\" Args: message (str, optional): Message to deliver. Defaults to \"Hello World\". author (str, optional): Author name. Defaults to \"Santa Claus\". Returns: str: the message said \"\"\" msg = author + \" says \" + message print ( msg ) return msg Note If you are using VS Code, you may want to use the Python Docstring Generator extension.","title":"Docstrings for Functions"},{"location":"contribute/coding/#comments","text":"If it is necessary to explain your code in between, use single line or block comments for some (or all) code that follows: # This is an example for a single line comment # This is an # example for a # block comment Try to avoid inline comments.","title":"Comments"},{"location":"contribute/coding/#dependencies","text":"We try to develop OpenTrafficCam using as few dependencies as possible. For basic functionality we prefer packages that come with the Python standard distribution (like pathlib). However, OpenTrafficCam would not be possible without code from third party libraries, especially when it comes to videos, images, data analysis, neural networks or GUI. These are the libraries we intend to use for specific functionalities throughout the whole framework (we are grateful to the authors): Functionality Library Paths pathlib (distributed with Python since 3.4) Videos, Images OpenCV Neural networks PyTorch , torchvision Data handling pandas , NumPy Shape handling GeoPandas , Shapely Plotting, graphing Plotly User interface PySimpleGui Web PySimpleGUIWeb (camera system), Dash (dashboards)","title":"Dependencies"},{"location":"contribute/coding/#lintingautoformatting","text":"To match PEP8 specs and write pretty code we use flake8 to check against structural and style errors and black for autoformatting. Additionally the package flake8-docstrings can be used to lint the docstrings. The following settings are set in the .flake8 file: [flake8] max-line-length = 88 docstring-convention = google","title":"Linting/Autoformatting"},{"location":"contribute/documentation/","text":"Documentation \u00b6 You can help correcting, improving and extending this documentation by clicking on the \"edit\" button on top right of each page that brings you directly to the corresponding file in the documentation repository on Github. You can edit the page using the rather simple markdown language . Then you commit your changes by choosing a meaningful commit meassage and the \"Create a new branch for this commit and start a pull request\" option. After internal review and discussion (if necessery) we will merge your pull request into the documentation repository and then it will be also visible on the website. If you want to create new pages, have a look at the README file of the documentation repository . There you will also find a description of how this website is rendered and how inserting a new page into the navigation works. The documentation website of OpenTrafficCam is generated by MkDocs using the awesome material theme. The documentation is fully written in Markdown .","title":"Documentation"},{"location":"contribute/documentation/#documentation","text":"You can help correcting, improving and extending this documentation by clicking on the \"edit\" button on top right of each page that brings you directly to the corresponding file in the documentation repository on Github. You can edit the page using the rather simple markdown language . Then you commit your changes by choosing a meaningful commit meassage and the \"Create a new branch for this commit and start a pull request\" option. After internal review and discussion (if necessery) we will merge your pull request into the documentation repository and then it will be also visible on the website. If you want to create new pages, have a look at the README file of the documentation repository . There you will also find a description of how this website is rendered and how inserting a new page into the navigation works. The documentation website of OpenTrafficCam is generated by MkDocs using the awesome material theme. The documentation is fully written in Markdown .","title":"Documentation"},{"location":"contribute/github/","text":"GitHub \u00b6 All code of OpenTrafficCam is hosted on GitHub . For each core module a separate repository exists: OTCamera OTVision OTAnalytics OTLabels Following the Feature Branch Workflow each repository has a master branch, which at all times should contain a stable version of the software. All coding should be committed to separate feature branches. Once you completed coding on a feature, issue or bugfix, you create a pull request for the respective feature branch (preferably labeled using the GitHub labeling system ). Your contributed code will then be reviewed by the main developers, discussed (if necessary) and merged into the master branch. Even without coding you can contribute to the project: If you encounter any bug or got any idea for enhancement, feel free to open an issue in the corresponding repository. Every issue should at least have two labels assigned: One label indicating the maturaty of the issue ( question , bug , feature or idea ) and one assigning the repository-specific subpackage or section ( black colored labels ). All pull request will be checked by GitHub's super-linter .","title":"GitHub"},{"location":"contribute/github/#github","text":"All code of OpenTrafficCam is hosted on GitHub . For each core module a separate repository exists: OTCamera OTVision OTAnalytics OTLabels Following the Feature Branch Workflow each repository has a master branch, which at all times should contain a stable version of the software. All coding should be committed to separate feature branches. Once you completed coding on a feature, issue or bugfix, you create a pull request for the respective feature branch (preferably labeled using the GitHub labeling system ). Your contributed code will then be reviewed by the main developers, discussed (if necessary) and merged into the master branch. Even without coding you can contribute to the project: If you encounter any bug or got any idea for enhancement, feel free to open an issue in the corresponding repository. Every issue should at least have two labels assigned: One label indicating the maturaty of the issue ( question , bug , feature or idea ) and one assigning the repository-specific subpackage or section ( black colored labels ). All pull request will be checked by GitHub's super-linter .","title":"GitHub"},{"location":"contribute/gui/","text":"GUI \u00b6 For now we decided on using PysimpleGUI to build the OpenTrafficCam user interface, as it is lightweight and easy to start with. Whenever possible, we try to separate the gui code from all other code. All the gui modules have a place in the \"gui\" subpackage of the respective OT repository: OTCamera/OTCamera/gui OTVision/OTVision/gui OTAnalytics/OTAnalytics/gui For the future we can also imagine using a web framework for user interaction. This would also help to enable remote access to local GPU machines or high performance computing centers.","title":"GUI"},{"location":"contribute/gui/#gui","text":"For now we decided on using PysimpleGUI to build the OpenTrafficCam user interface, as it is lightweight and easy to start with. Whenever possible, we try to separate the gui code from all other code. All the gui modules have a place in the \"gui\" subpackage of the respective OT repository: OTCamera/OTCamera/gui OTVision/OTVision/gui OTAnalytics/OTAnalytics/gui For the future we can also imagine using a web framework for user interaction. This would also help to enable remote access to local GPU machines or high performance computing centers.","title":"GUI"},{"location":"contribute/otcamera/","text":"OTCamera \u00b6 The Raspberry Pi Zero is wonderful to use outside (it consumes little power, is cheap, ...). But the development of OTCamera is more fun on a big Pi. Everything is much faster. You will need a Raspberry Pi 2B/3B(+)/4 (the 2 GB works well), and a camera module (no USB webcam). Download and install latest Raspberry Pi OS Lite (32-bit) on a micro SD card using the Raspberry Pi Imager . Add a empty file named ssh to the boot partition to enable ssh on first boot . Boot the Pi on LAN or wifi ( add your wifi first ). Password-less Authentication \u00b6 Generate SSH-Keys for password-less connection. On your desktop computer run ssh-keygen to generate a public private key combination. Add the private key to your ssh-agent (you may need to update OpenSSH on Windows ). Connect to the pi using ssh ( ssh pi@raspberry )to the Pi using password authentication. Create and edit the needed authorized_keys file. mkdir -p ~/.ssh nano ~/.ssh/authorized_keys Copy your public key on the host and paste it on the pi, save&close using Ctrl+X - Enter - Y. You should now be able to connect to the Pi without password. Setup the Raspberry \u00b6 Update the pi by running apt and reboot. sudo apt update && sudo apt upgrade -y sudo reboot Reconnect to your pi and run the raspberry configuration tool. sudo raspi-config Change the following settings to appropriate values: System Options Password Hostname Interface Options Camera (enable) Localization Options Locale (for example de_DE.UTF-8 UTF-8 as default) Timezone (Europe/Berlin) WLAN Country (DE) Reboot the Pi afterwards. Setup VS Code Remote Development Extension \u00b6 Install the Remote-SSH extension on your desktop using the marketplace. Add the Pi as remote host . Connect to the Pi using the Remote-SSH extension (rightclick on the ne host - \"Connect to Host in New Window\"). When asked for the operating system of the host, choose \"Linux\". VS code will download and install the necessary dependencies on the Pi. Open the extension manager in the new windows an install all nessacery extensions. Setup Git and GitHub \u00b6 Install git using apt. sudo apt install git To setup your git commit name and email, login to your github account and copy your private commit email . On the pi run git config --global user.name \"Your Name\" git config --global user.email \"123456+username@users.noreply.github.com\" The easiest way to setup your GitHub credentials is to use vs code. In the file explorer panel click \"Clone Repository\" and choose \"Clone from GitHub\". Depending on your desktop computer settings, a browser tab will open to login into your GitHub account. Afterwards, you can search for \"OpenTrafficCam/OTCamera\" inside the vs code command prompt and select a folder to pull (choose /home/pi by default). Setup Python and Dependencies \u00b6 By default, Raspberry OS light doesn't come with PIP installed. We will need it, to install required packages. sudo apt-get install python3-pip Raspberry OS ships with python 2 and python 3. By default python 2 is used. You may want to change that to python 3 by adding two single lines to .bashrc . nano ~/.bashrc # add the following two lines at the end of the file (without #) # alias python='/usr/bin/python3' # alias pip=pip3 # save & exit (Ctrl+X - Y) source ~/.bashrc python --version pip --version Both commands should state, that they are (using) python 3.(x). Install Screen to Run OTCamera in Background \u00b6 Screen is a full-screen window manager that multiplexes a physical terminal between several processes, typically interactive shells. To install screen on the developer pi just run sudo apt-get install screen You can now start screen and use it as an usual terminal. You can run the python OTCamera to start the camera in an active screen session. Hit Ctrl + A and then D to detach the current session. Screen will still run in background including the OTCamera. If you start screen including the -r option you can reconnect to this terminal session: screen -r You can safely disconnect you ssh connection to the pi, while the screen session is still running. Ready to Develop \u00b6 You should now be ready to pull the OTCamera repository and start developing.","title":"OTCamera"},{"location":"contribute/otcamera/#otcamera","text":"The Raspberry Pi Zero is wonderful to use outside (it consumes little power, is cheap, ...). But the development of OTCamera is more fun on a big Pi. Everything is much faster. You will need a Raspberry Pi 2B/3B(+)/4 (the 2 GB works well), and a camera module (no USB webcam). Download and install latest Raspberry Pi OS Lite (32-bit) on a micro SD card using the Raspberry Pi Imager . Add a empty file named ssh to the boot partition to enable ssh on first boot . Boot the Pi on LAN or wifi ( add your wifi first ).","title":"OTCamera"},{"location":"contribute/otcamera/#password-less-authentication","text":"Generate SSH-Keys for password-less connection. On your desktop computer run ssh-keygen to generate a public private key combination. Add the private key to your ssh-agent (you may need to update OpenSSH on Windows ). Connect to the pi using ssh ( ssh pi@raspberry )to the Pi using password authentication. Create and edit the needed authorized_keys file. mkdir -p ~/.ssh nano ~/.ssh/authorized_keys Copy your public key on the host and paste it on the pi, save&close using Ctrl+X - Enter - Y. You should now be able to connect to the Pi without password.","title":"Password-less Authentication"},{"location":"contribute/otcamera/#setup-the-raspberry","text":"Update the pi by running apt and reboot. sudo apt update && sudo apt upgrade -y sudo reboot Reconnect to your pi and run the raspberry configuration tool. sudo raspi-config Change the following settings to appropriate values: System Options Password Hostname Interface Options Camera (enable) Localization Options Locale (for example de_DE.UTF-8 UTF-8 as default) Timezone (Europe/Berlin) WLAN Country (DE) Reboot the Pi afterwards.","title":"Setup the Raspberry"},{"location":"contribute/otcamera/#setup-vs-code-remote-development-extension","text":"Install the Remote-SSH extension on your desktop using the marketplace. Add the Pi as remote host . Connect to the Pi using the Remote-SSH extension (rightclick on the ne host - \"Connect to Host in New Window\"). When asked for the operating system of the host, choose \"Linux\". VS code will download and install the necessary dependencies on the Pi. Open the extension manager in the new windows an install all nessacery extensions.","title":"Setup VS Code Remote Development Extension"},{"location":"contribute/otcamera/#setup-git-and-github","text":"Install git using apt. sudo apt install git To setup your git commit name and email, login to your github account and copy your private commit email . On the pi run git config --global user.name \"Your Name\" git config --global user.email \"123456+username@users.noreply.github.com\" The easiest way to setup your GitHub credentials is to use vs code. In the file explorer panel click \"Clone Repository\" and choose \"Clone from GitHub\". Depending on your desktop computer settings, a browser tab will open to login into your GitHub account. Afterwards, you can search for \"OpenTrafficCam/OTCamera\" inside the vs code command prompt and select a folder to pull (choose /home/pi by default).","title":"Setup Git and GitHub"},{"location":"contribute/otcamera/#setup-python-and-dependencies","text":"By default, Raspberry OS light doesn't come with PIP installed. We will need it, to install required packages. sudo apt-get install python3-pip Raspberry OS ships with python 2 and python 3. By default python 2 is used. You may want to change that to python 3 by adding two single lines to .bashrc . nano ~/.bashrc # add the following two lines at the end of the file (without #) # alias python='/usr/bin/python3' # alias pip=pip3 # save & exit (Ctrl+X - Y) source ~/.bashrc python --version pip --version Both commands should state, that they are (using) python 3.(x).","title":"Setup Python and Dependencies"},{"location":"contribute/otcamera/#install-screen-to-run-otcamera-in-background","text":"Screen is a full-screen window manager that multiplexes a physical terminal between several processes, typically interactive shells. To install screen on the developer pi just run sudo apt-get install screen You can now start screen and use it as an usual terminal. You can run the python OTCamera to start the camera in an active screen session. Hit Ctrl + A and then D to detach the current session. Screen will still run in background including the OTCamera. If you start screen including the -r option you can reconnect to this terminal session: screen -r You can safely disconnect you ssh connection to the pi, while the screen session is still running.","title":"Install Screen to Run OTCamera in Background"},{"location":"contribute/otcamera/#ready-to-develop","text":"You should now be ready to pull the OTCamera repository and start developing.","title":"Ready to Develop"},{"location":"contribute/vscode/","text":"We are developing OpenTrafficCam using Visual Studio Code using the following extensions: GitHub Pull Requests and Issues markdownlint Pylance Python Python Docstring Generator Pylance Settings \u00b6 To solve import errors in the repository, we need to add an extra path to the pylance config. For example in the workspace settings of OTCamera add: \"python.analysis.extraPaths\" : [ \"./OTCamera\" ] Or set it using the Settings-UI (Workspace -> Pylance -> Python - Analysis: Extra Path -> add './OTCamera') Snippets \u00b6 To comfortably insert code use snippets. You can open the configuration file for a specific language to add or modify the user snippets: Ctrl + Shift + p and Preferences: Configure User Snippets . You can insert the user snippets by start typing the \"prefix\" value. For example start typing gpl_ and autocompletection should recognize the user snippets stated below. GPL License Information \u00b6 Add the following snippet to vscodedata/user/snippets/python.json \"gpl_license_header\" : { \"prefix\" : \"gpl_add_head\" , \"body\" :[ \"$LINE_COMMENT ${1:Program Name and Function}\" , \"$LINE_COMMENT Copyright (C) $CURRENT_YEAR OpenTrafficCam Contributors\" , \"$LINE_COMMENT <https://github.com/OpenTrafficCam\" , \"$LINE_COMMENT <team@opentrafficcam.org>\" , \"$LINE_COMMENT\" , \"$LINE_COMMENT This program is free software: you can redistribute it and/or modify\" , \"$LINE_COMMENT it under the terms of the GNU General Public License as published by\" , \"$LINE_COMMENT the Free Software Foundation, either version 3 of the License, or\" , \"$LINE_COMMENT (at your option) any later version.\" , \"$LINE_COMMENT\" , \"$LINE_COMMENT This program is distributed in the hope that it will be useful,\" , \"$LINE_COMMENT but WITHOUT ANY WARRANTY; without even the implied warranty of\" , \"$LINE_COMMENT MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\" , \"$LINE_COMMENT GNU General Public License for more details.\" , \"$LINE_COMMENT\" , \"$LINE_COMMENT You should have received a copy of the GNU General Public License\" , \"$LINE_COMMENT along with this program. If not, see <https://www.gnu.org/licenses/>.\" , \"\" , ], \"description\" : \"Add GPLv3 license information in source code.\" } Add a short description and use tab to jump to the end of the snippet.","title":"VS Code Settings"},{"location":"contribute/vscode/#pylance-settings","text":"To solve import errors in the repository, we need to add an extra path to the pylance config. For example in the workspace settings of OTCamera add: \"python.analysis.extraPaths\" : [ \"./OTCamera\" ] Or set it using the Settings-UI (Workspace -> Pylance -> Python - Analysis: Extra Path -> add './OTCamera')","title":"Pylance Settings"},{"location":"contribute/vscode/#snippets","text":"To comfortably insert code use snippets. You can open the configuration file for a specific language to add or modify the user snippets: Ctrl + Shift + p and Preferences: Configure User Snippets . You can insert the user snippets by start typing the \"prefix\" value. For example start typing gpl_ and autocompletection should recognize the user snippets stated below.","title":"Snippets"},{"location":"contribute/vscode/#gpl-license-information","text":"Add the following snippet to vscodedata/user/snippets/python.json \"gpl_license_header\" : { \"prefix\" : \"gpl_add_head\" , \"body\" :[ \"$LINE_COMMENT ${1:Program Name and Function}\" , \"$LINE_COMMENT Copyright (C) $CURRENT_YEAR OpenTrafficCam Contributors\" , \"$LINE_COMMENT <https://github.com/OpenTrafficCam\" , \"$LINE_COMMENT <team@opentrafficcam.org>\" , \"$LINE_COMMENT\" , \"$LINE_COMMENT This program is free software: you can redistribute it and/or modify\" , \"$LINE_COMMENT it under the terms of the GNU General Public License as published by\" , \"$LINE_COMMENT the Free Software Foundation, either version 3 of the License, or\" , \"$LINE_COMMENT (at your option) any later version.\" , \"$LINE_COMMENT\" , \"$LINE_COMMENT This program is distributed in the hope that it will be useful,\" , \"$LINE_COMMENT but WITHOUT ANY WARRANTY; without even the implied warranty of\" , \"$LINE_COMMENT MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\" , \"$LINE_COMMENT GNU General Public License for more details.\" , \"$LINE_COMMENT\" , \"$LINE_COMMENT You should have received a copy of the GNU General Public License\" , \"$LINE_COMMENT along with this program. If not, see <https://www.gnu.org/licenses/>.\" , \"\" , ], \"description\" : \"Add GPLv3 license information in source code.\" } Add a short description and use tab to jump to the end of the snippet.","title":"GPL License Information"},{"location":"overview/datastructures/","text":"Data Structures \u00b6 OTCamera \u00b6 Property Description CamID MAC address, unique CamModel - CamName - Calib \u00b6 Property Description CalibID Unique CamID MAC address from object \"OTCamera\" CalibDateTime Date & Time when calibration was performed CalibK1 Dist. coeff. K1 CalibK2 Dist. coeff. K2 CalibK3 Dist. coeff. K3 CalibP1 Dist. coeff. P1 CalibP2 Dist. coeff. P2 Video \u00b6 Property Description VideoID Unique CamID MAC address from object \"OTCamera\" CalibID From object \"Calib\" VideoDateTime Date & Time when video starts VideoFPS - OTVision \u00b6 Property Description VisionID Unique VisionDetector e.g. YOLOv3, OcclusionNet, OpenPose VisionTracker e.g. IOU+mod VisionDistStatus Has undistortion been performed? VisionConvStatus Has conversion to world coordinates been performed? Bbox \u00b6 Property Description BboxID Unique BboxXmin bbox left limit BboxXmax bbox right limit BboxYmin bbox upper limit BboxYmax bbox lower limit BboxXctr x of bbox center BboxYctr y of bbox center BboxXheight Diff of BboxXmax and BboxXmin BboxYwidth Diff of BboxXmax and BboxXmin BboxTranspMode Mode of transportation BboxConfid Confidence of classification WireframeCar \u00b6 t.b.a. (alternative detector to YOLO) WireframeHuman \u00b6 t.b.a. (alternative detector to YOLO) Traj \u00b6 Property Description TrajID Unique TrajTranspMode Mode of transportation TrajConf Mean confidence of whole trajectory [%] TrajPt \u00b6 Property Description TrajPtID Unique TrajID From object \"Traj\" TrajPtTime Time stamp of a single trajectory point [sec] TrajPtXpx X coordinate of a single trajectory point [Px] TrajPtYpx Y coordinate of a single trajectory point [Px] TrajPtXworld X coordinate of a single trajectory point [UTM] TrajPtYworld Y coordinate of a single trajectory point [UTM] TrajPtSpeed Speed of a single trajectory point [km/h] TrajPtAcc Acceleration of a single trajectory point [m/s^2] TrajPtConf Confidence of a single trajectory point [%] RefPt \u00b6 Property Description RefPtID Unique RefPtXpx X Coord. [Px] RefPtYpx Y Coord. [Px] RefPtXworld X Coord. [UTM] RefPtYworld Y Coord. [UTM] OTAnalytics \u00b6 t.b.a. Gate \u00b6 Property Description GateID Unique GateX1px X Coord. Pt. 1 [Px] GateY1px Y Coord. Pt. 1 [Px] GateX2px X Coord. Pt. 2 [Px] GateY2px Y Coord. Pt. 2 [Px] GateX1world X Coord. Pt. 1 [UTM] GateY1world Y Coord. Pt. 1 [UTM] GateX2world X Coord. Pt. 2 [UTM] GateY2world Y Coord. Pt. 2 [UTM] Count \u00b6 Property Description CountID Unique GateID From object \"Gate\" TrajID From object \"Traj\" CountTranspMode Mode of transportation (from object \"Traj\") CountTime Video time [sec] CountDir Direction [1,2]","title":"Data Structures"},{"location":"overview/datastructures/#data-structures","text":"","title":"Data Structures"},{"location":"overview/datastructures/#otcamera","text":"Property Description CamID MAC address, unique CamModel - CamName -","title":"OTCamera"},{"location":"overview/datastructures/#calib","text":"Property Description CalibID Unique CamID MAC address from object \"OTCamera\" CalibDateTime Date & Time when calibration was performed CalibK1 Dist. coeff. K1 CalibK2 Dist. coeff. K2 CalibK3 Dist. coeff. K3 CalibP1 Dist. coeff. P1 CalibP2 Dist. coeff. P2","title":"Calib"},{"location":"overview/datastructures/#video","text":"Property Description VideoID Unique CamID MAC address from object \"OTCamera\" CalibID From object \"Calib\" VideoDateTime Date & Time when video starts VideoFPS -","title":"Video"},{"location":"overview/datastructures/#otvision","text":"Property Description VisionID Unique VisionDetector e.g. YOLOv3, OcclusionNet, OpenPose VisionTracker e.g. IOU+mod VisionDistStatus Has undistortion been performed? VisionConvStatus Has conversion to world coordinates been performed?","title":"OTVision"},{"location":"overview/datastructures/#bbox","text":"Property Description BboxID Unique BboxXmin bbox left limit BboxXmax bbox right limit BboxYmin bbox upper limit BboxYmax bbox lower limit BboxXctr x of bbox center BboxYctr y of bbox center BboxXheight Diff of BboxXmax and BboxXmin BboxYwidth Diff of BboxXmax and BboxXmin BboxTranspMode Mode of transportation BboxConfid Confidence of classification","title":"Bbox"},{"location":"overview/datastructures/#wireframecar","text":"t.b.a. (alternative detector to YOLO)","title":"WireframeCar"},{"location":"overview/datastructures/#wireframehuman","text":"t.b.a. (alternative detector to YOLO)","title":"WireframeHuman"},{"location":"overview/datastructures/#traj","text":"Property Description TrajID Unique TrajTranspMode Mode of transportation TrajConf Mean confidence of whole trajectory [%]","title":"Traj"},{"location":"overview/datastructures/#trajpt","text":"Property Description TrajPtID Unique TrajID From object \"Traj\" TrajPtTime Time stamp of a single trajectory point [sec] TrajPtXpx X coordinate of a single trajectory point [Px] TrajPtYpx Y coordinate of a single trajectory point [Px] TrajPtXworld X coordinate of a single trajectory point [UTM] TrajPtYworld Y coordinate of a single trajectory point [UTM] TrajPtSpeed Speed of a single trajectory point [km/h] TrajPtAcc Acceleration of a single trajectory point [m/s^2] TrajPtConf Confidence of a single trajectory point [%]","title":"TrajPt"},{"location":"overview/datastructures/#refpt","text":"Property Description RefPtID Unique RefPtXpx X Coord. [Px] RefPtYpx Y Coord. [Px] RefPtXworld X Coord. [UTM] RefPtYworld Y Coord. [UTM]","title":"RefPt"},{"location":"overview/datastructures/#otanalytics","text":"t.b.a.","title":"OTAnalytics"},{"location":"overview/datastructures/#gate","text":"Property Description GateID Unique GateX1px X Coord. Pt. 1 [Px] GateY1px Y Coord. Pt. 1 [Px] GateX2px X Coord. Pt. 2 [Px] GateY2px Y Coord. Pt. 2 [Px] GateX1world X Coord. Pt. 1 [UTM] GateY1world Y Coord. Pt. 1 [UTM] GateX2world X Coord. Pt. 2 [UTM] GateY2world Y Coord. Pt. 2 [UTM]","title":"Gate"},{"location":"overview/datastructures/#count","text":"Property Description CountID Unique GateID From object \"Gate\" TrajID From object \"Traj\" CountTranspMode Mode of transportation (from object \"Traj\") CountTime Video time [sec] CountDir Direction [1,2]","title":"Count"},{"location":"overview/howitworks/","text":"How it works \u00b6 The Framework of OpenTrafficCam's Core Functions","title":"How it Works"},{"location":"overview/howitworks/#how-it-works","text":"The Framework of OpenTrafficCam's Core Functions","title":"How it works"},{"location":"overview/whatitdoes/","text":"What it does \u00b6 OpenTrafficCam consists of three parts. The OTCamera hardware itself to record videos, OTVision , a collection of algorithms to generate trajectories of objects (road users) in the videos and OTAnalytics to gather traffic measures based on the trajectories. In addition to these three tools, we will provide a set of labelled images of German traffic objects with OTLabels .","title":"What it Does"},{"location":"overview/whatitdoes/#what-it-does","text":"OpenTrafficCam consists of three parts. The OTCamera hardware itself to record videos, OTVision , a collection of algorithms to generate trajectories of objects (road users) in the videos and OTAnalytics to gather traffic measures based on the trajectories. In addition to these three tools, we will provide a set of labelled images of German traffic objects with OTLabels .","title":"What it does"}]}