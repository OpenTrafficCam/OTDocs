{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to OpenTrafficCam \u00b6 Open Traffic Cam makes traffic detection easier, faster and more efficient. Welcome to OpenTrafficCam , the only fully integrated OpenSource-workflow for video-based traffic detection. This website will cover all the documentation in one place. Please also check out our github page for further information and the code. How it works \u00b6 OpenTrafficCam consists of three modules. The OTCamera hardware itself to record videos, OTVision , a collection of algorithms to generate trajectories of objects (road users) based on the videos and OTAnalytics to gather traffic measures based on the trajectories. In addition to these three modules, we will provide a set of labelled images of German traffic objects with OTLabels . Motivation and Development \u00b6 OpenTrafficCam is funded by the German Federal Ministry of Transport and Digital Infrastructure as part of the research initiative mFUND.","title":"Welcome"},{"location":"#welcome-to-opentrafficcam","text":"Open Traffic Cam makes traffic detection easier, faster and more efficient. Welcome to OpenTrafficCam , the only fully integrated OpenSource-workflow for video-based traffic detection. This website will cover all the documentation in one place. Please also check out our github page for further information and the code.","title":"Welcome to OpenTrafficCam"},{"location":"#how-it-works","text":"OpenTrafficCam consists of three modules. The OTCamera hardware itself to record videos, OTVision , a collection of algorithms to generate trajectories of objects (road users) based on the videos and OTAnalytics to gather traffic measures based on the trajectories. In addition to these three modules, we will provide a set of labelled images of German traffic objects with OTLabels .","title":"How it works"},{"location":"#motivation-and-development","text":"OpenTrafficCam is funded by the German Federal Ministry of Transport and Digital Infrastructure as part of the research initiative mFUND.","title":"Motivation and Development"},{"location":"OTAnalytics/","text":"Features \u00b6 The brain of OpenTrafficCam. OTAnalytics is a set of self developed algorithms to gather traffic measures based on trajectories of road users. Key features \u00b6 Generation of traffic flow matrices and node flow diagrams Measurement of time gaps, velocities and accelerations Long-term: Analysis of near-accidents (e.g. TTC, PET) Visualization of traffic data Import of trajectories from other systems (e.g. DataFromSky) Runs on any Windows laptop Content of documentation \u00b6 Most of the code will be developed by the OpenTrafficCam contributors (based on python libraries like NumPy , pandas , tkinter and OpenCV ).","title":"Features"},{"location":"OTAnalytics/#features","text":"The brain of OpenTrafficCam. OTAnalytics is a set of self developed algorithms to gather traffic measures based on trajectories of road users.","title":"Features"},{"location":"OTAnalytics/#key-features","text":"Generation of traffic flow matrices and node flow diagrams Measurement of time gaps, velocities and accelerations Long-term: Analysis of near-accidents (e.g. TTC, PET) Visualization of traffic data Import of trajectories from other systems (e.g. DataFromSky) Runs on any Windows laptop","title":"Key features"},{"location":"OTAnalytics/#content-of-documentation","text":"Most of the code will be developed by the OpenTrafficCam contributors (based on python libraries like NumPy , pandas , tkinter and OpenCV ).","title":"Content of documentation"},{"location":"OTAnalytics/Accuracy/counts/","text":"Traffic counts \u00b6 Why is a high quality of the counts so important? \u00b6 One of the main goals of OpenTrafficCam are automatic traffic counts at road sections and intersections. However, the results of such counts are only suitable for mobility planning if they are of high quality. In the worst case, traffic facilities planned based on inaccurate count data can waste public resources and even endanger the health of their users. How to measure the quality of the counts? \u00b6 Precision and Recall, which are used to evaluate machine learning algorithms, can similarly be used as meaningful metrics to evaluate the quality of automated traffic counts. The precision metric describes the ratio of vehicles counted correctly by OpenTrafficCam to all vehicles counted by OpenTrafficCam including incorrect ones. On the other hand, the recall metric describes the ratio of vehicles correctly counted by OpenTrafficCam to all vehicles passed in reality including those not counted by OpenTrafficCam. An Example: 100 vehicles pass through a junction 80 of them are correctly detected by the system (true positive) 20 are therefore not correctly detected (false negative) the system indicates that 110 vehicles would have passed the intersection the system has thus detected 30 vehicles that were not present in reality (false positive) 110 got detected 20 got not detected 100 vehicles passed 80 (true positive) 20 (false negative) 30 are not real 30 (false positive) -- (true negative) The prescision (or accuracy) is equal to true positive / (true positive + false positive). In this case the prescision would be 80 / (80 + 30) = 0.73. The recall (or sensitivity) is equal to true positive / (true positive + false negative). In this case the recall would be 80 / (80 + 20) = 0.80. The higher the two measurements are, the higher the quality of the results are. How accurate can you count using OpenTrafficCam? \u00b6 In his diploma thesis OpenTrafficCam contributor Armin Kollascheck investigated the quality of counting motorized traffic that got generated by OpenTrafficCam using YOLOv5x6 (Ultralytics), and a modified version of the IOU-Tracker (Bochinski et al.) for the following eight realistic traffic scenarios. Three-way intersection Four-way intersection (1) Four-way intersection (2) Four-way intersection (2) at night Four-way roundabout Section (1) Section (2) Section (2) at night Armin evaluated the count data automatically generated with OpenTrafficCam against a manually collected and verified ground truth. The sample sizes for determining the precision and recall of the automated traffic counts are 200 vehicles for each scenario. The ground truth of 200 vehicles was always counted manually. The results are shown in the table below. Traffic facility Lighting Camera angle Precision Recall Three-way intersection Day Steep 0.95 0.85 Four-way intersection (1) Day Steep 1.00 0.94 Four-way intersection (2) Day Steep 0.99 0.95 Four-way intersection (2) Night with street lights Steep 0.98 0.95 Four-way roundabout Day Flat 0.63 0.56 Section (1) Day Steep 1.00 0.91 Section (2) Day Steep 1.00 0.97 Section (2) Night without street lights Steep 0.06 0.12 Conclusion \u00b6 The high values for precision and recall in many scenarios show that OpenTrafficCam already works quite well with the official YOLOv5x6 model and a rather simple tracker. Thus, on different traffic facilities, counting qualities were achieved that probably exceed those of manual on-site counts. And even video recordings at night with street-side lighting seem to be counted correct with OpenTrafficCam. Unsatisfactory results, however, occurred with comparatively flat camera angles or night recordings without street lighting. This shows that the current pipeline is not yet perfect. We aim to further improve OpenTrafficCam to achieve satisfying counting results even under very difficult circumstances.","title":"Traffic counts"},{"location":"OTAnalytics/Accuracy/counts/#traffic-counts","text":"","title":"Traffic counts"},{"location":"OTAnalytics/Accuracy/counts/#why-is-a-high-quality-of-the-counts-so-important","text":"One of the main goals of OpenTrafficCam are automatic traffic counts at road sections and intersections. However, the results of such counts are only suitable for mobility planning if they are of high quality. In the worst case, traffic facilities planned based on inaccurate count data can waste public resources and even endanger the health of their users.","title":"Why is a high quality of the counts so important?"},{"location":"OTAnalytics/Accuracy/counts/#how-to-measure-the-quality-of-the-counts","text":"Precision and Recall, which are used to evaluate machine learning algorithms, can similarly be used as meaningful metrics to evaluate the quality of automated traffic counts. The precision metric describes the ratio of vehicles counted correctly by OpenTrafficCam to all vehicles counted by OpenTrafficCam including incorrect ones. On the other hand, the recall metric describes the ratio of vehicles correctly counted by OpenTrafficCam to all vehicles passed in reality including those not counted by OpenTrafficCam. An Example: 100 vehicles pass through a junction 80 of them are correctly detected by the system (true positive) 20 are therefore not correctly detected (false negative) the system indicates that 110 vehicles would have passed the intersection the system has thus detected 30 vehicles that were not present in reality (false positive) 110 got detected 20 got not detected 100 vehicles passed 80 (true positive) 20 (false negative) 30 are not real 30 (false positive) -- (true negative) The prescision (or accuracy) is equal to true positive / (true positive + false positive). In this case the prescision would be 80 / (80 + 30) = 0.73. The recall (or sensitivity) is equal to true positive / (true positive + false negative). In this case the recall would be 80 / (80 + 20) = 0.80. The higher the two measurements are, the higher the quality of the results are.","title":"How to measure the quality of the counts?"},{"location":"OTAnalytics/Accuracy/counts/#how-accurate-can-you-count-using-opentrafficcam","text":"In his diploma thesis OpenTrafficCam contributor Armin Kollascheck investigated the quality of counting motorized traffic that got generated by OpenTrafficCam using YOLOv5x6 (Ultralytics), and a modified version of the IOU-Tracker (Bochinski et al.) for the following eight realistic traffic scenarios. Three-way intersection Four-way intersection (1) Four-way intersection (2) Four-way intersection (2) at night Four-way roundabout Section (1) Section (2) Section (2) at night Armin evaluated the count data automatically generated with OpenTrafficCam against a manually collected and verified ground truth. The sample sizes for determining the precision and recall of the automated traffic counts are 200 vehicles for each scenario. The ground truth of 200 vehicles was always counted manually. The results are shown in the table below. Traffic facility Lighting Camera angle Precision Recall Three-way intersection Day Steep 0.95 0.85 Four-way intersection (1) Day Steep 1.00 0.94 Four-way intersection (2) Day Steep 0.99 0.95 Four-way intersection (2) Night with street lights Steep 0.98 0.95 Four-way roundabout Day Flat 0.63 0.56 Section (1) Day Steep 1.00 0.91 Section (2) Day Steep 1.00 0.97 Section (2) Night without street lights Steep 0.06 0.12","title":"How accurate can you count using OpenTrafficCam?"},{"location":"OTAnalytics/Accuracy/counts/#conclusion","text":"The high values for precision and recall in many scenarios show that OpenTrafficCam already works quite well with the official YOLOv5x6 model and a rather simple tracker. Thus, on different traffic facilities, counting qualities were achieved that probably exceed those of manual on-site counts. And even video recordings at night with street-side lighting seem to be counted correct with OpenTrafficCam. Unsatisfactory results, however, occurred with comparatively flat camera angles or night recordings without street lighting. This shows that the current pipeline is not yet perfect. We aim to further improve OpenTrafficCam to achieve satisfying counting results even under very difficult circumstances.","title":"Conclusion"},{"location":"OTAnalytics/gettingstarted/firstuse/","text":"First Use \u00b6 Load Video \u00b6 OTAnalytics provides a simple and classic Point-and-Click-Userinterface. Your first step should be to load a videofile in the most common formats such as mkv or mp4 by clicking on the \"load video\"-button. The chosen filename will be inserted in the upper left listbox and a canvas with the first videoframe will be created. You can navigate through the video by either by scrolling, sliding or simply playing the video. Load and select Detections \u00b6 With click on the \"Load tracks\"-button a dialogue window pops up which asks for the trackfile from OTVision . This file contains framewise information about vehicles such as class, position and size in pixel of detected objects. With this information OTAnalytics can calculate bounding boxes and driven tracks and colors them dependent on vehicleclass. After the import object IDs and object class will be displayed in die middle right listbox. You can display a single track or multiple tracks by selecting the object IDs or you can display all tracks at once. First glance \u00b6 After importing all necessary files. Its important to check if chosen data correspond to eachother. The easiest way to do this, is to play, slide or scroll through the video. If correct, bounding boxes should overlay vehicles, pedestrians and cyclist. The bounding boxes are annotated with object class and confidence factor. Its also possible to live draw tracks.","title":"First Use"},{"location":"OTAnalytics/gettingstarted/firstuse/#first-use","text":"","title":"First Use"},{"location":"OTAnalytics/gettingstarted/firstuse/#load-video","text":"OTAnalytics provides a simple and classic Point-and-Click-Userinterface. Your first step should be to load a videofile in the most common formats such as mkv or mp4 by clicking on the \"load video\"-button. The chosen filename will be inserted in the upper left listbox and a canvas with the first videoframe will be created. You can navigate through the video by either by scrolling, sliding or simply playing the video.","title":"Load Video"},{"location":"OTAnalytics/gettingstarted/firstuse/#load-and-select-detections","text":"With click on the \"Load tracks\"-button a dialogue window pops up which asks for the trackfile from OTVision . This file contains framewise information about vehicles such as class, position and size in pixel of detected objects. With this information OTAnalytics can calculate bounding boxes and driven tracks and colors them dependent on vehicleclass. After the import object IDs and object class will be displayed in die middle right listbox. You can display a single track or multiple tracks by selecting the object IDs or you can display all tracks at once.","title":"Load and select Detections"},{"location":"OTAnalytics/gettingstarted/firstuse/#first-glance","text":"After importing all necessary files. Its important to check if chosen data correspond to eachother. The easiest way to do this, is to play, slide or scroll through the video. If correct, bounding boxes should overlay vehicles, pedestrians and cyclist. The bounding boxes are annotated with object class and confidence factor. Its also possible to live draw tracks.","title":"First glance"},{"location":"OTAnalytics/gettingstarted/installation/","text":"Installation \u00b6 Install Python 3.9 \u00b6 If not done yet, install the 64-bit version Python 3.9.x via Windows installer from www.python.org/downloads/ as follows: What if I already have another Python version installed? In addition, also install Python 3.9. The last installed Python will automatically be the default Python interpreter of your system. On Windows, it is also possible to change the default Python interpreter by changing the order of the system-wide environment variables (move Python39 and Python39\\Scripts to the top, see animation below). This is necessary e.g. if you have already installed Python 3.9, but another Python version is your default because you installed it in the meantime (e.g. 3.10). Install OTAnalytics \u00b6 Download and unzip the latest version of OTAnalytics from Github. In the unzipped folder Double-click the \"install.bat\" and wait until the installation of the dependencies is complete. To run OTAnalytics \u00b6 ... double click the \"OTAnalytics.bat\" to run the Software via graphical user interface. If you encounter problems \u00b6 ... please open an issue in the OTAnalytics repository on Github. We also welcome code contributions (e.g. fixing bugs or adding features) from other programmers by forking the repository and creating a pull request. Please check the contribute section of this documentation first.","title":"Installation"},{"location":"OTAnalytics/gettingstarted/installation/#installation","text":"","title":"Installation"},{"location":"OTAnalytics/gettingstarted/installation/#install-python-39","text":"If not done yet, install the 64-bit version Python 3.9.x via Windows installer from www.python.org/downloads/ as follows: What if I already have another Python version installed? In addition, also install Python 3.9. The last installed Python will automatically be the default Python interpreter of your system. On Windows, it is also possible to change the default Python interpreter by changing the order of the system-wide environment variables (move Python39 and Python39\\Scripts to the top, see animation below). This is necessary e.g. if you have already installed Python 3.9, but another Python version is your default because you installed it in the meantime (e.g. 3.10).","title":"Install Python 3.9"},{"location":"OTAnalytics/gettingstarted/installation/#install-otanalytics","text":"Download and unzip the latest version of OTAnalytics from Github. In the unzipped folder Double-click the \"install.bat\" and wait until the installation of the dependencies is complete.","title":"Install OTAnalytics"},{"location":"OTAnalytics/gettingstarted/installation/#to-run-otanalytics","text":"... double click the \"OTAnalytics.bat\" to run the Software via graphical user interface.","title":"To run OTAnalytics"},{"location":"OTAnalytics/gettingstarted/installation/#if-you-encounter-problems","text":"... please open an issue in the OTAnalytics repository on Github. We also welcome code contributions (e.g. fixing bugs or adding features) from other programmers by forking the repository and creating a pull request. Please check the contribute section of this documentation first.","title":"If you encounter problems"},{"location":"OTAnalytics/gettingstarted/requirements/","text":"Requirements \u00b6 Hardware prerequisites \u00b6 Both OTVision and OTAnalytics run on modern 64 bit Windows 10 desktops and laptops (e.g. i5 processor and 8 GB RAM).","title":"Requirements"},{"location":"OTAnalytics/gettingstarted/requirements/#requirements","text":"","title":"Requirements"},{"location":"OTAnalytics/gettingstarted/requirements/#hardware-prerequisites","text":"Both OTVision and OTAnalytics run on modern 64 bit Windows 10 desktops and laptops (e.g. i5 processor and 8 GB RAM).","title":"Hardware prerequisites"},{"location":"OTAnalytics/preparingtracks/movements/","text":"Movements \u00b6 Movements represent source destination relationships of traffic flow and single objects like cars, trucks or pedestrians. The definition of movements play a big role if your analysis needs to be more detailed and specific. Create an delete Movements \u00b6 Movements can be defined with a click on the \"new movement\"-button and adding pre defined Sections while being selected. Therefore movements can consist of infinite sections where the first section is the source and the last section stands for the destination. As sections, movements can be saved, renamed, deleted and imported.","title":"Movements"},{"location":"OTAnalytics/preparingtracks/movements/#movements","text":"Movements represent source destination relationships of traffic flow and single objects like cars, trucks or pedestrians. The definition of movements play a big role if your analysis needs to be more detailed and specific.","title":"Movements"},{"location":"OTAnalytics/preparingtracks/movements/#create-an-delete-movements","text":"Movements can be defined with a click on the \"new movement\"-button and adding pre defined Sections while being selected. Therefore movements can consist of infinite sections where the first section is the source and the last section stands for the destination. As sections, movements can be saved, renamed, deleted and imported.","title":"Create an delete Movements"},{"location":"OTAnalytics/preparingtracks/sections/","text":"Sections \u00b6 OTAnalytics provides functions to lay down sections for analyzing traffic flow and volume over time or traffic composition. Sections work as sensors which can detect crossing events and associated time and also the direction of objects if driven over multiple sections. Sections can easily be modeled as line or areasections depending on your purpose. Create and delete Sections \u00b6 For a linesection simply toggle the \"Line\"-button, click- and drag your mouse over the canvas. Press enter to define a sectionname and finish the creationprocess. Areasections work almost similar. Leftclick to define boundaries and rightclick to automatically close the polygon. Created sections will appear in the middle left listbox. Selected section will be highlighted on canvas and can be deleted or renamed. If you want to continue your work you can ex- and later import a flow-file that contains information about sections and movements.","title":"Sections"},{"location":"OTAnalytics/preparingtracks/sections/#sections","text":"OTAnalytics provides functions to lay down sections for analyzing traffic flow and volume over time or traffic composition. Sections work as sensors which can detect crossing events and associated time and also the direction of objects if driven over multiple sections. Sections can easily be modeled as line or areasections depending on your purpose.","title":"Sections"},{"location":"OTAnalytics/preparingtracks/sections/#create-and-delete-sections","text":"For a linesection simply toggle the \"Line\"-button, click- and drag your mouse over the canvas. Press enter to define a sectionname and finish the creationprocess. Areasections work almost similar. Leftclick to define boundaries and rightclick to automatically close the polygon. Created sections will appear in the middle left listbox. Selected section will be highlighted on canvas and can be deleted or renamed. If you want to continue your work you can ex- and later import a flow-file that contains information about sections and movements.","title":"Create and delete Sections"},{"location":"OTAnalytics/trafficanalytics/counts/","text":"Traffic Counts \u00b6 The main purpose of OTAnalytics is to determine traffic flow and counting. By analyzing your data vehicle class will be grouped by gatecrossing events, movements and chosen timeintervall and can be exported as excel-file for further remodeling and use. To present your analysis OTAnalytics provide different easy to adjust printable diagrams.","title":"Traffic Counts"},{"location":"OTAnalytics/trafficanalytics/counts/#traffic-counts","text":"The main purpose of OTAnalytics is to determine traffic flow and counting. By analyzing your data vehicle class will be grouped by gatecrossing events, movements and chosen timeintervall and can be exported as excel-file for further remodeling and use. To present your analysis OTAnalytics provide different easy to adjust printable diagrams.","title":"Traffic Counts"},{"location":"OTCamera/","text":"Features \u00b6 The eye of OpenTrafficCam. OTCamera is a mobile camera system for capturing traffic videos. It is not for sale, but for DIY. You don't want to build the camera yourself and preferably don't want to have anything to do with the measurement? Well, just write us . Key features \u00b6 Based on Raspberry Pi Zero W Waterproof case Operation with buttons and/or smartphone Continuous recording for about one week Privacy compliant recording Under 400 \u20ac per camera system Content of this documentation \u00b6 List of hardware components (and where to purchase them) Software needed for video capturing and camera calibration Instructions for assembling the hardware and installing the software Recommendations for mounting, alignment and settings of the camera We are providing this information for two different prototypes. The Development Version is intended to actively participate in the development of OTCamera. It is also recommended to set up a development version to adapt the software and hardware to your own requirements. In addition, getting started with this prototype is very easy and can easily be done directly at your desk without the need for additional hardware. For the Field Version you need the ability to solder (simple) components to a blank circuit board. For this you get the most buttons (yay buttons) and the easiest operation in the field. All parts can be purchased from various dealers and are then easily plugged together. Also, the power consumption is the lowest (and the battery life the longest) and you can fully customize the hardware to your own needs.","title":"Features"},{"location":"OTCamera/#features","text":"The eye of OpenTrafficCam. OTCamera is a mobile camera system for capturing traffic videos. It is not for sale, but for DIY. You don't want to build the camera yourself and preferably don't want to have anything to do with the measurement? Well, just write us .","title":"Features"},{"location":"OTCamera/#key-features","text":"Based on Raspberry Pi Zero W Waterproof case Operation with buttons and/or smartphone Continuous recording for about one week Privacy compliant recording Under 400 \u20ac per camera system","title":"Key features"},{"location":"OTCamera/#content-of-this-documentation","text":"List of hardware components (and where to purchase them) Software needed for video capturing and camera calibration Instructions for assembling the hardware and installing the software Recommendations for mounting, alignment and settings of the camera We are providing this information for two different prototypes. The Development Version is intended to actively participate in the development of OTCamera. It is also recommended to set up a development version to adapt the software and hardware to your own requirements. In addition, getting started with this prototype is very easy and can easily be done directly at your desk without the need for additional hardware. For the Field Version you need the ability to solder (simple) components to a blank circuit board. For this you get the most buttons (yay buttons) and the easiest operation in the field. All parts can be purchased from various dealers and are then easily plugged together. Also, the power consumption is the lowest (and the battery life the longest) and you can fully customize the hardware to your own needs.","title":"Content of this documentation"},{"location":"OTCamera/calibration/calibrate/","text":"How to calibrate OTCamera \u00b6 OTCamera gives you the opportunity to receive distinctive camera parameters. These include intrinsics, extrinsics and distortion coefficients. By using our web browser based program and multiple images of a calibration pattern, like a chessboard you can calculate a camera matrix and distortion coefficients. You will be able to evaluate your calibration with a reprojection error which is used to quantify how closely you can recreate true projection. In other words: You can undistort pictures, trajectories or any other pixel coordinates. First step First you need to download a chessboard pattern (you can search for calibration board). The pattern size and dimension will play no big role for now, so feel free to use any chessboard. Attach your printout to a solid object. The image should be as flat as possible. For best results its unavoidable to let your calibration pattern be manufactured by professionals. Procedure Start the calibration application on your raspberrypi. Create a new calibration with a self-chosen CamID. Enter the number of chessboardcolumns, -rows and the squaresize in mm. Choose a number of wanted successful calibration pictures and resolution. Start the calibrationprocess. By clicking on \"receive coefficients\" your camera parameters will be stored as json-file on your raspberrypi. Evaluate the reprojection error displayed in the userinterface.","title":"How to calibrate OTCamera"},{"location":"OTCamera/calibration/calibrate/#how-to-calibrate-otcamera","text":"OTCamera gives you the opportunity to receive distinctive camera parameters. These include intrinsics, extrinsics and distortion coefficients. By using our web browser based program and multiple images of a calibration pattern, like a chessboard you can calculate a camera matrix and distortion coefficients. You will be able to evaluate your calibration with a reprojection error which is used to quantify how closely you can recreate true projection. In other words: You can undistort pictures, trajectories or any other pixel coordinates. First step First you need to download a chessboard pattern (you can search for calibration board). The pattern size and dimension will play no big role for now, so feel free to use any chessboard. Attach your printout to a solid object. The image should be as flat as possible. For best results its unavoidable to let your calibration pattern be manufactured by professionals. Procedure Start the calibration application on your raspberrypi. Create a new calibration with a self-chosen CamID. Enter the number of chessboardcolumns, -rows and the squaresize in mm. Choose a number of wanted successful calibration pictures and resolution. Start the calibrationprocess. By clicking on \"receive coefficients\" your camera parameters will be stored as json-file on your raspberrypi. Evaluate the reprojection error displayed in the userinterface.","title":"How to calibrate OTCamera"},{"location":"OTCamera/gettingstarted/assembly/","text":"Assembly \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Assembly"},{"location":"OTCamera/gettingstarted/assembly/#assembly","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Assembly"},{"location":"OTCamera/gettingstarted/firstuse/","text":"First Use \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"First Use"},{"location":"OTCamera/gettingstarted/firstuse/#first-use","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"First Use"},{"location":"OTCamera/gettingstarted/installation/","text":"Installation \u00b6 No matter which version you want to install, you will always use a Raspberry Pi as a base. This guide describes the basic steps you need to perform for each version. You will need: Raspberry Pi 2B/3B(+)/4/Zero W and power supply Micro SD card (a High Endurance version is recommended) SD Card Reader Raspberry Pi Imager . Warning Even though we have completed the guide ourselves, every PC is different and Raspberry OS can change as well. There are steps described with which you can break your Windows on the PC. You should know what you are doing. We are not responsible for any damage that may occur. Prepare the SD Card \u00b6 Download and install the Raspberry Pi Imager , insert the SD-Card and start the Imager. It's recommended to use the Raspberry Pi Imager Advanced Options to do some basic setup. Just press Ctrl + Shift + X on startscreen to access advanced options. Setup a hostname, enable SSH, either password based or using public-key authentication (recommended!), configure your Wifi credentials and choose the correct locale for Wifi, timezone and keyboard layout. You can also skip the first-run wizard. How to generate a public key Generate SSH-Keys for password-less connection. On your desktop computer open a command line terminal (CMD or Powershell on Windows or a bash on Linux) and run ssh-keygen to generate a public private key combination. Add the private key to your ssh-agent (you may need to update OpenSSH on Windows ). Now insert the SD card into the PC. Select Raspberry Pi OS Lite (32-bit) as operating system under \"Raspberry Pi (Other)\". Then select the SD card on which the operating system will be installed. Warning It is possible to select not only the SD card, but also the hard drive or SSD where your operating system or other PC data is stored. Make sure that you really select the right SD card. Most of the time, you can tell by the size. Press the Write button and wait until the Raspberry Pi Imager has completely written and verified your SD card. Tip Sometimes the verification may fail. Try another USB port on your PC or another card reader. If this does not help, maybe the SD card is broken. Setup without Raspberry Pi Imager Add an empty file named ssh to the boot partition to enable ssh on first boot . Add your Wifi credentials as described in the Raspberry Pi Documentation Connect to the pi using ssh ( ssh pi@raspberry )to the Pi using password authentication. Create and edit the needed authorized_keys file. mkdir -p ~/.ssh nano ~/.ssh/authorized_keys Copy your public key on the host and paste it on the pi, save&close using Ctrl+X - Enter - Y. Setup the Raspberry \u00b6 Now, take the SD-Card and insert it in the Raspberry. You can now boot the Pi on LAN or within your Wifi by connecting it to the power supply. Try to connect to the Pi using a command line or PowerShell: ssh pi @otcamera01 If everything is setup correctly, you will be asked to add the host key to the list of known hosts (answer: yes) and you should be connected to your Raspberry Pi. warning: agent returned different signature type ssh-rsa (expected rsa-sha2-512) If you are on Windows you may need to update OpenSSH if you ar getting this error: warning : agent returned different signature type ssh-rsa ( expected rsa-sha2 - 512 ) Warning This guide should run on Windows 10 but you are modifing your system files. Please do not do anything you do not understand! Read the source for more information. We are not liable for any damage you may cause to your system. But we followed the steps ourselves and everything went well. Download the latest OpenSSH for Windows release here . Depending on your Windows version it is probably OpenSSH-Win64.zip . Open Windows Explorer and navigate to your Download folder. You should see the OpenSSH-Win64.zip . Open an elevated PowerShell via File \u2192 Open Windows PowerShell \u2192 Open Windows PowerShell as administrator. If you copy and paste the following code, you will unzip OpenSSH and overwrite the system's default OpenSSH version. # Overwrite windows installed bins $openSshBins = ( Get-ChildItem 'C:\\WINDOWS\\System32\\OpenSSH\\' ). Name Expand-Archive -Path .\\ OpenSSH-Win64 . zip -DestinationPath . takeown . exe / a / r / f C :\\ Windows \\ System32 \\ OpenSSH \\ icacls . exe 'C:\\Windows\\System32\\OpenSSH' / grant 'BUILTIN\\Administrators:(OI)(CI)F' icacls . exe 'C:\\Windows\\System32\\OpenSSH' / grant 'BUILTIN\\Administrators:F' / t Stop-Service ssh-agent $openSshBins | %{ Copy-Item -Path .\\ OpenSSH-Win64 \\ $_ -Destination C :\\ Windows \\ System32 \\ OpenSSH \\ } Start-Service ssh-agent If you have successfully logged in now, we can configure the Raspberry Pi for the OpenTrafficCam. Update the pi by running apt and reboot. sudo apt update && sudo apt upgrade -y sudo reboot A new version of configuration file is available If you get this message, don't worry. Keep the local version currently installed, since we changed the ssh server configuration using the RPi Imager. Reconnect to your pi (open PowerShell and run ssh pi@otcamera01 ) and run the raspberry configuration tool. sudo raspi-config Change the following settings to appropriate values: System Options \u2192 Password (choose a new password for security reasons) Interface Options \u2192 Camera \u2192 enable Interface Options \u2192 Serial Port \u2192 no \u2192 Serial Hardware \u2192 yes Setup without Raspberry Pi Imager If you did not use the Raspberry Pi Imager, you will need to setup a few more things. System Options Hostname Localization Options Timezone (Europe/Berlin) WLAN Country (DE) Exit the raspi-config selecting \"Finish\" and reboot the Pi afterwards. Setup Python and Dependencies \u00b6 By default, Raspberry OS light doesn't come with PIP installed. We will need it, to install required packages. sudo apt install python3-pip -y Raspberry OS ships with python 2 and python 3. By default python 2 is used. We want to change that to python 3 by adding two single lines to .bashrc . echo \"alias python='/usr/bin/python3'\" >> ~/.bashrc echo \"alias pip=pip3\" >> ~/.bashrc source ~/.bashrc python --version pip --version Both commands should state, that they are (using) python 3.(x). OTCamera requires additional python packages, which need to be installed. sudo apt install python3-picamera python3-gpiozero -y pip3 install pysimpleguiweb art psutil Note In the future, we would like to offer a ready-to-use image for the Raspberry Pi, which can be easily installed. The setup will then be much easier.","title":"Installation"},{"location":"OTCamera/gettingstarted/installation/#installation","text":"No matter which version you want to install, you will always use a Raspberry Pi as a base. This guide describes the basic steps you need to perform for each version. You will need: Raspberry Pi 2B/3B(+)/4/Zero W and power supply Micro SD card (a High Endurance version is recommended) SD Card Reader Raspberry Pi Imager . Warning Even though we have completed the guide ourselves, every PC is different and Raspberry OS can change as well. There are steps described with which you can break your Windows on the PC. You should know what you are doing. We are not responsible for any damage that may occur.","title":"Installation"},{"location":"OTCamera/gettingstarted/installation/#prepare-the-sd-card","text":"Download and install the Raspberry Pi Imager , insert the SD-Card and start the Imager. It's recommended to use the Raspberry Pi Imager Advanced Options to do some basic setup. Just press Ctrl + Shift + X on startscreen to access advanced options. Setup a hostname, enable SSH, either password based or using public-key authentication (recommended!), configure your Wifi credentials and choose the correct locale for Wifi, timezone and keyboard layout. You can also skip the first-run wizard. How to generate a public key Generate SSH-Keys for password-less connection. On your desktop computer open a command line terminal (CMD or Powershell on Windows or a bash on Linux) and run ssh-keygen to generate a public private key combination. Add the private key to your ssh-agent (you may need to update OpenSSH on Windows ). Now insert the SD card into the PC. Select Raspberry Pi OS Lite (32-bit) as operating system under \"Raspberry Pi (Other)\". Then select the SD card on which the operating system will be installed. Warning It is possible to select not only the SD card, but also the hard drive or SSD where your operating system or other PC data is stored. Make sure that you really select the right SD card. Most of the time, you can tell by the size. Press the Write button and wait until the Raspberry Pi Imager has completely written and verified your SD card. Tip Sometimes the verification may fail. Try another USB port on your PC or another card reader. If this does not help, maybe the SD card is broken. Setup without Raspberry Pi Imager Add an empty file named ssh to the boot partition to enable ssh on first boot . Add your Wifi credentials as described in the Raspberry Pi Documentation Connect to the pi using ssh ( ssh pi@raspberry )to the Pi using password authentication. Create and edit the needed authorized_keys file. mkdir -p ~/.ssh nano ~/.ssh/authorized_keys Copy your public key on the host and paste it on the pi, save&close using Ctrl+X - Enter - Y.","title":"Prepare the SD Card"},{"location":"OTCamera/gettingstarted/installation/#setup-the-raspberry","text":"Now, take the SD-Card and insert it in the Raspberry. You can now boot the Pi on LAN or within your Wifi by connecting it to the power supply. Try to connect to the Pi using a command line or PowerShell: ssh pi @otcamera01 If everything is setup correctly, you will be asked to add the host key to the list of known hosts (answer: yes) and you should be connected to your Raspberry Pi. warning: agent returned different signature type ssh-rsa (expected rsa-sha2-512) If you are on Windows you may need to update OpenSSH if you ar getting this error: warning : agent returned different signature type ssh-rsa ( expected rsa-sha2 - 512 ) Warning This guide should run on Windows 10 but you are modifing your system files. Please do not do anything you do not understand! Read the source for more information. We are not liable for any damage you may cause to your system. But we followed the steps ourselves and everything went well. Download the latest OpenSSH for Windows release here . Depending on your Windows version it is probably OpenSSH-Win64.zip . Open Windows Explorer and navigate to your Download folder. You should see the OpenSSH-Win64.zip . Open an elevated PowerShell via File \u2192 Open Windows PowerShell \u2192 Open Windows PowerShell as administrator. If you copy and paste the following code, you will unzip OpenSSH and overwrite the system's default OpenSSH version. # Overwrite windows installed bins $openSshBins = ( Get-ChildItem 'C:\\WINDOWS\\System32\\OpenSSH\\' ). Name Expand-Archive -Path .\\ OpenSSH-Win64 . zip -DestinationPath . takeown . exe / a / r / f C :\\ Windows \\ System32 \\ OpenSSH \\ icacls . exe 'C:\\Windows\\System32\\OpenSSH' / grant 'BUILTIN\\Administrators:(OI)(CI)F' icacls . exe 'C:\\Windows\\System32\\OpenSSH' / grant 'BUILTIN\\Administrators:F' / t Stop-Service ssh-agent $openSshBins | %{ Copy-Item -Path .\\ OpenSSH-Win64 \\ $_ -Destination C :\\ Windows \\ System32 \\ OpenSSH \\ } Start-Service ssh-agent If you have successfully logged in now, we can configure the Raspberry Pi for the OpenTrafficCam. Update the pi by running apt and reboot. sudo apt update && sudo apt upgrade -y sudo reboot A new version of configuration file is available If you get this message, don't worry. Keep the local version currently installed, since we changed the ssh server configuration using the RPi Imager. Reconnect to your pi (open PowerShell and run ssh pi@otcamera01 ) and run the raspberry configuration tool. sudo raspi-config Change the following settings to appropriate values: System Options \u2192 Password (choose a new password for security reasons) Interface Options \u2192 Camera \u2192 enable Interface Options \u2192 Serial Port \u2192 no \u2192 Serial Hardware \u2192 yes Setup without Raspberry Pi Imager If you did not use the Raspberry Pi Imager, you will need to setup a few more things. System Options Hostname Localization Options Timezone (Europe/Berlin) WLAN Country (DE) Exit the raspi-config selecting \"Finish\" and reboot the Pi afterwards.","title":"Setup the Raspberry"},{"location":"OTCamera/gettingstarted/installation/#setup-python-and-dependencies","text":"By default, Raspberry OS light doesn't come with PIP installed. We will need it, to install required packages. sudo apt install python3-pip -y Raspberry OS ships with python 2 and python 3. By default python 2 is used. We want to change that to python 3 by adding two single lines to .bashrc . echo \"alias python='/usr/bin/python3'\" >> ~/.bashrc echo \"alias pip=pip3\" >> ~/.bashrc source ~/.bashrc python --version pip --version Both commands should state, that they are (using) python 3.(x). OTCamera requires additional python packages, which need to be installed. sudo apt install python3-picamera python3-gpiozero -y pip3 install pysimpleguiweb art psutil Note In the future, we would like to offer a ready-to-use image for the Raspberry Pi, which can be easily installed. The setup will then be much easier.","title":"Setup Python and Dependencies"},{"location":"OTCamera/gettingstarted/requirements/","text":"Requirements \u00b6 You will need some special hardware to build your own OTCamera to record videos. OTCamera is based on a Raspberry Pi Zero W and the official Rapberry Pi Camera Module V2. A specially designed PCB (printed circuit board) is needed to connect all additional parts to the Raspberry Pi. The single parts for the PCB as well as basic soldering skills are required to assemble a OTCamera. As housing a common waterproof explorer case is used in addition to a 3d printed inlay which carries all OTCamera parts. To power everthing up, we will use a USB Power Bank. All in all we will need: Raspberry Pi Zero W Raspbrry Pi Camera Module V2 Ribbon cable to connect Camera to Raspberry micro SD card OTCamera PCB Adafruit PowerBoost 1000C 1x8 header female and male 1x5 header female two 2x3 male header 2x4 male header 40 Pin GPIO female header 2x1 male header 7.5 mm 4700 \u00b5F capacitor two 100 k\u2126 resitors 10 k\u2126 resistor 47 k\u2126 resistor Si-diode 1 cell LiPo battery four switches (3d print is optimized for Marquardt 1801) three LEDs (for example Barthelme 6V DC, 9.5mm) some cables and cable shoes To assemble everything, we will need a drill, a soldering iron and some basics tools as well.","title":"Requirements"},{"location":"OTCamera/gettingstarted/requirements/#requirements","text":"You will need some special hardware to build your own OTCamera to record videos. OTCamera is based on a Raspberry Pi Zero W and the official Rapberry Pi Camera Module V2. A specially designed PCB (printed circuit board) is needed to connect all additional parts to the Raspberry Pi. The single parts for the PCB as well as basic soldering skills are required to assemble a OTCamera. As housing a common waterproof explorer case is used in addition to a 3d printed inlay which carries all OTCamera parts. To power everthing up, we will use a USB Power Bank. All in all we will need: Raspberry Pi Zero W Raspbrry Pi Camera Module V2 Ribbon cable to connect Camera to Raspberry micro SD card OTCamera PCB Adafruit PowerBoost 1000C 1x8 header female and male 1x5 header female two 2x3 male header 2x4 male header 40 Pin GPIO female header 2x1 male header 7.5 mm 4700 \u00b5F capacitor two 100 k\u2126 resitors 10 k\u2126 resistor 47 k\u2126 resistor Si-diode 1 cell LiPo battery four switches (3d print is optimized for Marquardt 1801) three LEDs (for example Barthelme 6V DC, 9.5mm) some cables and cable shoes To assemble everything, we will need a drill, a soldering iron and some basics tools as well.","title":"Requirements"},{"location":"OTCamera/recording/getvideos/","text":"Get the Videos \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Get the Videos"},{"location":"OTCamera/recording/getvideos/#get-the-videos","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Get the Videos"},{"location":"OTCamera/recording/mounting/","text":"Mounting \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Mounting"},{"location":"OTCamera/recording/mounting/#mounting","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Mounting"},{"location":"OTCamera/recording/preparation/","text":"Preparation \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Preparation"},{"location":"OTCamera/recording/preparation/#preparation","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Preparation"},{"location":"OTCamera/recording/record/","text":"Record \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Record"},{"location":"OTCamera/recording/record/#record","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Record"},{"location":"OTCamera/recording/safetywarning/","text":"Safety Warning \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Safety Warning"},{"location":"OTCamera/recording/safetywarning/#safety-warning","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Safety Warning"},{"location":"OTCamera/recording/settings/","text":"Settings \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Settings"},{"location":"OTCamera/recording/settings/#settings","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Settings"},{"location":"OTLabels/","text":"Features \u00b6 The backbone of OpenTrafficCam. OTLabels is a workflow for retraining AI models to detect road users in videos based on custom video frames and existing open source software packages ( CVAT , YOLO v5 ). Key features \u00b6 Annotation of custom video frames using existing open source software ( CVAT ) Retrain existing neural network models with YOLO v5 for individual optimization of the detection ability Runs on any Windows laptop. However, a decent GPU with CUDA support is strongly recommended. Content of documentation \u00b6 Training Validation Models Most of the code describing the workflow will be developed by the OpenTrafficCam contributors (based on python libraries like NumPy , pandas , tkinter and OpenCV ).","title":"Features"},{"location":"OTLabels/#features","text":"The backbone of OpenTrafficCam. OTLabels is a workflow for retraining AI models to detect road users in videos based on custom video frames and existing open source software packages ( CVAT , YOLO v5 ).","title":"Features"},{"location":"OTLabels/#key-features","text":"Annotation of custom video frames using existing open source software ( CVAT ) Retrain existing neural network models with YOLO v5 for individual optimization of the detection ability Runs on any Windows laptop. However, a decent GPU with CUDA support is strongly recommended.","title":"Key features"},{"location":"OTLabels/#content-of-documentation","text":"Training Validation Models Most of the code describing the workflow will be developed by the OpenTrafficCam contributors (based on python libraries like NumPy , pandas , tkinter and OpenCV ).","title":"Content of documentation"},{"location":"OTLabels/gettingstarted/data/","text":"Data \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Data"},{"location":"OTLabels/gettingstarted/data/#data","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Data"},{"location":"OTLabels/gettingstarted/installation/","text":"Installation \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Installation"},{"location":"OTLabels/gettingstarted/installation/#installation","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Installation"},{"location":"OTLabels/gettingstarted/requirements/","text":"Requirements \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Requirements"},{"location":"OTLabels/gettingstarted/requirements/#requirements","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Requirements"},{"location":"OTLabels/models/coco6/","text":"COCO 6-class \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"COCO 6-class"},{"location":"OTLabels/models/coco6/#coco-6-class","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"COCO 6-class"},{"location":"OTLabels/training/cvat/","text":"CVAT \u00b6 CVAT is a free video and image annotation tool for computer vision. We recommend the developers guides for Installation as well as the user's guide . If you want to label your own dataset to retrain models, keep in mind that the format of your new labels needs to be the same as the format of the original dataset. On this page we define our intended organizational and technical annotation workflow using CVAT. Since YOLOv5 is based on the the MS COCO dataset of 2D bounding boxes, you need to apply your own labels in the frames from your sample videos in the same format. We therefore recommend the following steps: Install and setup an instance of CVAT either on your local computer or on a server (recommended when working in a team). Import the videos in CVAT and select the frames you want to use for labelling. Download the dataset from CVAT using the YOLO v1.1 format. Pre-annotate your frames with the standard YOLOv5x model. The results do probably not meet the desired quality. However, the pre-annotated labels will save you some time since it not necessary to draw all labels from scratch. Upload the pre-annotated frames to CVAT and revise the detected labels. Download the final labels from CVAT and convert them to COCO format (using our cvat_to_coco.py script). The COCO dataset and consequently the YOLOv5 models can detect 80 classes. However, for the detection of traffic, only six classes are relevant. Target classes \u00b6 Target classes are all road user classes from the initial COCO data set that are relevant for our sample videos: Person Bicycle Motorcycle Car Bus Truck All vehicles or combinations of vehicles are labeled as \"truck\" if they have twin tires (except for regular service buses or coaches), a superstructure or a loading area or a trailer (also cars with trailer, the 2D box includes vehicle and trailer). Here, as in the COCO dataset, both the \"person\" and the \"bicycle\" are labeled separately for bicyclists. Object dimensions \u00b6 The 2D box boundaries are defined by the outermost parts of the objects visible in that frame of the video. Even if only a part of an object is visible and the class of the object is recognizable for the annotator, it should be labeled. Projects, Tasks and Jobs \u00b6 We define certain set of videos as a project . Every project has multiple tasks (each consists of one video to annotate). Each task has a status (\"Pending\", \"In Progress\" and \"Completed\") and a progress bar showing the number of completed jobs. Open the task with klick on \"Open\". For each task (or video) multiple jobs with a certain amounts of frames from the video are defined. Each job has a status (\"annotation\", \"validation\" and \"completed\"), an \"assignee\" (responsible for annotation) and a \"reviewer\" (responsible for reviewing the assignees annotation). Open the job with klick on \"Job #...\". Import datafix \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website. Workflow \u00b6 For each job, the project coordinator assigns the roles of the \"assignee\" (= annotator) and the \"reviewer\". The reviewer then starts working on a job by initial labeling of the video frames. The assignee should save his labeling work from time to time and can also stop and later repeat working on a job. After finishing the initial labeling, the assignee requests a review (menu - request review, the assigned reviewer should already be named there) and thus sets status of the job to \"validation\". Then the reviewer checks the initial labeling done by the assignee and raises issues when there is a problem or question. After reviewing all video frames of the job, the reviewer submits the review by either accepting and completing the job or rejecting it (which sets the status of the job bach to \"Annotation\"). Now the assignee has to check the issues, update the labels, resolve the issues and again request a review. The review process starts again and if necessary also next annotations, until the reviewer accepts the job and it is marked as \"Completed\". We recommend the following procedure for annotation in CVAT: Go through all pre-labeled objects on the right Check and delete false positive labels (if there is no object) Check and delete duplicate labels for the same object Check and correct object class for target classes Check and delete objects from other classes if they are classified wrong Zoom in on one quadrant of the image at a time: Check and correct position of object\u00b4s 2D boxes Check if there are objects from the target classes not already pre-labeled, draw new 2D box and annotate object class Download data \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"CVAT"},{"location":"OTLabels/training/cvat/#cvat","text":"CVAT is a free video and image annotation tool for computer vision. We recommend the developers guides for Installation as well as the user's guide . If you want to label your own dataset to retrain models, keep in mind that the format of your new labels needs to be the same as the format of the original dataset. On this page we define our intended organizational and technical annotation workflow using CVAT. Since YOLOv5 is based on the the MS COCO dataset of 2D bounding boxes, you need to apply your own labels in the frames from your sample videos in the same format. We therefore recommend the following steps: Install and setup an instance of CVAT either on your local computer or on a server (recommended when working in a team). Import the videos in CVAT and select the frames you want to use for labelling. Download the dataset from CVAT using the YOLO v1.1 format. Pre-annotate your frames with the standard YOLOv5x model. The results do probably not meet the desired quality. However, the pre-annotated labels will save you some time since it not necessary to draw all labels from scratch. Upload the pre-annotated frames to CVAT and revise the detected labels. Download the final labels from CVAT and convert them to COCO format (using our cvat_to_coco.py script). The COCO dataset and consequently the YOLOv5 models can detect 80 classes. However, for the detection of traffic, only six classes are relevant.","title":"CVAT"},{"location":"OTLabels/training/cvat/#target-classes","text":"Target classes are all road user classes from the initial COCO data set that are relevant for our sample videos: Person Bicycle Motorcycle Car Bus Truck All vehicles or combinations of vehicles are labeled as \"truck\" if they have twin tires (except for regular service buses or coaches), a superstructure or a loading area or a trailer (also cars with trailer, the 2D box includes vehicle and trailer). Here, as in the COCO dataset, both the \"person\" and the \"bicycle\" are labeled separately for bicyclists.","title":"Target classes"},{"location":"OTLabels/training/cvat/#object-dimensions","text":"The 2D box boundaries are defined by the outermost parts of the objects visible in that frame of the video. Even if only a part of an object is visible and the class of the object is recognizable for the annotator, it should be labeled.","title":"Object dimensions"},{"location":"OTLabels/training/cvat/#projects-tasks-and-jobs","text":"We define certain set of videos as a project . Every project has multiple tasks (each consists of one video to annotate). Each task has a status (\"Pending\", \"In Progress\" and \"Completed\") and a progress bar showing the number of completed jobs. Open the task with klick on \"Open\". For each task (or video) multiple jobs with a certain amounts of frames from the video are defined. Each job has a status (\"annotation\", \"validation\" and \"completed\"), an \"assignee\" (responsible for annotation) and a \"reviewer\" (responsible for reviewing the assignees annotation). Open the job with klick on \"Job #...\".","title":"Projects, Tasks and Jobs"},{"location":"OTLabels/training/cvat/#import-datafix","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Import datafix"},{"location":"OTLabels/training/cvat/#workflow","text":"For each job, the project coordinator assigns the roles of the \"assignee\" (= annotator) and the \"reviewer\". The reviewer then starts working on a job by initial labeling of the video frames. The assignee should save his labeling work from time to time and can also stop and later repeat working on a job. After finishing the initial labeling, the assignee requests a review (menu - request review, the assigned reviewer should already be named there) and thus sets status of the job to \"validation\". Then the reviewer checks the initial labeling done by the assignee and raises issues when there is a problem or question. After reviewing all video frames of the job, the reviewer submits the review by either accepting and completing the job or rejecting it (which sets the status of the job bach to \"Annotation\"). Now the assignee has to check the issues, update the labels, resolve the issues and again request a review. The review process starts again and if necessary also next annotations, until the reviewer accepts the job and it is marked as \"Completed\". We recommend the following procedure for annotation in CVAT: Go through all pre-labeled objects on the right Check and delete false positive labels (if there is no object) Check and delete duplicate labels for the same object Check and correct object class for target classes Check and delete objects from other classes if they are classified wrong Zoom in on one quadrant of the image at a time: Check and correct position of object\u00b4s 2D boxes Check if there are objects from the target classes not already pre-labeled, draw new 2D box and annotate object class","title":"Workflow"},{"location":"OTLabels/training/cvat/#download-data","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Download data"},{"location":"OTLabels/training/preprocessingdata/","text":"Preprocessing Data \u00b6 Before retraining the YOLOv5 model, you might want to preprocess your labeled data to ensure that your frames are in the right format. Convert CVAT output to YOLOv5 \u00b6 OTLabels provides the cvat_to_yolo.py script to convert the annotated data to the appropriate format YOLOv5 needs for retraining the model. Before executing the script, you have to setup the configurations for your needs: Within the script: Set destPath: path where your data is stored (typically ./data/*). Set annFile: path to the COCO-annotation file (if you do not have it yet, see below how to get it). Set catFile: path to a textfile containing your CVAT labels (standard: labels_CVAT.txt). Set cvatFile: path to a CVAT output file (containing images and lables, see also section CVAT : Download data) or to a folder containing multiple CVAT output files. Set name: the name of subfolder of destPath/images and destPath/labels to store the data in. labels_CVAT.txt: a textfile with two columns seperated by acutal commas with headers named Cat and CatId containing the name and the ID of your CVAT labels. For a example, see the labels_CVAT.txt file in the OTLabels repository. Please note that labels not provided in this file will not be converted and consequently be deleted . The script performs the following steps: Unzip the CVAT output file Copy the images to the directory destPath/images/name . Import the label files and convert the CVAT labels to the COCO labels used by YOLOv5. Export the converted label files to the directory destPath/labels/name . Filter the labels \u00b6 If for some reason you want to filter the labels that are already stored in your data directory, you can use the filter_labels.py script. Before executing the script, you have to setup the configurations for your needs: Within the script: Set path: path where the data is stored (typically ./data/*). Set name: the name of one or more subfolder of destPath/images and destPath/lables to store the data in. More than one name must be provided as list. Set annFile: path to the COCO-annotation file (if you do not have it yet, see below how to get it). Set labelsFilter: path to a text file (standard: label_filter.txt) containing the labels you want to keep. label_filter.txt: a textfile containing the category names (e.g., \"car\") of the labels you want to keep (without quotes and one category name per row). Please note that the images will not be copied to the corresponding folder in destPath/images in order to save file space. Instead, a textfile with all images still containing the filtered labels is created. This file of file names can also be referred to by YOLOv5. The script performs the following steps: Get the category ids to the corresponding category name. Import the label files. Filter the labels by the provided category names. Export the lables to the directory destPath/labels/name_filtered . Create a textfile with all image files in the directory path . Please note that images and label files not including any label after filtering are not exported . Get COCO annotation file \u00b6 This file is necessary to ensure your labels match the labels of the pretrained YOLOv5 models. Use the get_coco_annotation_files.py script to download one or more COCO annotation files. Configurations to be made: Within the script: Set path: path where the data is stored (typically ./data/*). Set URLFile: path to the config file that stores the URLs of the annotation files coco_annotation_json_URLs.txt: a textfile containing the URLS of the annotation files (without quotes and one URL per row) Get the original COCO dataset \u00b6 In some cases you might want to get the original COCO dataset that was used to train the original YOLOv5 models. Therefore, OTLabels provides the get_coco_data.py script to download the data. Please note that you could also use the get_coco.sh script from the YOLOv5 repository . Before executing the script, you have to setup the configurations for your needs: Within the script: Set imageURLs: path of a textfile (standard: coco_image_URLs.txt) containing the URLs of the image data sets. Set annURL: URL to the labels. Set destPath: path where your data is stored (typically ./data/*). coco_image_URLs.txt: a textfile containing the URLS of the images for training, validation and testing (without quotes and one URL per row).","title":"Preprocessing Data"},{"location":"OTLabels/training/preprocessingdata/#preprocessing-data","text":"Before retraining the YOLOv5 model, you might want to preprocess your labeled data to ensure that your frames are in the right format.","title":"Preprocessing Data"},{"location":"OTLabels/training/preprocessingdata/#convert-cvat-output-to-yolov5","text":"OTLabels provides the cvat_to_yolo.py script to convert the annotated data to the appropriate format YOLOv5 needs for retraining the model. Before executing the script, you have to setup the configurations for your needs: Within the script: Set destPath: path where your data is stored (typically ./data/*). Set annFile: path to the COCO-annotation file (if you do not have it yet, see below how to get it). Set catFile: path to a textfile containing your CVAT labels (standard: labels_CVAT.txt). Set cvatFile: path to a CVAT output file (containing images and lables, see also section CVAT : Download data) or to a folder containing multiple CVAT output files. Set name: the name of subfolder of destPath/images and destPath/labels to store the data in. labels_CVAT.txt: a textfile with two columns seperated by acutal commas with headers named Cat and CatId containing the name and the ID of your CVAT labels. For a example, see the labels_CVAT.txt file in the OTLabels repository. Please note that labels not provided in this file will not be converted and consequently be deleted . The script performs the following steps: Unzip the CVAT output file Copy the images to the directory destPath/images/name . Import the label files and convert the CVAT labels to the COCO labels used by YOLOv5. Export the converted label files to the directory destPath/labels/name .","title":"Convert CVAT output to YOLOv5"},{"location":"OTLabels/training/preprocessingdata/#filter-the-labels","text":"If for some reason you want to filter the labels that are already stored in your data directory, you can use the filter_labels.py script. Before executing the script, you have to setup the configurations for your needs: Within the script: Set path: path where the data is stored (typically ./data/*). Set name: the name of one or more subfolder of destPath/images and destPath/lables to store the data in. More than one name must be provided as list. Set annFile: path to the COCO-annotation file (if you do not have it yet, see below how to get it). Set labelsFilter: path to a text file (standard: label_filter.txt) containing the labels you want to keep. label_filter.txt: a textfile containing the category names (e.g., \"car\") of the labels you want to keep (without quotes and one category name per row). Please note that the images will not be copied to the corresponding folder in destPath/images in order to save file space. Instead, a textfile with all images still containing the filtered labels is created. This file of file names can also be referred to by YOLOv5. The script performs the following steps: Get the category ids to the corresponding category name. Import the label files. Filter the labels by the provided category names. Export the lables to the directory destPath/labels/name_filtered . Create a textfile with all image files in the directory path . Please note that images and label files not including any label after filtering are not exported .","title":"Filter the labels"},{"location":"OTLabels/training/preprocessingdata/#get-coco-annotation-file","text":"This file is necessary to ensure your labels match the labels of the pretrained YOLOv5 models. Use the get_coco_annotation_files.py script to download one or more COCO annotation files. Configurations to be made: Within the script: Set path: path where the data is stored (typically ./data/*). Set URLFile: path to the config file that stores the URLs of the annotation files coco_annotation_json_URLs.txt: a textfile containing the URLS of the annotation files (without quotes and one URL per row)","title":"Get COCO annotation file"},{"location":"OTLabels/training/preprocessingdata/#get-the-original-coco-dataset","text":"In some cases you might want to get the original COCO dataset that was used to train the original YOLOv5 models. Therefore, OTLabels provides the get_coco_data.py script to download the data. Please note that you could also use the get_coco.sh script from the YOLOv5 repository . Before executing the script, you have to setup the configurations for your needs: Within the script: Set imageURLs: path of a textfile (standard: coco_image_URLs.txt) containing the URLs of the image data sets. Set annURL: URL to the labels. Set destPath: path where your data is stored (typically ./data/*). coco_image_URLs.txt: a textfile containing the URLS of the images for training, validation and testing (without quotes and one URL per row).","title":"Get the original COCO dataset"},{"location":"OTLabels/training/retrainmodel/","text":"Retraining a Model \u00b6 Although the pretrained YOLOv5 models (especially the bigger ones like YOLOv5l or YOLOv5xl) return acceptable detection results, we experienced some shortcomings for certain object classes which are not well represented and/or distributed in the COCO dataset used for training the YOLOv5 models. Especially county specific differences were identified, since the COCO dataset contains mainly pictures taken in North-American areas (US trucks vs. European trucks). Also, less represented object classes (e.g., bicycles) might cause worse detection rate or even false detections and hence, to an uneven detection accuracy between the object classes. This might lead to biases in the final counts (e.g., every car is detected, but only every second motorcycle). Further, one might condense the 80 classes of the pretrained YOLOv5 models to the relevant six classes (pedestrian, bicycle, car, bus, truck, motorcycle) for detecting moving traffic to reduce noise from non-relevant classes. First step: setting up the config files \u00b6 For retraining, there are two relevant config files that needs to be set up. Data structure and labels \u00b6 YOLOv5 needs a config file in yaml format that contains information about file locations and the labels. This config file is usually stored in the data folder in the OTLabels directory. data_structure_labels .yaml \u00b6 # COCO 2017 dataset http://cocodataset.org # Train command: python train.py --data coco.yaml # Default dataset location is next to /yolov5: # /parent_folder # /coco # /yolov5 # download command/URL (optional) # download: bash data/scripts/get_coco.sh # train and val data as # 1) directory: path/images/, # 2) file: path/images.txt, or # 3) list: [path1/images/, path2/images/] train : ../OTLabels/data/path_to_structure_file_training.txt val : ../OTLabels/data/coco/path_to_structure_file_validation.txt test : ../OTLabels/data/coco/path_to_structure_file_test.txt # number of classes nc : 6 # class names names : [ \"person\" , \"bicycle\" , \"car\" , \"motorcycle\" , \"bus\" , \"truck\" ] Model structure \u00b6 Further, the config file containing the model structure (also in yaml format) needs to be set up or adapted. These config files are usually stored in the models folder within the yolov5 directory. Natively, YOLOv5 comes with one config file for each model (e.g., yolov5s.yaml ). We strongly recommend to keep the model structure itself as it is and only adapt the number of classes, since the retraining process bases on the trained standard weights, which rely on the original structure. model_structure .yaml \u00b6 # parameters nc : 6 # number of classes ... Second step: connecting to wandb \u00b6 If you want to have your trining process logged and visualized, YOLOv5 comes with the option to connect to weights and biases (wandb). For further instructions, please visit the wandb issue on Github . Otherwise you can chose the option not to use wandb when asked. Third step: retraining the model \u00b6 Now, you are ready to start with the retraining of your models. To start the process, simply execute the train.py with the desired options. python yolov5/train.py --img **img_size** --batch **batch_size** --epochs **n_epochs** --data path_to/**data_structure_labels**.yaml --weights yolov5/weights/**yolov5_weights** --cfg path_to/**model_structure**.yaml For further information about the whole retraining process with YOLOv5, please see the original documentation on Github .","title":"Retraining a Model"},{"location":"OTLabels/training/retrainmodel/#retraining-a-model","text":"Although the pretrained YOLOv5 models (especially the bigger ones like YOLOv5l or YOLOv5xl) return acceptable detection results, we experienced some shortcomings for certain object classes which are not well represented and/or distributed in the COCO dataset used for training the YOLOv5 models. Especially county specific differences were identified, since the COCO dataset contains mainly pictures taken in North-American areas (US trucks vs. European trucks). Also, less represented object classes (e.g., bicycles) might cause worse detection rate or even false detections and hence, to an uneven detection accuracy between the object classes. This might lead to biases in the final counts (e.g., every car is detected, but only every second motorcycle). Further, one might condense the 80 classes of the pretrained YOLOv5 models to the relevant six classes (pedestrian, bicycle, car, bus, truck, motorcycle) for detecting moving traffic to reduce noise from non-relevant classes.","title":"Retraining a Model"},{"location":"OTLabels/training/retrainmodel/#first-step-setting-up-the-config-files","text":"For retraining, there are two relevant config files that needs to be set up.","title":"First step: setting up the config files"},{"location":"OTLabels/training/retrainmodel/#data-structure-and-labels","text":"YOLOv5 needs a config file in yaml format that contains information about file locations and the labels. This config file is usually stored in the data folder in the OTLabels directory.","title":"Data structure and labels"},{"location":"OTLabels/training/retrainmodel/#model-structure","text":"Further, the config file containing the model structure (also in yaml format) needs to be set up or adapted. These config files are usually stored in the models folder within the yolov5 directory. Natively, YOLOv5 comes with one config file for each model (e.g., yolov5s.yaml ). We strongly recommend to keep the model structure itself as it is and only adapt the number of classes, since the retraining process bases on the trained standard weights, which rely on the original structure.","title":"Model structure"},{"location":"OTLabels/training/retrainmodel/#second-step-connecting-to-wandb","text":"If you want to have your trining process logged and visualized, YOLOv5 comes with the option to connect to weights and biases (wandb). For further instructions, please visit the wandb issue on Github . Otherwise you can chose the option not to use wandb when asked.","title":"Second step: connecting to wandb"},{"location":"OTLabels/training/retrainmodel/#third-step-retraining-the-model","text":"Now, you are ready to start with the retraining of your models. To start the process, simply execute the train.py with the desired options. python yolov5/train.py --img **img_size** --batch **batch_size** --epochs **n_epochs** --data path_to/**data_structure_labels**.yaml --weights yolov5/weights/**yolov5_weights** --cfg path_to/**model_structure**.yaml For further information about the whole retraining process with YOLOv5, please see the original documentation on Github .","title":"Third step: retraining the model"},{"location":"OTLabels/validation/gettingstarted/","text":"Getting Started \u00b6 This section provides a guide on how to install and use OTValidate. Installation \u00b6 Windows Linux/ macOS Intel Apple M1 Note Installation instructions for Windows are following soon. Install OTValidate by cloning the repository with git: git clone https://github.com/OpenTrafficCam/OTValidate.git cd OTValidate pip install -r requirements.txt pip install . Or install by using the Makefile : git clone https://github.com/OpenTrafficCam/OTValidate.git cd OTValidate make install The installation for machines running on Apple's M1 chip is not as straightforward. There are two ways to install OTValidate on an M1 Mac. As a prerequisite the package manager Homebrew is required. By executing these commands in the following order: git clone https://github.com/OpenTrafficCam/OTValidate.git cd OTValidate brew install openblas OPENBLAS = $( brew --prefix openblas ) CFLAGS = \"-falign-functions=8 ${ CFLAGS } \" pip install scipy == 1 .7.2 pip install -r requirements.txt pip install . By using the Makefile git clone https://github.com/OpenTrafficCam/OTValidate.git cd OTValidate make install_m1 Prerequisites \u00b6 Image Annotation Data \u00b6 The folder containing the ground truth annotations of the images need to be in the YOLO format: annotation_data \u2502 obj.data \u2502 obj.names \u2502 train.txt \u2502 \u2514\u2500\u2500\u2500obj_train_data \u2502 frame_01.png \u2502 frame_01.txt \u2502 frame_02.png \u2502 frame_02.txt \u2502 ... \u2502 Usage \u00b6 Analyse Object Detection Performance \u00b6 Quickstart Guide \u00b6 from OTValidate import evaluate_detection_performance # path to the directory containing the annotated dataset in otdet format gt_data = \"path/to/data1\" # model weights model1 = \"path/to/model_weights1.pt\" model2 = \"path/to/model_weights2.pt\" model3 = \"path/to/model_weights3.pt\" Use the evaluate_detection_performance function to calculate a set of object detection metrics of the respective models: evaluate_detection_performance ( path_to_model_weights = [ model1 , model2 , model3 ], yolo_path = yolo_path , otdet_gt_dir = gt_data , is_gt_xyxy_format = False , # whether the ground truth's bounding box is in xyxy or xywh format normalized = True , ) Results \u00b6 The evaluation results of the models will be saved in the directories containing the annotation data. An out directory containing all the results will be created there.","title":"Getting Started"},{"location":"OTLabels/validation/gettingstarted/#getting-started","text":"This section provides a guide on how to install and use OTValidate.","title":"Getting Started"},{"location":"OTLabels/validation/gettingstarted/#installation","text":"Windows Linux/ macOS Intel Apple M1 Note Installation instructions for Windows are following soon. Install OTValidate by cloning the repository with git: git clone https://github.com/OpenTrafficCam/OTValidate.git cd OTValidate pip install -r requirements.txt pip install . Or install by using the Makefile : git clone https://github.com/OpenTrafficCam/OTValidate.git cd OTValidate make install The installation for machines running on Apple's M1 chip is not as straightforward. There are two ways to install OTValidate on an M1 Mac. As a prerequisite the package manager Homebrew is required. By executing these commands in the following order: git clone https://github.com/OpenTrafficCam/OTValidate.git cd OTValidate brew install openblas OPENBLAS = $( brew --prefix openblas ) CFLAGS = \"-falign-functions=8 ${ CFLAGS } \" pip install scipy == 1 .7.2 pip install -r requirements.txt pip install . By using the Makefile git clone https://github.com/OpenTrafficCam/OTValidate.git cd OTValidate make install_m1","title":"Installation"},{"location":"OTLabels/validation/gettingstarted/#prerequisites","text":"","title":"Prerequisites"},{"location":"OTLabels/validation/gettingstarted/#image-annotation-data","text":"The folder containing the ground truth annotations of the images need to be in the YOLO format: annotation_data \u2502 obj.data \u2502 obj.names \u2502 train.txt \u2502 \u2514\u2500\u2500\u2500obj_train_data \u2502 frame_01.png \u2502 frame_01.txt \u2502 frame_02.png \u2502 frame_02.txt \u2502 ... \u2502","title":"Image Annotation Data"},{"location":"OTLabels/validation/gettingstarted/#usage","text":"","title":"Usage"},{"location":"OTLabels/validation/gettingstarted/#analyse-object-detection-performance","text":"","title":"Analyse Object Detection Performance"},{"location":"OTLabels/validation/metrics/","text":"Performance Metrics \u00b6 In this section we briefly go over the different metrics to evaluate our models on how they fare in the tasks of object detection and object tracking. Furthermore, we will look into the different metrics used to evaluate the results in terms of traffic analysis. Object Detection \u00b6 In the task of object detection a model is considered to be good if it is able to detect and classify an object correctly. In this section we are going to have a look in the different object detection performance metrics. Intersection Over Union (IOU) \u00b6 We can tell if a predicted bounding box matches a ground truth bounding box by calculating and looking at the IOU of the two bounding boxes. As Padilla et al. explained in the paper 1 , \"a perfect match is considered when the area and location of the predicted and ground-truth boxes are the same\". Therefore, the IOU is calculated by determining the area of the intersection of the two bounding boxes and dividing it by the area of the union of the two bounding boxes as shown in: Illustration adapted from the paper \"Analysis of Object Detection Metrics with a Companion Open-Source Toolkit\" Thus, two bounding boxes are considered a perfect match if the IOU = 1 . Meaning the predicted and ground truth bounding boxes share the same location and the same size. On the other hand, the IOU = 0 when there is no intersection between the predicted and the ground truth bounding box. Usually an IOU threshold is defined in order to decide whether a predicted and ground truth bounding box are considered a match. True Positives, False Positives, False Negatives \u00b6 This section will explain what true positives, false positives, false negatives and true negatives are in the task of object detection. Thus, we will look at their definitions as defined by Padilla et al. 1 : A True Positive is a correct detection of a ground-truth bounding box. An incorrect detection of a non-existing object or a misplaced detection of an existing object is a False Positive . An undetected ground-truth bounding box is named False Negative . Precision \u00b6 Padilla et al. 1 explain precision as \"the ability of a model to identify only relevant objects. It is the percentage of correct positive predictions.\" Precision is calculated as: \\[ Precision = \\frac{TP}{TP + FP} \\] Recall \u00b6 Padilla et al. 1 explain recall as \"the ability of a model to find all relevant cases (all ground-truth bounding boxes). It is the percentage of correct positive predictions among all given ground truths.\" Recall is calculated as: \\[ Recall = \\frac{TP}{TP + FN} \\] Average Precision (AP) \u00b6 As Padilla et al. explained that an object detection model \"can be considered good if, when the confidence threshold decreases, its precision remains high as its recall increases\"[ 1 ]. Taking this into account a model with a large area under a precision-recall curve indicates a high precision and a high recall. Therefore, the average precision \"is a metric based on the area under a [precision-recall curve]\" 1 with a selected IOU threshold. Thus the following notation for example, AP@50 denotes the average precision with IOU threshold at 50%. Mean Average Precision (mAP) \u00b6 We need to keep in mind that the average precision needs to be calculated for each class individually. Hence, the mean average precision \"is a metric that is able to represent the exactness of the detections among all classes\" 1 . The mAP is calculated as follows: \\[ mAP = \\frac{1}{C} \\sum_{i=1}^{C} AP_i \\] where C is the total number of classes and \\(AP_i\\) is the average precision of class \\(i\\) 1 . TIDE Metrics \u00b6 Bolya et al. created TIDE a General Toolbox for Identifying Object Detection Errors 2 . As Bolya et al. explain in their paper 2 \"mAP succinctly summarizes the performance of a model in one number\". Thus, the mAP performance metric does not give us any insight on what and how the different error types influence its score, that is the mAP score. The aim of TIDE is exactly that, to give us this insight on how the different error types affect the mAP score and as Bolya et al. 2 stated giving us \"a comprehensive analysis of each model's strengths and weaknesses\". TIDE defines six main error types as follows: Info The following descriptions of the error types are directly taken from the TIDE source code Classification Error : Error caused when a prediction would have been marked positive if it had the correct class. Localization Error : Error caused when a prediction would have been marked positive if it was localized better. Both Cls and Loc Error : This detection didn't fall into any of the other error categories. Duplicate Detection Error : Error caused when a prediction would have been marked positive if the GT wasn't already in use by another detection. Background Error : Error caused when this detection should have been classified as background (IoU < 0.1). Missed Ground Truth Error : Represents GT missed by the model. Doesn't include GT corrected elsewhere in the model. Confusion Matrix \u00b6 The confusion matrix gives us a visual insight on how our object detection model performs in the classification task. Let us have a look first at the confusion matrix of a binary classification problem : Illustration adapted from the paper \"Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation\" by David M. W. Powers[^3] The rows of the above confusion matrix represent the predicted class whereas the columns represent the ground truth class. Thus a prediction can be categorized as follows: A prediction that has been predicted as positive class and that is found to be an actual/real positive class in the ground truth, is counted as a true positive . A prediction that has been predicted as negative class and is found to be an actual/real negative class is counted as a true negative . A prediction that has been predicted as positive and is found to be not an actual/real positive class is counted as a false positive . A prediction that has been predicted as negative and is found to be not an actual/real negative class is counted as a false negative . Hence, the confusion matrix gives us a clear visualization of how many of our predictions were classified correctly or incorrectly. The confusion matrix of multi classification problem looks a little bit different: As the image above implies, we now have multiple classes. For this example we want to classify the class car , person and truck . The green color coded tiles denote the true positive predictions. Let's take for example the row denoted with the class car . Here is how it is to be interpreted: Out of the 8 cars that have been predicted: three were correctly classified as a car , none were incorrectly classified as a person and 5 were incorrectly classified as truck Object Tracking \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website. Traffic Measures \u00b6 To see, how well OpenTrafficCam performs see OTAnalytics . References \u00b6 Padilla, R., Passos, W. L., Dias, T. L., Netto, S. L., & da Silva, E. A. (2021). A comparative analysis of object detection metrics with a companion open-source toolkit. Electronics, 10(3), 279. https://doi.org/10.3390/electronics10030279 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 Bolya, D., Foley, S., Hays, J., & Hoffman, J. (2020). Tide: A general toolbox for identifying object detection errors. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part III 16 (pp. 558-573). Springer International Publishing. https://dbolya.github.io/tide/paper.pdf \u21a9 \u21a9 \u21a9 Powers, D. M. (2020). Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation. arXiv preprint arXiv:2010.16061. \u21a9","title":"Performance Metrics"},{"location":"OTLabels/validation/metrics/#performance-metrics","text":"In this section we briefly go over the different metrics to evaluate our models on how they fare in the tasks of object detection and object tracking. Furthermore, we will look into the different metrics used to evaluate the results in terms of traffic analysis.","title":"Performance Metrics"},{"location":"OTLabels/validation/metrics/#object-detection","text":"In the task of object detection a model is considered to be good if it is able to detect and classify an object correctly. In this section we are going to have a look in the different object detection performance metrics.","title":"Object Detection"},{"location":"OTLabels/validation/metrics/#intersection-over-union-iou","text":"We can tell if a predicted bounding box matches a ground truth bounding box by calculating and looking at the IOU of the two bounding boxes. As Padilla et al. explained in the paper 1 , \"a perfect match is considered when the area and location of the predicted and ground-truth boxes are the same\". Therefore, the IOU is calculated by determining the area of the intersection of the two bounding boxes and dividing it by the area of the union of the two bounding boxes as shown in: Illustration adapted from the paper \"Analysis of Object Detection Metrics with a Companion Open-Source Toolkit\" Thus, two bounding boxes are considered a perfect match if the IOU = 1 . Meaning the predicted and ground truth bounding boxes share the same location and the same size. On the other hand, the IOU = 0 when there is no intersection between the predicted and the ground truth bounding box. Usually an IOU threshold is defined in order to decide whether a predicted and ground truth bounding box are considered a match.","title":"Intersection Over Union (IOU)"},{"location":"OTLabels/validation/metrics/#true-positives-false-positives-false-negatives","text":"This section will explain what true positives, false positives, false negatives and true negatives are in the task of object detection. Thus, we will look at their definitions as defined by Padilla et al. 1 : A True Positive is a correct detection of a ground-truth bounding box. An incorrect detection of a non-existing object or a misplaced detection of an existing object is a False Positive . An undetected ground-truth bounding box is named False Negative .","title":"True Positives, False Positives, False Negatives"},{"location":"OTLabels/validation/metrics/#precision","text":"Padilla et al. 1 explain precision as \"the ability of a model to identify only relevant objects. It is the percentage of correct positive predictions.\" Precision is calculated as: \\[ Precision = \\frac{TP}{TP + FP} \\]","title":"Precision"},{"location":"OTLabels/validation/metrics/#recall","text":"Padilla et al. 1 explain recall as \"the ability of a model to find all relevant cases (all ground-truth bounding boxes). It is the percentage of correct positive predictions among all given ground truths.\" Recall is calculated as: \\[ Recall = \\frac{TP}{TP + FN} \\]","title":"Recall"},{"location":"OTLabels/validation/metrics/#average-precision-ap","text":"As Padilla et al. explained that an object detection model \"can be considered good if, when the confidence threshold decreases, its precision remains high as its recall increases\"[ 1 ]. Taking this into account a model with a large area under a precision-recall curve indicates a high precision and a high recall. Therefore, the average precision \"is a metric based on the area under a [precision-recall curve]\" 1 with a selected IOU threshold. Thus the following notation for example, AP@50 denotes the average precision with IOU threshold at 50%.","title":"Average Precision (AP)"},{"location":"OTLabels/validation/metrics/#mean-average-precision-map","text":"We need to keep in mind that the average precision needs to be calculated for each class individually. Hence, the mean average precision \"is a metric that is able to represent the exactness of the detections among all classes\" 1 . The mAP is calculated as follows: \\[ mAP = \\frac{1}{C} \\sum_{i=1}^{C} AP_i \\] where C is the total number of classes and \\(AP_i\\) is the average precision of class \\(i\\) 1 .","title":"Mean Average Precision (mAP)"},{"location":"OTLabels/validation/metrics/#tide-metrics","text":"Bolya et al. created TIDE a General Toolbox for Identifying Object Detection Errors 2 . As Bolya et al. explain in their paper 2 \"mAP succinctly summarizes the performance of a model in one number\". Thus, the mAP performance metric does not give us any insight on what and how the different error types influence its score, that is the mAP score. The aim of TIDE is exactly that, to give us this insight on how the different error types affect the mAP score and as Bolya et al. 2 stated giving us \"a comprehensive analysis of each model's strengths and weaknesses\". TIDE defines six main error types as follows: Info The following descriptions of the error types are directly taken from the TIDE source code Classification Error : Error caused when a prediction would have been marked positive if it had the correct class. Localization Error : Error caused when a prediction would have been marked positive if it was localized better. Both Cls and Loc Error : This detection didn't fall into any of the other error categories. Duplicate Detection Error : Error caused when a prediction would have been marked positive if the GT wasn't already in use by another detection. Background Error : Error caused when this detection should have been classified as background (IoU < 0.1). Missed Ground Truth Error : Represents GT missed by the model. Doesn't include GT corrected elsewhere in the model.","title":"TIDE Metrics"},{"location":"OTLabels/validation/metrics/#object-tracking","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Object Tracking"},{"location":"OTLabels/validation/metrics/#traffic-measures","text":"To see, how well OpenTrafficCam performs see OTAnalytics .","title":"Traffic Measures"},{"location":"OTLabels/validation/metrics/#references","text":"Padilla, R., Passos, W. L., Dias, T. L., Netto, S. L., & da Silva, E. A. (2021). A comparative analysis of object detection metrics with a companion open-source toolkit. Electronics, 10(3), 279. https://doi.org/10.3390/electronics10030279 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 Bolya, D., Foley, S., Hays, J., & Hoffman, J. (2020). Tide: A general toolbox for identifying object detection errors. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part III 16 (pp. 558-573). Springer International Publishing. https://dbolya.github.io/tide/paper.pdf \u21a9 \u21a9 \u21a9 Powers, D. M. (2020). Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation. arXiv preprint arXiv:2010.16061. \u21a9","title":"References"},{"location":"OTLabels/validation/modelvalidation/","text":"Model Validation \u00b6 In this section we will compare the different YOLO models on how good they fare in the object detection task. The YOLO models to be evaluated are YOLOv5s, YOLOv5m, YOLOv5l and YOLOv5x. The models are evaluated on a custom dataset consisting of custom video recordings. Thus, we want to investigate which of the four YOLO models might be best suited in the detection of traffic objects. Dataset \u00b6 As mentioned above, the dataset consists of different video recordings capturing different traffic scenes. Thus, the video recordings are turned into datasets that will be used to evaluate the object detection models. It is important to note that not every single frame is selected, but only every \\(n\\) -th frame where \\(n \\in \\mathbb{N}\\) and \\(n\\) is arbitrarily chosen by the user. Here is how the scenes are looking like: Scenes As seen above, there are two instances where scenes were also recorded at night. This makes it interesting to see how the YOLO models perform in detecting traffic objects in low light conditions. The class labels to be considered for our evaluation of the YOLO models are: person bicycle car motorcycle bus truck Evaluation Procedure \u00b6 The YOLO models are evaluated with the help of OTValidate . As a prerequisite, what OTValidate needs in order to start the evaluation are the YOLO models and the labeled ground truth data. In our case the ground truth labeled data is our custom dataset, which needs to be in the CVAT YOLO format. As for the YOLO model, a custom trained or an existing model can be loaded into OTValidate. OTValidate then uses each model to predict the ground truth images. Afterwards, the prediction results and ground truth data are used to calculate the different object detection metrics for model comparison. In our case, we will use the mAP and the TIDE metrics . Info OTValidate takes a list of class labels as a parameter. All predictions and ground truth data are then filtered according to that list of class labels. Meaning, all predictions or ground truth data, whose predicted or labeled class are not contained in the list, are discarded and therefore not regarded in the evaluation process. This is especially useful if the ground truth data contains class labels that the model can't predict. Evaluation \u00b6 In this section we will discuss the evaluation results of each model and put them under comparison. Specifically, we will evaluate the models on each dataset depicted by the scenes as shown above in the table and on all the ground truth data. The models confidence and IOU threshold are set to 0.25 and 0.5 respectively. Meaning, all detections that have an IOU lower than 0.5 are not regarded as possible detections and all detections with a confidence lower than 0.25 are discarded. Evaluation on all data \u00b6 This diagram depicts each models mAP at different IOU thresholds: We can clearly see that the YOLOv5s model's mAP is the lowest compared to the other models. Meaning, by only looking at the mAP metric the YOLOv5s, YOLOv5m, and the YOLOv5x models are to be considered. Nevertheless, let us also have a look at the TIDE metrics to get an insight on the types of errors made by the models: TIDE Metrics What immediately stands out are the Missed Ground Truth Error (Miss) and the Classification Error (Cls) . Meaning, the models were not able to detect many ground truth bounding boxes or be able to classify them correctly. Thus, the models were able to detect the majority of the bounding boxes, but had problems predicting the correct classes. The YOLOv5s model had the highest classification error rate out of the four models. Although, the missed ground truth error rate is also the highest, it does not differ much from the other three models. It becomes apparent that the bigger models perform better at detecting bounding boxes than the smaller ones. But there is also a point where the models' performance, namely the YOLOv5m, YOLOv5l and YOLOv5x, don't differ much at all. Implying that there is not much of a trade-off in choosing the YOLOv5m or YOLOv5l over the YOLOv5x. Still, let us try to find out why the Missed Ground Truth Error appears to be a problem for all four models by evaluating them on the data of each scene. Evaluation on each dataset of a scene \u00b6 Scene 1 & 2 \u00b6 Scene 1 Scene 2 Scene 1 captures a three way junction. Compared to the combined dataset, the models' mAP value doesn't differ that much. The missed ground truth error of all four models is above 20%. Our assumption as to why the missed ground truth error is so high might be due to moving objects appearing in the distance. The models might not be able to detect these objects as they appear to be very small in the picture, as marked in red bounding boxes in the following image: It is also possible that the timestamp covers up parts of objects making the models not be able to detect them. Scene 2 is similar to scene 1 in respect to the metrics calculated on all four models. The models' mAP evaluated on the dataset of scene 2 is higher than those of scene 1 when looking at the YOLOv5's m , l and x models. Scene 2's TIDE errors behave also very similar to those of scene 1. Also, small objects that have been annotated with small bounding boxes appear in images of the dataset of Scene 2. This could also be related to the high Missed Ground Truth Error . Scene 3 \u00b6 Scene 3 captures a four-way junction. There are two datasets capturing scene 2 at daylight and nighttime respectively. Scene 3 At Night Comparing these two datasets, it is not surprising that the models' mAP evaluated on the dataset taken at daylight fares much better to the one evaluated at nighttime. The low light condition makes it really hard for the models to detect any of the objects. The biggest drop in performance in terms of the mAP metric is seen from the YOLOv5s model. But then, why is the Missed Ground Truth Error (Miss) much lower and the Classification Error (Cls) that much higher of the nighttime dataset of Scene 3 compared to the one taken at daylight? Let's have a closer look at the two datasets first: Scene 3 at daylight Scene 3 at nighttime Overall, the amount of ground truth bounding boxes in scene 3 taken at daylight is more than twice as big as the one taken at nighttime. This implies that there is less traffic at nighttime, which makes sense. Thus, there is not much to detect at night resulting in a lower Missed Ground Truth Error . On the other hand, the classification error for all models is above 40% with the exception of the YOLOv5l model whose classification error is at approximately 24%. In addition to that the Localization Error (Loc) is very low. Meaning, the model is able to detect objects very well, but has trouble assigning them to the correct class in badly lit areas. Here is an example where a car is barely recognizable: Speaking of scene 3 at daylight, the MissedGround Truth Error is relatively high across all models which is due to the same reason as explained in Scene 1 . Traffic objects whose bounding boxes are really small might be detected by the models resulting in misses. But it is also important to take into account that there is the possibility of not moving objects appearing in every image at the same spot in the dataset. Such case can be seen in scene 3 at nighttime where cars have been parked on the sidewalk in the following image: Thus, such case could influence the values of the calculated metrics for better or worse. Scene 4 \u00b6 Scene 4 captures a rural road. Scene 4 The Missed Ground Truth Error (Miss) is pretty high for the YOLOv5s and the YOLOv5x models with an error value over 50% compared to the YOLOv5m and YOLOv5l models. At this moment, we could not come up with an explanation of why the YOLOv5x model's missed ground truth error is that much higher than its m and l counterpart. But there is an explanation why the missed ground truth error is high across all models. The reason lies in the dataset of scene 4 itself: Scene 4 - images with cut off objects The images above contain objects, which are surrounded by red bounding boxes, that are only partly in their respective image. Therefore, a model might not have been able to detect these objects due to a low confidence score and thus resulting in misses. These type of images appear often in the dataset. Another thing to keep in mind is that there is not much traffic on this rural road. Most images in the dataset do not contain bounding box annotations. There are a total of 110 labeled objects in a dataset consisting of 401 images. As a result, even a small number of cut off objects could have a great impact on the metrics. Scene 5 \u00b6 Scene 5 captures a rural road. Scene 5 At Night Scene 5 is captured at two different times, namely at day and night. The only difference to scene 3 is that scene 5 captures a rural road instead of a four way junction. Unsurprisingly, the models performed tremendously better on the dataset taken during the day compared to the one taken at night when looking at the mAP diagram. It is not surprising that all models evaluated on the nighttime dataset have a high Missed Ground Truth Error (Miss) which is due to the worse light conditions. What is standing out is the fact that the models evaluated on the day dataset also have a high miss error rate. To get behind the possible reason for this we should have quick peek in the ground truth dataset: Objects or vehicles emerging from the road in the upper left corner of the image make it harder for the models to detect and assign them to the correct class. This is reflected in the missed ground truth error and the localization error. Thus the further the objects move to the right area of the image, the better they are detected by the models. Conclusion and Future Work \u00b6 We have evaluated the YOLOv5s , YOLOv5m , YOLOv5x , and the YOLOv5l object detection models on a custom dataset consisting of videos capturing different scenes of traffic junctions and roads. Looking at mAP and TIDE metrics gave us insight to how the YOLOv5 models performed on our custom dataset and we can come to the conclusion that a YOLOv5m , YOLOv5l and YOLOv5x models performed far better than the YOLOv5s . But there is not a big increase in performance in terms of object detection upon choosing the YOLOv5x over the YOLOv5m or YOLOv5l model. Another important aspect to look into in the future is how much time the models under discussion take to detect the images. Depending on the use case and resources at hand, choosing the YOLOv5x model which might need much more time to finish the detection might not be suitable and thus taking the YOLOv5m or YOLOv5l model might be the better choice. What we are also currently working on is to train our own models using the YOLOv5 models as our foundation on a custom dataset. Thus it would be interesting to see if there would be a significant increase in performance by using custom trained models. If there is a significant increase in performance, the next question to ask if it makes sense to invest time in training a custom model.","title":"Model Validation"},{"location":"OTLabels/validation/modelvalidation/#model-validation","text":"In this section we will compare the different YOLO models on how good they fare in the object detection task. The YOLO models to be evaluated are YOLOv5s, YOLOv5m, YOLOv5l and YOLOv5x. The models are evaluated on a custom dataset consisting of custom video recordings. Thus, we want to investigate which of the four YOLO models might be best suited in the detection of traffic objects.","title":"Model Validation"},{"location":"OTLabels/validation/modelvalidation/#dataset","text":"As mentioned above, the dataset consists of different video recordings capturing different traffic scenes. Thus, the video recordings are turned into datasets that will be used to evaluate the object detection models. It is important to note that not every single frame is selected, but only every \\(n\\) -th frame where \\(n \\in \\mathbb{N}\\) and \\(n\\) is arbitrarily chosen by the user. Here is how the scenes are looking like: Scenes As seen above, there are two instances where scenes were also recorded at night. This makes it interesting to see how the YOLO models perform in detecting traffic objects in low light conditions. The class labels to be considered for our evaluation of the YOLO models are: person bicycle car motorcycle bus truck","title":"Dataset"},{"location":"OTLabels/validation/modelvalidation/#evaluation-procedure","text":"The YOLO models are evaluated with the help of OTValidate . As a prerequisite, what OTValidate needs in order to start the evaluation are the YOLO models and the labeled ground truth data. In our case the ground truth labeled data is our custom dataset, which needs to be in the CVAT YOLO format. As for the YOLO model, a custom trained or an existing model can be loaded into OTValidate. OTValidate then uses each model to predict the ground truth images. Afterwards, the prediction results and ground truth data are used to calculate the different object detection metrics for model comparison. In our case, we will use the mAP and the TIDE metrics . Info OTValidate takes a list of class labels as a parameter. All predictions and ground truth data are then filtered according to that list of class labels. Meaning, all predictions or ground truth data, whose predicted or labeled class are not contained in the list, are discarded and therefore not regarded in the evaluation process. This is especially useful if the ground truth data contains class labels that the model can't predict.","title":"Evaluation Procedure"},{"location":"OTLabels/validation/modelvalidation/#evaluation","text":"In this section we will discuss the evaluation results of each model and put them under comparison. Specifically, we will evaluate the models on each dataset depicted by the scenes as shown above in the table and on all the ground truth data. The models confidence and IOU threshold are set to 0.25 and 0.5 respectively. Meaning, all detections that have an IOU lower than 0.5 are not regarded as possible detections and all detections with a confidence lower than 0.25 are discarded.","title":"Evaluation"},{"location":"OTLabels/validation/modelvalidation/#evaluation-on-all-data","text":"This diagram depicts each models mAP at different IOU thresholds: We can clearly see that the YOLOv5s model's mAP is the lowest compared to the other models. Meaning, by only looking at the mAP metric the YOLOv5s, YOLOv5m, and the YOLOv5x models are to be considered. Nevertheless, let us also have a look at the TIDE metrics to get an insight on the types of errors made by the models: TIDE Metrics What immediately stands out are the Missed Ground Truth Error (Miss) and the Classification Error (Cls) . Meaning, the models were not able to detect many ground truth bounding boxes or be able to classify them correctly. Thus, the models were able to detect the majority of the bounding boxes, but had problems predicting the correct classes. The YOLOv5s model had the highest classification error rate out of the four models. Although, the missed ground truth error rate is also the highest, it does not differ much from the other three models. It becomes apparent that the bigger models perform better at detecting bounding boxes than the smaller ones. But there is also a point where the models' performance, namely the YOLOv5m, YOLOv5l and YOLOv5x, don't differ much at all. Implying that there is not much of a trade-off in choosing the YOLOv5m or YOLOv5l over the YOLOv5x. Still, let us try to find out why the Missed Ground Truth Error appears to be a problem for all four models by evaluating them on the data of each scene.","title":"Evaluation on all data"},{"location":"OTLabels/validation/modelvalidation/#evaluation-on-each-dataset-of-a-scene","text":"","title":"Evaluation on each dataset of a scene"},{"location":"OTLabels/validation/modelvalidation/#conclusion-and-future-work","text":"We have evaluated the YOLOv5s , YOLOv5m , YOLOv5x , and the YOLOv5l object detection models on a custom dataset consisting of videos capturing different scenes of traffic junctions and roads. Looking at mAP and TIDE metrics gave us insight to how the YOLOv5 models performed on our custom dataset and we can come to the conclusion that a YOLOv5m , YOLOv5l and YOLOv5x models performed far better than the YOLOv5s . But there is not a big increase in performance in terms of object detection upon choosing the YOLOv5x over the YOLOv5m or YOLOv5l model. Another important aspect to look into in the future is how much time the models under discussion take to detect the images. Depending on the use case and resources at hand, choosing the YOLOv5x model which might need much more time to finish the detection might not be suitable and thus taking the YOLOv5m or YOLOv5l model might be the better choice. What we are also currently working on is to train our own models using the YOLOv5 models as our foundation on a custom dataset. Thus it would be interesting to see if there would be a significant increase in performance by using custom trained models. If there is a significant increase in performance, the next question to ask if it makes sense to invest time in training a custom model.","title":"Conclusion and Future Work"},{"location":"OTLabels/validation/overview/","text":"Overview \u00b6 Validation gives insight on how well a model or software performs. In the case OpenTrafficCam , we want to evaluate and compare different object detection models on how well they perform in object detection and object tracking. Furthermore, we want to analyse the results of OTAnalytics on a set of traffic performance metrics. For this purpose we are currently developing OTValidate which allows the user to analyse and compare the models' performances on object detection and tracking by calculating metrics corresponding to the specific task at hand. As mentioned above OTValidate will also provide tools to analyse the results of OTAnalytics in regards to traffic performance. OTValidate \u00b6 The image below gives a good overview on the structure of OTValidate : For the object detection task OTValidate needs two input files namely an .otdet file and the ground truth label data for the object detection task. Alternatively, a custom or an existing model can be given over as an input instead of an .otdet file.","title":"Overview"},{"location":"OTLabels/validation/overview/#overview","text":"Validation gives insight on how well a model or software performs. In the case OpenTrafficCam , we want to evaluate and compare different object detection models on how well they perform in object detection and object tracking. Furthermore, we want to analyse the results of OTAnalytics on a set of traffic performance metrics. For this purpose we are currently developing OTValidate which allows the user to analyse and compare the models' performances on object detection and tracking by calculating metrics corresponding to the specific task at hand. As mentioned above OTValidate will also provide tools to analyse the results of OTAnalytics in regards to traffic performance.","title":"Overview"},{"location":"OTLabels/validation/overview/#otvalidate","text":"The image below gives a good overview on the structure of OTValidate : For the object detection task OTValidate needs two input files namely an .otdet file and the ground truth label data for the object detection task. Alternatively, a custom or an existing model can be given over as an input instead of an .otdet file.","title":"OTValidate"},{"location":"OTVision/","text":"Features \u00b6 The heart of OpenTrafficCam. OTVision , is a collection of algorithms to generate trajectories of road users in traffic videos. Most of the algorithms have been developed in previous open source projects. We modify and link them to a working pipeline. Key features \u00b6 Automated video analysis Detection using neural networks Can be used without programming knowledge Result: Trajectories in image and world coordinates Necessary: Windows computer with powerful graphics card (approx. 2.500 \u20ac) Content of this documentation \u00b6 Conversion of h264 video files to other formats ( ffmpeg ) Detection (joint localization and classification) of road users in single frames ( YOLOv5 by Jocher/Ultralytics ) Tracking of detected road users between single frames ( IOU-Tracker by Bochinski et al. ) Correction of trajectories for lens distortion ( OpenCV-Library ) Transformation of trajectories to world coordinates ( OpenCV-Library )","title":"Features"},{"location":"OTVision/#features","text":"The heart of OpenTrafficCam. OTVision , is a collection of algorithms to generate trajectories of road users in traffic videos. Most of the algorithms have been developed in previous open source projects. We modify and link them to a working pipeline.","title":"Features"},{"location":"OTVision/#key-features","text":"Automated video analysis Detection using neural networks Can be used without programming knowledge Result: Trajectories in image and world coordinates Necessary: Windows computer with powerful graphics card (approx. 2.500 \u20ac)","title":"Key features"},{"location":"OTVision/#content-of-this-documentation","text":"Conversion of h264 video files to other formats ( ffmpeg ) Detection (joint localization and classification) of road users in single frames ( YOLOv5 by Jocher/Ultralytics ) Tracking of detected road users between single frames ( IOU-Tracker by Bochinski et al. ) Correction of trajectories for lens distortion ( OpenCV-Library ) Transformation of trajectories to world coordinates ( OpenCV-Library )","title":"Content of this documentation"},{"location":"OTVision/gettingstarted/firstuse/","text":"First Use \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"First Use"},{"location":"OTVision/gettingstarted/firstuse/#first-use","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"First Use"},{"location":"OTVision/gettingstarted/installation/","text":"Installation \u00b6 Install Python 3.9 \u00b6 If not done yet, install the latest 64-bit version of Python 3.9 via Windows installer from www.python.org/downloads/ as follows (Python 3.8.x should also work, the 32-bit version is not supported): What if I already have another Python version installed? In addition, also install Python 3.9. The last installed Python will automatically be the default Python interpreter of your system. On Windows, it is also possible to change the default Python interpreter by changing the order of the system-wide environment variables (move Python39 and Python39\\Scripts to the top, see animation below). This is necessary e.g. if you have already installed Python 3.9, but another Python version is your default because you installed it in the meantime (e.g. 3.10). Install NVIDIA Cuda 11.0 \u00b6 If you have a modern NVidia graphics card. If you intend to use OTVision on a PC with a modern NVidia graphics card, download and install the latest version of the NVIDIA Cuda Toolkit with default settings from the NVIDIA toolkit. Install OTVision \u00b6 Download and unzip the latest version of OTVision from Github. In the unzipped folder double-click the \"install_win-py39.bat\" (or \"install_win-py38.bat\" respectively) and wait until the installation of the dependencies is complete. To run OTVision \u00b6 ... double click the \"OTVision.bat\" to run the Software via graphical user interface. If you encounter problems \u00b6 Maybe you also have to manually install Microsoft Visual C++ 14.0 or greater from the Visual Studio Build Tools . In case of further problems please open an issue in the OTVision repository on Github. We also welcome code contributions (e.g. fixing bugs or adding features) from other programmers by forking the repository and creating a pull request. Please check the contribute section of this documentation first.","title":"Installation"},{"location":"OTVision/gettingstarted/installation/#installation","text":"","title":"Installation"},{"location":"OTVision/gettingstarted/installation/#install-python-39","text":"If not done yet, install the latest 64-bit version of Python 3.9 via Windows installer from www.python.org/downloads/ as follows (Python 3.8.x should also work, the 32-bit version is not supported): What if I already have another Python version installed? In addition, also install Python 3.9. The last installed Python will automatically be the default Python interpreter of your system. On Windows, it is also possible to change the default Python interpreter by changing the order of the system-wide environment variables (move Python39 and Python39\\Scripts to the top, see animation below). This is necessary e.g. if you have already installed Python 3.9, but another Python version is your default because you installed it in the meantime (e.g. 3.10).","title":"Install Python 3.9"},{"location":"OTVision/gettingstarted/installation/#install-nvidia-cuda-110","text":"If you have a modern NVidia graphics card. If you intend to use OTVision on a PC with a modern NVidia graphics card, download and install the latest version of the NVIDIA Cuda Toolkit with default settings from the NVIDIA toolkit.","title":"Install NVIDIA Cuda 11.0"},{"location":"OTVision/gettingstarted/installation/#install-otvision","text":"Download and unzip the latest version of OTVision from Github. In the unzipped folder double-click the \"install_win-py39.bat\" (or \"install_win-py38.bat\" respectively) and wait until the installation of the dependencies is complete.","title":"Install OTVision"},{"location":"OTVision/gettingstarted/installation/#to-run-otvision","text":"... double click the \"OTVision.bat\" to run the Software via graphical user interface.","title":"To run OTVision"},{"location":"OTVision/gettingstarted/installation/#if-you-encounter-problems","text":"Maybe you also have to manually install Microsoft Visual C++ 14.0 or greater from the Visual Studio Build Tools . In case of further problems please open an issue in the OTVision repository on Github. We also welcome code contributions (e.g. fixing bugs or adding features) from other programmers by forking the repository and creating a pull request. Please check the contribute section of this documentation first.","title":"If you encounter problems"},{"location":"OTVision/gettingstarted/requirements/","text":"Requirements \u00b6 Hardware prerequisites \u00b6 Both OTVision and OTAnalytics run on modern 64 bit Windows 10 desktops and laptops (e.g. i5 processor and 8 GB RAM). However, if you want to detect road users in videos with OTVision on a regular basis, we strongly recommend purchasing a powerful desktop computer (>= i7, >= 64 GB RAM) with a modern NVidia graphics card (>= GeForce 3070). Make sure that the drivers of the graphics card are installed.","title":"Requirements"},{"location":"OTVision/gettingstarted/requirements/#requirements","text":"","title":"Requirements"},{"location":"OTVision/gettingstarted/requirements/#hardware-prerequisites","text":"Both OTVision and OTAnalytics run on modern 64 bit Windows 10 desktops and laptops (e.g. i5 processor and 8 GB RAM). However, if you want to detect road users in videos with OTVision on a regular basis, we strongly recommend purchasing a powerful desktop computer (>= i7, >= 64 GB RAM) with a modern NVidia graphics card (>= GeForce 3070). Make sure that the drivers of the graphics card are installed.","title":"Hardware prerequisites"},{"location":"OTVision/objectdetection/convert/","text":"Convert \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Convert"},{"location":"OTVision/objectdetection/convert/#convert","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Convert"},{"location":"OTVision/objectdetection/detect/","text":"Detect \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Detect"},{"location":"OTVision/objectdetection/detect/#detect","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Detect"},{"location":"OTVision/tracking/track/","text":"Track \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Track"},{"location":"OTVision/tracking/track/#track","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Track"},{"location":"OTVision/tracking/transform/","text":"Transform \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Transform"},{"location":"OTVision/tracking/transform/#transform","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Transform"},{"location":"OTVision/tracking/undistort/","text":"Undistort \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Undistort"},{"location":"OTVision/tracking/undistort/#undistort","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Undistort"},{"location":"contribute/","text":"Getting Started \u00b6 Become a part of Open Traffic Cam We are happy if you contribute your own code to OpenTrafficCam. This can be bug fixes in the current code, or even completely new stuff. It is best to start at GitHub to get a first overview. We are also happy to receive reports about the experience using OpenTrafficCam. If you find a bug in the software or the documentation, please report it as an issue in the corresponding repository on GitHub or via email. If you have any questions, feel free to send us an email .","title":"Getting Started"},{"location":"contribute/#getting-started","text":"Become a part of Open Traffic Cam We are happy if you contribute your own code to OpenTrafficCam. This can be bug fixes in the current code, or even completely new stuff. It is best to start at GitHub to get a first overview. We are also happy to receive reports about the experience using OpenTrafficCam. If you find a bug in the software or the documentation, please report it as an issue in the corresponding repository on GitHub or via email. If you have any questions, feel free to send us an email .","title":"Getting Started"},{"location":"contribute/coding/","text":"Coding (Python) \u00b6 Coding standards are not about right and wrong or good and bad. They are about uniformity, so people can easily read and contribute to the code regardless who worked on the software before. For now, nearly all code is Python so we just agreed on coding conventions for the Python programming language. We basically follow the PEP 8 Style Guide for Python Code . A good summary and some additions can be found in the Hitchhikers Guide or at RealPython.com . Below we defined what we consider the most important standards. Naming Conventions \u00b6 All names should be as short as possible but as long as necessary to understand them. General \u00b6 The following table from RealPython.com summarizes the PEP8 naming conventions: Type Naming Convention Examples Function Use a lowercase word or words. Separate words by underscores to improve readability. function, my_function Variable Use a lowercase single letter, word, or words. Separate words with underscores to improve readability. x, var, my_variable Class Start each word with a capital letter. Do not separate words with underscores. This style is called camel case. Model, MyClass Method Use a lowercase word or words. Separate words with underscores to improve readability. class_method, method Constant Use an uppercase single letter, word, or words. Separate words with underscores to improve readability. CONSTANT, MY_CONSTANT, MY_LONG_CONSTANT Module Use a short, lowercase word or words. Separate words with underscores to improve readability. module.py, my_module.py Package Use a short, lowercase word or words. Do not separate words with underscores. package, mypackage Files, Folder, Dirs \u00b6 Naming Convention Examples dir D:\\tmp\\ filename testfile suffix _detections filetype .csv file D:\\tmp\\testfile_detections.csv path \"D:\\tmp\\\" or \"D:\\tmp\\testfile_detections.csv\" Suffix \u00b6 Files get a suffix according to their content. Suffix Description _detections detections as bounding boxes _tracks-px trajectories in pixel coordinates _tracks-corr trajectories in pixel coordinates corrected for lens distortion _tracks-utm trajectories transformed to UTM coordinates _refpts reference points to convert pixel to UTM coordinates Code documentation \u00b6 Each module, function, class or method should be described in a docstring ( Google style ) Docstrings for Modules \u00b6 Each file should start with the license snippet followed by a docstring describing the contents and usage of the module: \"\"\"A one line summary of the module or program, terminated by a period. Leave one blank line. The rest of this docstring should contain an overall description of the module or program. Optionally, it may also contain a brief description of exported classes and functions and/or usage examples. Typical usage example: foo = ClassFoo() bar = foo.FunctionBar() \"\"\" Docstrings for Functions \u00b6 Each function must be described by a docstring: def hello_world ( message : str = \"Hello World\" , author : str = \"Santa Claus\" ): \"\"\"Deliver a message from an author to the world. \"author says message\" Args: message (str, optional): Message to deliver. Defaults to \"Hello World\". author (str, optional): Author name. Defaults to \"Santa Claus\". Returns: str: the message said \"\"\" msg = author + \" says \" + message print ( msg ) return msg Note If you are using VS Code, you may want to use the Python Docstring Generator extension. Comments \u00b6 If it is necessary to explain your code in between, use single line or block comments for some (or all) code that follows: # This is an example for a single line comment # This is an # example for a # block comment Try to avoid inline comments. Dependencies \u00b6 We try to develop OpenTrafficCam using as few dependencies as possible. For basic functionality we prefer packages that come with the Python standard distribution (like pathlib). However, OpenTrafficCam would not be possible without code from third party libraries, especially when it comes to videos, images, data analysis, neural networks or GUI. These are the libraries we intend to use for specific functionalities throughout the whole framework (we are grateful to the authors): Functionality Library Paths pathlib (distributed with Python since 3.4) Videos, Images OpenCV Neural networks PyTorch , torchvision Data handling pandas , NumPy Shape handling GeoPandas , Shapely Plotting, graphing Plotly User interface PySimpleGui Web PySimpleGUIWeb (camera system), Dash (dashboards) Linting/Autoformatting \u00b6 To match PEP8 specs and write pretty code we use flake8 to check against structural and style errors and black for autoformatting. Additionally the package flake8-docstrings can be used to lint the docstrings. The following settings are set in the .flake8 file: [flake8] max-line-length = 88 docstring-convention = google","title":"Coding (Python)"},{"location":"contribute/coding/#coding-python","text":"Coding standards are not about right and wrong or good and bad. They are about uniformity, so people can easily read and contribute to the code regardless who worked on the software before. For now, nearly all code is Python so we just agreed on coding conventions for the Python programming language. We basically follow the PEP 8 Style Guide for Python Code . A good summary and some additions can be found in the Hitchhikers Guide or at RealPython.com . Below we defined what we consider the most important standards.","title":"Coding (Python)"},{"location":"contribute/coding/#naming-conventions","text":"All names should be as short as possible but as long as necessary to understand them.","title":"Naming Conventions"},{"location":"contribute/coding/#general","text":"The following table from RealPython.com summarizes the PEP8 naming conventions: Type Naming Convention Examples Function Use a lowercase word or words. Separate words by underscores to improve readability. function, my_function Variable Use a lowercase single letter, word, or words. Separate words with underscores to improve readability. x, var, my_variable Class Start each word with a capital letter. Do not separate words with underscores. This style is called camel case. Model, MyClass Method Use a lowercase word or words. Separate words with underscores to improve readability. class_method, method Constant Use an uppercase single letter, word, or words. Separate words with underscores to improve readability. CONSTANT, MY_CONSTANT, MY_LONG_CONSTANT Module Use a short, lowercase word or words. Separate words with underscores to improve readability. module.py, my_module.py Package Use a short, lowercase word or words. Do not separate words with underscores. package, mypackage","title":"General"},{"location":"contribute/coding/#files-folder-dirs","text":"Naming Convention Examples dir D:\\tmp\\ filename testfile suffix _detections filetype .csv file D:\\tmp\\testfile_detections.csv path \"D:\\tmp\\\" or \"D:\\tmp\\testfile_detections.csv\"","title":"Files, Folder, Dirs"},{"location":"contribute/coding/#suffix","text":"Files get a suffix according to their content. Suffix Description _detections detections as bounding boxes _tracks-px trajectories in pixel coordinates _tracks-corr trajectories in pixel coordinates corrected for lens distortion _tracks-utm trajectories transformed to UTM coordinates _refpts reference points to convert pixel to UTM coordinates","title":"Suffix"},{"location":"contribute/coding/#code-documentation","text":"Each module, function, class or method should be described in a docstring ( Google style )","title":"Code documentation"},{"location":"contribute/coding/#docstrings-for-modules","text":"Each file should start with the license snippet followed by a docstring describing the contents and usage of the module: \"\"\"A one line summary of the module or program, terminated by a period. Leave one blank line. The rest of this docstring should contain an overall description of the module or program. Optionally, it may also contain a brief description of exported classes and functions and/or usage examples. Typical usage example: foo = ClassFoo() bar = foo.FunctionBar() \"\"\"","title":"Docstrings for Modules"},{"location":"contribute/coding/#docstrings-for-functions","text":"Each function must be described by a docstring: def hello_world ( message : str = \"Hello World\" , author : str = \"Santa Claus\" ): \"\"\"Deliver a message from an author to the world. \"author says message\" Args: message (str, optional): Message to deliver. Defaults to \"Hello World\". author (str, optional): Author name. Defaults to \"Santa Claus\". Returns: str: the message said \"\"\" msg = author + \" says \" + message print ( msg ) return msg Note If you are using VS Code, you may want to use the Python Docstring Generator extension.","title":"Docstrings for Functions"},{"location":"contribute/coding/#comments","text":"If it is necessary to explain your code in between, use single line or block comments for some (or all) code that follows: # This is an example for a single line comment # This is an # example for a # block comment Try to avoid inline comments.","title":"Comments"},{"location":"contribute/coding/#dependencies","text":"We try to develop OpenTrafficCam using as few dependencies as possible. For basic functionality we prefer packages that come with the Python standard distribution (like pathlib). However, OpenTrafficCam would not be possible without code from third party libraries, especially when it comes to videos, images, data analysis, neural networks or GUI. These are the libraries we intend to use for specific functionalities throughout the whole framework (we are grateful to the authors): Functionality Library Paths pathlib (distributed with Python since 3.4) Videos, Images OpenCV Neural networks PyTorch , torchvision Data handling pandas , NumPy Shape handling GeoPandas , Shapely Plotting, graphing Plotly User interface PySimpleGui Web PySimpleGUIWeb (camera system), Dash (dashboards)","title":"Dependencies"},{"location":"contribute/coding/#lintingautoformatting","text":"To match PEP8 specs and write pretty code we use flake8 to check against structural and style errors and black for autoformatting. Additionally the package flake8-docstrings can be used to lint the docstrings. The following settings are set in the .flake8 file: [flake8] max-line-length = 88 docstring-convention = google","title":"Linting/Autoformatting"},{"location":"contribute/documentation/","text":"Documentation \u00b6 You can help correcting, improving and extending this documentation by clicking on the \"edit\" button on top right of each page that brings you directly to the corresponding file in the documentation repository on Github. You can edit the page using the rather simple markdown language . Then you commit your changes by choosing a meaningful commit meassage and the \"Create a new branch for this commit and start a pull request\" option. After internal review and discussion (if necessery) we will merge your pull request into the documentation repository and then it will be also visible on the website. If you want to create new pages, have a look at the README file of the documentation repository . There you will also find a description of how this website is rendered and how inserting a new page into the navigation works. The documentation website of OpenTrafficCam is generated by MkDocs using the awesome material theme. The documentation is fully written in Markdown .","title":"Documentation"},{"location":"contribute/documentation/#documentation","text":"You can help correcting, improving and extending this documentation by clicking on the \"edit\" button on top right of each page that brings you directly to the corresponding file in the documentation repository on Github. You can edit the page using the rather simple markdown language . Then you commit your changes by choosing a meaningful commit meassage and the \"Create a new branch for this commit and start a pull request\" option. After internal review and discussion (if necessery) we will merge your pull request into the documentation repository and then it will be also visible on the website. If you want to create new pages, have a look at the README file of the documentation repository . There you will also find a description of how this website is rendered and how inserting a new page into the navigation works. The documentation website of OpenTrafficCam is generated by MkDocs using the awesome material theme. The documentation is fully written in Markdown .","title":"Documentation"},{"location":"contribute/github/","text":"GitHub \u00b6 All code of OpenTrafficCam is hosted on GitHub . For each core module a separate repository exists: OTCamera OTVision OTAnalytics OTLabels Following the Feature Branch Workflow each repository has a master branch, which at all times should contain a stable version of the software. All coding should be committed to separate feature branches. Once you completed coding on a feature, issue or bugfix, you create a pull request for the respective feature branch (preferably labeled using the GitHub labeling system ). Your contributed code will then be reviewed by the main developers, discussed (if necessary) and merged into the master branch. Even without coding you can contribute to the project: If you encounter any bug or got any idea for enhancement, feel free to open an issue in the corresponding repository. Every issue should at least have two labels assigned: One label indicating the maturaty of the issue ( question , bug , feature or idea ) and one assigning the repository-specific subpackage or section ( black colored labels ). All pull request will be checked by GitHub's super-linter .","title":"GitHub"},{"location":"contribute/github/#github","text":"All code of OpenTrafficCam is hosted on GitHub . For each core module a separate repository exists: OTCamera OTVision OTAnalytics OTLabels Following the Feature Branch Workflow each repository has a master branch, which at all times should contain a stable version of the software. All coding should be committed to separate feature branches. Once you completed coding on a feature, issue or bugfix, you create a pull request for the respective feature branch (preferably labeled using the GitHub labeling system ). Your contributed code will then be reviewed by the main developers, discussed (if necessary) and merged into the master branch. Even without coding you can contribute to the project: If you encounter any bug or got any idea for enhancement, feel free to open an issue in the corresponding repository. Every issue should at least have two labels assigned: One label indicating the maturaty of the issue ( question , bug , feature or idea ) and one assigning the repository-specific subpackage or section ( black colored labels ). All pull request will be checked by GitHub's super-linter .","title":"GitHub"},{"location":"contribute/gui/","text":"GUI \u00b6 For now we decided on using PysimpleGUI to build the OpenTrafficCam user interface, as it is lightweight and easy to start with. Whenever possible, we try to separate the gui code from all other code. All the gui modules have a place in the \"gui\" subpackage of the respective OT repository: OTCamera/OTCamera/gui OTVision/OTVision/gui OTAnalytics/OTAnalytics/gui For the future we can also imagine using a web framework for user interaction. This would also help to enable remote access to local GPU machines or high performance computing centers.","title":"GUI"},{"location":"contribute/gui/#gui","text":"For now we decided on using PysimpleGUI to build the OpenTrafficCam user interface, as it is lightweight and easy to start with. Whenever possible, we try to separate the gui code from all other code. All the gui modules have a place in the \"gui\" subpackage of the respective OT repository: OTCamera/OTCamera/gui OTVision/OTVision/gui OTAnalytics/OTAnalytics/gui For the future we can also imagine using a web framework for user interaction. This would also help to enable remote access to local GPU machines or high performance computing centers.","title":"GUI"},{"location":"contribute/otcamera/","text":"OTCamera Dev Version \u00b6 The Raspberry Pi Zero is wonderful to use outside (it consumes little power, is cheap, ...). But the development of OTCamera is more fun on a big Pi. Everything is much faster. You will need a prepared Raspberry Pi 2B/3B(+)/4 (the 2 GB works well) Camera module (no USB webcam) Setup VS Code Remote Development Extension \u00b6 Install the Remote-SSH extension on your desktop using the marketplace. Add the Pi as remote host . Connect to the Pi using the Remote-SSH extension (rightclick on the ne host - \"Connect to Host in New Window\"). When asked for the operating system of the host, choose \"Linux\". VS code will download and install the necessary dependencies on the Pi. Open the extension manager in the new windows an install all necessary extensions. Install the linter and autoformatter. pip install black flake8 Setup Git and GitHub \u00b6 Install git using apt. sudo apt install git -y To setup your git commit name and email, login to your github account and copy your private commit email . On the pi run git config --global user.name \"Your Name\" git config --global user.email \"123456+username@users.noreply.github.com\" The easiest way to setup your GitHub credentials is to use vs code. In the file explorer panel click \"Clone Repository\" and choose \"Clone from GitHub\". Depending on your desktop computer settings, a browser tab will open to login into your GitHub account. Afterwards, you can search for \"OpenTrafficCam/OTCamera\" inside the vs code command prompt and select a folder to pull (choose /home/pi by default). Install Screen to Run OTCamera in Background \u00b6 Quote Screen is a full-screen window manager that multiplexes a physical terminal between several processes, typically interactive shells. To install screen on the developer pi just run sudo apt install screen -y You can now start screen and use it as an usual terminal. You can run the python OTCamera to start the camera in an active screen session. Hit Ctrl + A and then D to detach the current session. Screen will still run in background including the OTCamera. If you start screen including the -r option you can reconnect to this terminal session: screen -r You can safely disconnect you ssh connection to the pi, while the screen session is still running. Ready to Develop \u00b6 You should now be ready to pull the OTCamera repository and start developing.","title":"OTCamera Dev Version"},{"location":"contribute/otcamera/#otcamera-dev-version","text":"The Raspberry Pi Zero is wonderful to use outside (it consumes little power, is cheap, ...). But the development of OTCamera is more fun on a big Pi. Everything is much faster. You will need a prepared Raspberry Pi 2B/3B(+)/4 (the 2 GB works well) Camera module (no USB webcam)","title":"OTCamera Dev Version"},{"location":"contribute/otcamera/#setup-vs-code-remote-development-extension","text":"Install the Remote-SSH extension on your desktop using the marketplace. Add the Pi as remote host . Connect to the Pi using the Remote-SSH extension (rightclick on the ne host - \"Connect to Host in New Window\"). When asked for the operating system of the host, choose \"Linux\". VS code will download and install the necessary dependencies on the Pi. Open the extension manager in the new windows an install all necessary extensions. Install the linter and autoformatter. pip install black flake8","title":"Setup VS Code Remote Development Extension"},{"location":"contribute/otcamera/#setup-git-and-github","text":"Install git using apt. sudo apt install git -y To setup your git commit name and email, login to your github account and copy your private commit email . On the pi run git config --global user.name \"Your Name\" git config --global user.email \"123456+username@users.noreply.github.com\" The easiest way to setup your GitHub credentials is to use vs code. In the file explorer panel click \"Clone Repository\" and choose \"Clone from GitHub\". Depending on your desktop computer settings, a browser tab will open to login into your GitHub account. Afterwards, you can search for \"OpenTrafficCam/OTCamera\" inside the vs code command prompt and select a folder to pull (choose /home/pi by default).","title":"Setup Git and GitHub"},{"location":"contribute/otcamera/#install-screen-to-run-otcamera-in-background","text":"Quote Screen is a full-screen window manager that multiplexes a physical terminal between several processes, typically interactive shells. To install screen on the developer pi just run sudo apt install screen -y You can now start screen and use it as an usual terminal. You can run the python OTCamera to start the camera in an active screen session. Hit Ctrl + A and then D to detach the current session. Screen will still run in background including the OTCamera. If you start screen including the -r option you can reconnect to this terminal session: screen -r You can safely disconnect you ssh connection to the pi, while the screen session is still running.","title":"Install Screen to Run OTCamera in Background"},{"location":"contribute/otcamera/#ready-to-develop","text":"You should now be ready to pull the OTCamera repository and start developing.","title":"Ready to Develop"},{"location":"contribute/vscode/","text":"VS Code \u00b6 We are developing OpenTrafficCam using Visual Studio Code using the following extensions: GitHub Pull Requests and Issues markdownlint Pylance Python Python Docstring Generator Pylance Settings \u00b6 To solve import errors in the repository, we need to add an extra path to the pylance config. For example in the workspace settings of OTCamera add: \"python.analysis.extraPaths\" : [ \"./OTCamera\" ] Or set it using the Settings-UI (Workspace -> Pylance -> Python - Analysis: Extra Path -> add './OTCamera') Snippets \u00b6 To comfortably insert code use snippets. You can open the configuration file for a specific language to add or modify the user snippets: Ctrl + Shift + p and Preferences: Configure User Snippets . You can insert the user snippets by start typing the \"prefix\" value. For example start typing gpl_ and autocompletection should recognize the user snippets stated below. GPL License Information \u00b6 Add the following snippet to vscodedata/user/snippets/python.json \"gpl_license_header\" : { \"prefix\" : \"gpl_add_head\" , \"body\" :[ \"$LINE_COMMENT ${1:Program Name and Function}\" , \"$LINE_COMMENT Copyright (C) $CURRENT_YEAR OpenTrafficCam Contributors\" , \"$LINE_COMMENT <https://github.com/OpenTrafficCam\" , \"$LINE_COMMENT <team@opentrafficcam.org>\" , \"$LINE_COMMENT\" , \"$LINE_COMMENT This program is free software: you can redistribute it and/or modify\" , \"$LINE_COMMENT it under the terms of the GNU General Public License as published by\" , \"$LINE_COMMENT the Free Software Foundation, either version 3 of the License, or\" , \"$LINE_COMMENT (at your option) any later version.\" , \"$LINE_COMMENT\" , \"$LINE_COMMENT This program is distributed in the hope that it will be useful,\" , \"$LINE_COMMENT but WITHOUT ANY WARRANTY; without even the implied warranty of\" , \"$LINE_COMMENT MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\" , \"$LINE_COMMENT GNU General Public License for more details.\" , \"$LINE_COMMENT\" , \"$LINE_COMMENT You should have received a copy of the GNU General Public License\" , \"$LINE_COMMENT along with this program. If not, see <https://www.gnu.org/licenses/>.\" , \"\" , ], \"description\" : \"Add GPLv3 license information in source code.\" } Add a short description and use tab to jump to the end of the snippet.","title":"VS Code"},{"location":"contribute/vscode/#vs-code","text":"We are developing OpenTrafficCam using Visual Studio Code using the following extensions: GitHub Pull Requests and Issues markdownlint Pylance Python Python Docstring Generator","title":"VS Code"},{"location":"contribute/vscode/#pylance-settings","text":"To solve import errors in the repository, we need to add an extra path to the pylance config. For example in the workspace settings of OTCamera add: \"python.analysis.extraPaths\" : [ \"./OTCamera\" ] Or set it using the Settings-UI (Workspace -> Pylance -> Python - Analysis: Extra Path -> add './OTCamera')","title":"Pylance Settings"},{"location":"contribute/vscode/#snippets","text":"To comfortably insert code use snippets. You can open the configuration file for a specific language to add or modify the user snippets: Ctrl + Shift + p and Preferences: Configure User Snippets . You can insert the user snippets by start typing the \"prefix\" value. For example start typing gpl_ and autocompletection should recognize the user snippets stated below.","title":"Snippets"},{"location":"contribute/vscode/#gpl-license-information","text":"Add the following snippet to vscodedata/user/snippets/python.json \"gpl_license_header\" : { \"prefix\" : \"gpl_add_head\" , \"body\" :[ \"$LINE_COMMENT ${1:Program Name and Function}\" , \"$LINE_COMMENT Copyright (C) $CURRENT_YEAR OpenTrafficCam Contributors\" , \"$LINE_COMMENT <https://github.com/OpenTrafficCam\" , \"$LINE_COMMENT <team@opentrafficcam.org>\" , \"$LINE_COMMENT\" , \"$LINE_COMMENT This program is free software: you can redistribute it and/or modify\" , \"$LINE_COMMENT it under the terms of the GNU General Public License as published by\" , \"$LINE_COMMENT the Free Software Foundation, either version 3 of the License, or\" , \"$LINE_COMMENT (at your option) any later version.\" , \"$LINE_COMMENT\" , \"$LINE_COMMENT This program is distributed in the hope that it will be useful,\" , \"$LINE_COMMENT but WITHOUT ANY WARRANTY; without even the implied warranty of\" , \"$LINE_COMMENT MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\" , \"$LINE_COMMENT GNU General Public License for more details.\" , \"$LINE_COMMENT\" , \"$LINE_COMMENT You should have received a copy of the GNU General Public License\" , \"$LINE_COMMENT along with this program. If not, see <https://www.gnu.org/licenses/>.\" , \"\" , ], \"description\" : \"Add GPLv3 license information in source code.\" } Add a short description and use tab to jump to the end of the snippet.","title":"GPL License Information"},{"location":"overview/dataprivacy/","text":"Data Privacy (Germany) \u00b6 German law only The following FAQ on data protection are intentionally only available in German, as they refer exclusively to German law. They also do not apply to other countries with German as an official language (e.g. Austria, Switzerland). Die nachfolgenden FAQ zum Datenschutz sind mit Absicht nur in deutscher Sprache verf\u00fcgbar, da sie sich ausschlie\u00dflich auf deutsches Recht beziehen. Sie gelten daher auch nicht f\u00fcr andere L\u00e4nder mit Deutsch als Amtssprache (z. B. \u00d6sterreich, Schweiz). Vorwort \u00b6 In einem Workshop im Rahmen der durch das BMVI gef\u00f6rderten mFund-Machbarkeitsstudie zu OpenTrafficCam haben wir gemeinsam mit anderen Verkehrsfachleuten h\u00e4ufige Fragen zum Datenschutz bei der Videoerhebung des Stra\u00dfenverkehrs gesammelt. Diese Fragen sind nachfolgend aufgelistet. Die Beantwortung der folgenden Fragen erfolgte in mehreren Interviews mit dem Datenschutzbeauftragten der TU Dresden. Sie dienen als initiale Informationsquelle zur Einhaltung des Datenschutzes bei der Videoaufzeichnung in Forschungsprojekten . Sie ersetzen keine Rechtsberatung und k\u00f6nnen verst\u00e4ndlicherweise nicht jedes einzelne Verfahren sowie die rechtlichen Spezifika der Bundesl\u00e4nder ber\u00fccksichtigen. Deshalb ist es auf jeden Fall erforderlich, die/den jeweils zust\u00e4ndigen Datenschutzbeauftragten fr\u00fchzeitig in das konkrete Projekt einzubeziehen. Als ein Ergebnis der Gespr\u00e4che hat sich auch gezeigt, dass zur Kl\u00e4rung detaillierter und \u00fcber den Forschungsbereich hinausgehender Fragen des Datenschutzes bei der automatisierten, videobasierten Verkehrserfassung eine dezidierte Besch\u00e4ftigung einer spezialisierten Anwaltskanzlei zielf\u00fchrend sein kann. Zuk\u00fcnftig ist eine Erweiterung der FAQ um Inhalte zum Datenschutz bei der Videoerhebung in nichtwissenschaftlichen Projekten geplant. FAQ zum Datenschutz bei videobasierter Verkehrserfassung in Forschungsprojekten \u00b6 Grunds\u00e4tzliches \u00b6 Wer ist in einem Projekt zust\u00e4ndig f\u00fcr den Datenschutz und tr\u00e4gt die Verantwortung? Die Auftragnehmenden oder die Auftraggebenden? Bei Forschungsf\u00f6rderung: Die erhebende Institution ist verantwortlich. Hier ist der Forschungsparagraph des jeweiligen Landesdatenschutzgesetzes relevant, der relativ viele Freiheitsgrade beinhaltet. Dementsprechend ist die Freiheit von Forschung und Lehre besonders sch\u00fctzenswert und ist mit dem Recht auf Schutz personenbezogener Daten abzuw\u00e4gen. Grundsatz dabei ist immer die weitgehende Minimierung der Eingriffe in das Pers\u00f6nlichkeitsrecht. Bei Forschungsauftr\u00e4gen: Hier kommt es darauf an, in wessen Interesse die Datenerhebung liegt. Wenn die Forschungsmethodik durch die Auftraggebenden vorgegeben wird oder die Forschungsfrage nicht mit alternativen Erhebungsdesigns beantwortet werden kann (z.B. falls Videobeobachtungen in geringer Aufl\u00f6sung ohne die Erfassung personenbezogener Daten daf\u00fcr nicht ausreichen), liegt die Verantwortung aus rechtlicher Sicht allein beim Auftraggeber. Dies gilt auch, wenn die Auftragnehmenden die (zu diesem Zeitpunkt bereits anonymisierten) Daten \u00fcber den Projektgegenstand hinaus publizieren (es z\u00e4hlt der initiale Zweck der Datenerfassung). In diesem Fall sind die Auftraggebenden nicht nur verpflichtet, nach einem Datenschutzkonzept zu fragen. Sie sind auch dazu verpflichtet, Vorgaben f\u00fcr den Datenschutz in der Ausschreibung zu formulieren und Angebote darauf zu pr\u00fcfen, dass diese eigenen Vorgaben des Datenschutzes eingehalten werden. Wenn die Ausschreibung und die zu beantwortenden Forschungsfragen es zulassen, dass die Auftragnehmenden das Erhebungsdesign frei w\u00e4hlen, gilt die gemeinsame Verantwortlichkeit der Auftraggebenden und Auftragnehmenden gem\u00e4\u00df Art. 26 DSGVO (\u201cgemeinsames Interesse an der Datenauswertung\u201d / Gemeinsame Verantwortlichkeit). Ist dar\u00fcber hinaus eine Instanz (z. B. Stra\u00dfenbaulasttr\u00e4ger) befugt, eine Messung aus Gr\u00fcnden des Datenschutzes zu verbieten? Nein, weder der Stra\u00dfenbaulasttr\u00e4ger, noch eine Gebietsk\u00f6rperschaft oder der entsprechende Landesdatenschutzbeauftragte d\u00fcrfen die Datenerhebung auf \u00f6ffentlichen Fl\u00e4chen aus Gr\u00fcnden des Datenschutzes erlauben oder verbieten. Die Einhaltung des Datenschutzes liegt wie bereits beschrieben entweder in der Verantwortung der Forschungseinrichtung oder des Auftraggebers. Achtung! Neben dem Datenschutz k\u00f6nnten jedoch der Stra\u00dfenbaulasttr\u00e4ger bzw. die Gebietsk\u00f6rperschaft aufgrund ihrer Verkehrssicherungspflicht Einw\u00e4nde haben oder aufgrund der Montage von Kameras an Einbauten oder B\u00e4umen in deren Zust\u00e4ndigkeit. Welche sind dabei die relevanten Rechtsvorschriften und wo/f\u00fcr wen gelten sie? Gibt es bereichsspezifische Regelungen? Grunds\u00e4tzlich gilt zun\u00e4chst immer die europ\u00e4ische DSGVO, die jedoch gewisse Punkte offenl\u00e4sst, die wiederum von den Gesetzen nachgeordneter Gebietsk\u00f6rperschaften geregelt werden. F\u00fcr die Privatwirtschaft und Institutionen des Bundes gilt dabei spezialgesetzlich das BDSG. F\u00fcr \u00f6ffentliche Einrichtungen der Bundesl\u00e4nder (also die meisten Hochschulen), der Landkreise und kreisfreien St\u00e4dte gilt das jeweilige Landesdatenschutzgesetz, welches sich in der Regel bzgl. Forschung nicht stark vom BDSG unterscheidet. Der Geltungsbereich bereichsspezifischer Regelungen wird also durch die rechtliche Zuordnung der Institution zu einer Gebietsk\u00f6rperschaft bestimmt und nicht durch den Ort der Datenerhebung. Wenn mehrere Institutionen gemeinsam f\u00fcr den Datenschutz verantwortlich sind, gilt f\u00fcr jede das f\u00fcr sie relevante Gesetz bzw. die Gesetze. Gibt es Unterschiede je nach Erhebungszweck bzw. Zweck des Projekts? Es gibt Unterschiede zwischen Datenerhebungen zum Zweck der Forschung und anderen Zwecken wie Wirtschaft oder Verwaltung (z.B. Wahrung der \u00f6ffentlichen Sicherheit) sowie in der Verantwortung der Beteiligten. Ansonsten sind keine weiteren Unterschiede nach dem Erhebungszweck bekannt. Wie \"dehnbar\" ist das Datenschutzrecht, wie unterschiedlich kann es ausgelegt werden? Es existieren tats\u00e4chlich viele Grauzonen aufgrund der Nichtregulierung spezifischer Einzelf\u00e4lle (zu denen auch die videobasierte, automatisierte Verkehrserfassung f\u00fcr die Forschung und die Planung z\u00e4hlt), die im Falle rechtlicher Streitigkeiten separat bewertet werden m\u00fcssen. Wenn f\u00fcr rechtliche Fragestellungen keine dezidierten Rechtsnormen vorliegen, kann sich ein Gericht einerseits auf Pr\u00e4zedenzurteile berufen. Zur videobasierten Verkehrserfassung sind bisher jedoch nur wenige Urteile bekannt (siehe Frage \"Gibt es Pr\u00e4zedenz-Urteile zur videobasierten Verkehrserfassung?\"). Andererseits ist eine rechtliche Anerkennung niedergeschriebener Branchenstandards m\u00f6glich, die in Zusammenarbeit zwischen den datenerhebenden Institutionen und den Aufsichtsbeh\u00f6rden erstellt wurden. Ein Beispiel daf\u00fcr ist der Arbeitskreis \"Datenschutz\" f\u00fcr Gesundheitsdaten, der in Abstimmung mit den Aufsichtsbeh\u00f6rden ein Rahmenkonzept entwickelt hat, auf das sich im Falle rechtlicher Streitigkeiten berufen werden kann. Zur Schaffung einer gewissen Rechtssicherheit bei den Details der videobasierten Verkehrserhebung f\u00fcr die Forschung und Planung kann ein \u00e4quivalentes Vorgehen sinnvoll sein. Gibt es Pr\u00e4zedenz-Urteile zur videobasierten Verkehrserfassung? Urteile zur Verkehrserfassung per Video zum Zweck der allgemeinen Verkehrsdatenerhebung f\u00fcr oder zur Unterst\u00fctzung von Forschung oder Verwaltungst\u00e4tigkeiten sind uns nicht bekannt. Folgende Urteile zum Datenschutz bei der Bilderfassung im Stra\u00dfenverkehr sind dar\u00fcber hinaus bekannt: OLG Frankfurt, 06.11.2019 - 2 Ss - OWi 942/19 Das OLG Frankfurt hat 2019 entschieden, dass private Dienstleister keine Verkehrs\u00fcberwachungen durf\u00fchren d\u00fcrfen und entsprechende Bu\u00dfgeldbescheide gesetzeswidrig sind. BGH 15.05.2018 \u2013 VI ZR 233/17 Der Bundesgerichtshof hat 2018 das sogenannte Dashcam-Urteil gef\u00e4llt, nachdem es f\u00fcr private Personen datenschutzrechtlich unzul\u00e4ssig ist, personenbezogene Merkmale mit einer Kamera aus einem fahrenden Auto \u201cpermanent und anlasslos\u201d aufzuzeichnen. Kameras mit Ringspeicher, die das Geschehen nur im Falle eines Unfalls permanent speichern, w\u00e4ren demnach zul\u00e4ssig. Im entsprechenden Fall hat das Gericht die permanente Videoaufzeichnung dennoch als Beweismittel zugelassen. Muss eine Verkehrskamera als solche erkennbar und ausgewiesen sein oder darf man \"verdeckt\" filmen? Falls keine personenbezogenen Daten erfasst werden, ist keine Information der Verkehrsbeteiligten notwendig. Falls personenbezogene Daten erfasst werden, ist grunds\u00e4tzlich eine Einverst\u00e4ndniserkl\u00e4rung der jeweiligen Personen einzuholen. Da dies bei Videobeobachtungen im Verkehrsraum meist nicht m\u00f6glich ist, soll stattdessen eine gute erkennbare Information der Verkehrsteilnehmenden erfolgen (z.B. \u00fcber ein auch f\u00fcr Kfz-F\u00fchrende gut lesbares Schild \u201cVerkehrserhebung\u201d) und Detailinformationen zur Erhebung an bzw. in der N\u00e4he der Kamera (mind. erhebende Institution und Kontaktinformationen, weitere Details z.B. auf einer Website, die per QR-Code erreichbar ist). Falls personenbezogene Daten erfasst werden und im Rahmen von Forschungsprojekten eine Information der Verkehrsteilnehmenden \u00fcber die Verkehrserhebung die Verl\u00e4sslichkeit der angestrebten Ergebnisse beeintr\u00e4chtigen w\u00fcrde (z.B. bei der Untersuchung regelkonformen oder sicherheitsrelevanten Verkehrsverhaltens), kann in Einzelf\u00e4llen darauf verzichtet werden. Grundlage hierf\u00fcr ist der Forschungsparagraph des jeweiligen Landesdatenschutzgesetzes, nach dem die Freiheit von Forschung und Lehre besonders sch\u00fctzenswert ist und das Pers\u00f6nlichkeitsrecht \u00fcberwiegen kann. Dieses Vorgehen sollte jedoch ausf\u00fchrlich begr\u00fcndet und von Beginn an dokumentiert werden, um etwaigen Klagen durch Verkehrsteilnehmende vor Gericht standzuhalten. Eine Abstimmung mit der zust\u00e4ndigen Stelle f\u00fcr Informationssicherheit wird dringend empfohlen. Vermeidung der Erfassung personenbezogener Daten \u00b6 Wie sind personenbezogene Daten definiert? Z\u00e4hlen dazu neben Gesicht und Nummernschild auch die Gr\u00f6\u00dfe der Person, die Art ihres Gangs, die Art und Farbe ihrer Kleidung oder auff\u00e4llige Frisuren? Laut Art. 4 Abs. 1 DSGVO sind alle Namen, Kennnummern, Standortdaten und Merkmale von nat\u00fcrlichen Personen als \u201cpersonenbezogene Daten\u201d definiert, die R\u00fcckschl\u00fcsse auf deren Identit\u00e4t erlauben. Es gilt als grunds\u00e4tzlich anerkannt, dass im Bereich des Stra\u00dfenverkehrs das Kfz-Kennzeichen und das Gesicht einer Person als personenbezogene Daten gelten. In Einzelf\u00e4llen k\u00f6nnte auch die Kombination anderer Merkmale wie Gr\u00f6\u00dfe, Kleidung und Frisur mit weiteren Informationen (z.B. dem Standort) die Identifikation einer Person erm\u00f6glichen. Hier liegt die Beweispflicht jedoch bei potentiellen Klagenden und die Identifikation der Person im Bild/Video muss beispielhaft durch unabh\u00e4ngige Personen nachgewiesen werden (d.h. es reicht nicht aus, wenn die Person selbst oder eine ihr bekannte Person sie im Bild/Video identifizieren kann). Es sind Klagen und Urteile zur Identifikation von Personen auf Fotographien des \u00f6ffentlichen Raums bekannt, in denen f\u00fcr die Kl\u00e4ger entschieden wurde \u2013 diese sind jedoch nicht einfach auf Videoerhebungen im Stra\u00dfenverkehr zum Zwecke der allgemeinen Verkehrsdatenerfassung \u00fcbertragbar. Welche Entfernung zur Kamera z\u00e4hlt f\u00fcr die Datenschutzkonformit\u00e4t? Die Kamera ist grunds\u00e4tzlich so zu konfigurieren, dass nur der relevante Untersuchungsbereich erfasst wird. Wenn dies technisch nicht m\u00f6glich ist, dann muss im Nachgang eine Sichtung und L\u00f6schung erfolgen. Dies gilt z. B. auch f\u00fcr F\u00e4lle, in denen eine Person sich aktiv auf H\u00f6he der Kamera begibt, um diese aus der N\u00e4he zu betrachten und dadurch selbst deutlich erkennbar wird. Sichtung und L\u00f6schung k\u00f6nnen auch automatisiert im Nachgang erfolgen (technische Ma\u00dfnahme, falls z.B. Objekte automatisiert detektiert werden und ein Grenzwert f\u00fcr die Gr\u00f6\u00dfe des erfassten Objekts \u00fcberschritten wird). Reicht es aus, in den Videos sichtbare Gesichter und Kennzeichen zu verpixeln? Wann und wie muss das geschehen? Grunds\u00e4tzlich sollte wenn m\u00f6glich bereits durch niedrige Aufl\u00f6sung und De-Fokussierung die Erfassung von Kennzeichen und Gesichtern vermieden werden. Wenn dies f\u00fcr bestimmte, nicht interessierende Bildbereiche nicht m\u00f6glich ist, kann als erste Alternative eine Nicht-Erfassung (z. B. Schw\u00e4rzung) dieser Bildbereiche bereits im Kamerasystem gepr\u00fcft und technisch umgesetzt werden. Wenn auch dies nicht m\u00f6glich ist, k\u00f6nnen die entsprechenden Bildbereiche direkt auf der Kamera live und permanent verpixelt und somit anonymisiert werden. Diese Methode setzt eine automatisierte Detektion des Kennzeichens bzw. Gesichts voraus. Wenn daraus keine weiteren Daten abgeleitet und gespeichert oder \u00fcbermittelt werden (z.B. das Kennzeichen als Klartext), gilt diese Methode als anerkannt, da sie dem Stand der Technik entspricht. Vor dem Einsatz dieser Methode wird jedoch dringend empfohlen, die Konformit\u00e4t mit der entsprechend geltenden Rechtsgrundlage zu pr\u00fcfen. Das zeitversetzte, nachtr\u00e4gliche (manuelle oder automatische) Verpixeln von Kennzeichen oder Gesichtern bei der Sichtung/Auswertung des Videomaterials sollte auf Einzelf\u00e4lle beschr\u00e4nkt bleiben (siehe Frage \"Welche Entfernung zur Kamera z\u00e4hlt f\u00fcr die Datenschutzkonformit\u00e4t?\"). Sollten die zuvor genannten Ma\u00dfnahmen nicht zielf\u00fchrend sein und sind dennoch geh\u00e4uft Kennzeichen oder Gesichter in den Videos zu vermuten, kann nicht mehr von einer anonymen Erhebung (Vermeidung der Erhebung personenbezogener Daten) gesprochen werden. In diesem Fall m\u00fcssen die nachfolgend beschriebenen Rahmenbedingungen f\u00fcr die Erhebung personenbezogener Daten eingehalten werden. Ablauf in F\u00e4llen, in denen die Erfassung personenbezogener Daten nicht vermieden werden kann \u00b6 Unter welchen Umst\u00e4nden d\u00fcrfen personenbezogene Daten erhoben werden? Grunds\u00e4tzlich ist zu pr\u00fcfen, ob f\u00fcr den Erhebungszweck auch eine Videobeobachtung ohne die Erfassung personenbezogener Daten (bzw. eine Methode g\u00e4nzlich ohne Videobeobachtung) m\u00f6glich ist. Wenn dies nicht m\u00f6glich ist und das Interesse der Allgemeinheit an den Ergebnissen der Videobeobachtung die Interessen der einzelnen Personen (deren Datenschutz) \u00fcberwiegt, ist eine videobasierte Erfassung personenbezogener Daten grunds\u00e4tzlich denkbar. Dabei muss die Zustimmung der betroffenen Personen eingeholt werden oder zumindest eine Information der betroffenen Personen erfolgen (siehe Frage \u201cMuss eine Verkehrskamera als solche erkennbar und ausgewiesen sein oder darf man \"verdeckt\" filmen?\u201d). Was beinhaltet ein Datenschutzkonzept und in welchen F\u00e4llen braucht man es? Wer muss dieses \"absegnen\"? Die Erstellung eines Datenschutzkonzepts ist nicht in allen F\u00e4llen n\u00f6tig, in denen die Erhebung personenbezogener Daten geplant ist. Es ist jedoch sinnvoll, ein solches Konzept zu erstellen, da eine Rechenschaftspflicht besteht, d.h. eine Dokumentation zu f\u00fchren ist. In der DSGVO und in den Landesdatenschutzgesetzen ist f\u00fcr viele Aspekte auch grunds\u00e4tzlich geregelt, ob und wie personenbezogene Daten erhoben werden d\u00fcrfen. Wenn die Konformit\u00e4t mit diesen Rechtsgrundlagen eingehalten wird, reicht es aus, dies im Nachgang belegen zu k\u00f6nnen. Falls diesbez\u00fcglich Unklarheiten existieren, empfiehlt sich die R\u00fccksprache mit Datenschutzfachleuten (z.B. den zust\u00e4ndigen Beauftragten f\u00fcr Informationssicherheit und dem/der Datenschutzbeauftragten). In Zusammenarbeit mit diesen kann au\u00dferdem in F\u00e4llen, in denen vereinzelte Grunds\u00e4tze der Verarbeitung personenbezogener Daten nicht eingehalten werden k\u00f6nnen, eine sogenannte \u201cDatenschutzfolgenabsch\u00e4tzung\u201d erarbeitet werden. In dieser werden Erhebungszweck und -design, der Umgang mit den Daten sowie Gr\u00fcnde f\u00fcr die vereinzelten Abweichungen von den Rechtsgrundlagen detailliert erl\u00e4utert. Sind die Daten bereits w\u00e4hrend der Messung vor Diebstahl zu sch\u00fctzen? Ist es datenschutzkonform m\u00f6glich, einen externen Zugang auf die Kamera w\u00e4hrend der Messung herzustellen? Wenn personenbezogenen Daten erfasst werden, muss bereits w\u00e4hrend der Erfassung technisch sichergestellt werden, dass unbefugte Personen keinen Zugriff auf diese Daten haben. Daher sollten die Daten auf Kameras im Verkehrsraum nach dem aktuellen Stand der Technik gesch\u00fctzt werden. Momentan k\u00f6nnte dies durch ein sicheres Passwort erfolgen. Falls personenbezogene Daten \u00fcbermittelt werden, sind verschl\u00fcsselte Verbindungen zu empfehlen (z. B. per SSH, HTTPS, VPN). Unter welchen Umst\u00e4nden darf man die Videos speichern und aufheben? D\u00fcrfen die Videos f\u00fcr einen anderen als f\u00fcr den urspr\u00fcnglichen Zweck verwendet werden? Personenbezogene Daten m\u00fcssen so gespeichert werden, dass ausschlie\u00dflich befugte Personen Zugriff zu diesen haben. Au\u00dferdem d\u00fcrfen sie grunds\u00e4tzlich nur so lange gespeichert werden bis deren spezifischer Erhebungszweck erf\u00fcllt wurde (also z.B. die spezifische Forschungsfrage innerhalb eines Projekts beantwortet wurde). Anschlie\u00dfend m\u00fcssen die Daten gel\u00f6scht oder um die Eigenschaften reduziert werden, die einen Personenbezug erm\u00f6glichen (bei Videos z.B. Verringerung der Aufl\u00f6sung). Personenbezogene Daten d\u00fcrfen nur dann auch nach Erf\u00fcllung des spezifischen Erhebungszwecks vorgehalten werden, wenn die betroffenen Personen dem ausdr\u00fccklich zugestimmt haben. Vor der Verwendung gespeicherter personenbezogener Daten f\u00fcr weitere Zwecke ist jedoch erneut die Konformit\u00e4t des Zwecks sowohl mit den Rechtsgrundlagen als auch mit dem Wortlaut der Einwilligung der betroffenen Personen zu pr\u00fcfen. Darf ich die Verkehrsvideos weitergeben? Was muss ich dabei beachten? (Stichworte \"Nachnutzbarkeit\" und \"Datensparsamkeit\") Wenn es sich um personenbezogene Daten handelt und der \u00f6ffentliche F\u00f6rder- oder Auftraggeber f\u00fcr den Datenschutz verantwortlich ist, liegt diese Entscheidung bei ihm (siehe Frage \"Wer ist in einem Projekt zust\u00e4ndig f\u00fcr den Datenschutz und tr\u00e4gt die Verantwortung? Die Auftragnehmenden oder die Auftraggebenden?\"). Wenn der F\u00f6rder- oder Auftragnehmer selbst f\u00fcr den Datenschutz verantwortlich ist und die Anfrage von \u00f6ffentlichen Stellen (z.B. Gebietsk\u00f6rperschaft, Beh\u00f6rde, Universit\u00e4t) kommt, ist die anfragende \u00f6ffentliche Stelle auch f\u00fcr die \u00dcbermittlung verantwortlich. Das bedeutet, dass die anfragende Stelle sicherstellen muss, dass die \u00dcbermittlung der Daten gerechtfertigt ist. Einer \u00dcbermittlung an eine \u00f6ffentliche Stelle steht also nichts im Wege. Dies gilt auch, falls die \u00f6ffentliche Stelle die Daten f\u00fcr einen anderen als den urspr\u00fcnglichen Erhebungszweck verarbeiten m\u00f6chte (diese \u00f6ffentliche Stelle muss dann selbst pr\u00fcfen, ob dies rechtskonform ist). Vor einer \u00dcbermittlung an eine nicht\u00f6ffentliche Stelle (z.B. an ein Ingenieurb\u00fcro) empfehlen wir dringend die Konsultation von Fachleuten f\u00fcr Datenschutz. Die \u00dcbermittlung anonymisierter Verkehrsvideos stellt aus datenschutzrechtlicher Sicht nat\u00fcrlich kein Problem dar und kann aus dieser Hinsicht an alle Stellen erfolgen.","title":"Data Privacy (Germany)"},{"location":"overview/dataprivacy/#data-privacy-germany","text":"German law only The following FAQ on data protection are intentionally only available in German, as they refer exclusively to German law. They also do not apply to other countries with German as an official language (e.g. Austria, Switzerland). Die nachfolgenden FAQ zum Datenschutz sind mit Absicht nur in deutscher Sprache verf\u00fcgbar, da sie sich ausschlie\u00dflich auf deutsches Recht beziehen. Sie gelten daher auch nicht f\u00fcr andere L\u00e4nder mit Deutsch als Amtssprache (z. B. \u00d6sterreich, Schweiz).","title":"Data Privacy (Germany)"},{"location":"overview/dataprivacy/#vorwort","text":"In einem Workshop im Rahmen der durch das BMVI gef\u00f6rderten mFund-Machbarkeitsstudie zu OpenTrafficCam haben wir gemeinsam mit anderen Verkehrsfachleuten h\u00e4ufige Fragen zum Datenschutz bei der Videoerhebung des Stra\u00dfenverkehrs gesammelt. Diese Fragen sind nachfolgend aufgelistet. Die Beantwortung der folgenden Fragen erfolgte in mehreren Interviews mit dem Datenschutzbeauftragten der TU Dresden. Sie dienen als initiale Informationsquelle zur Einhaltung des Datenschutzes bei der Videoaufzeichnung in Forschungsprojekten . Sie ersetzen keine Rechtsberatung und k\u00f6nnen verst\u00e4ndlicherweise nicht jedes einzelne Verfahren sowie die rechtlichen Spezifika der Bundesl\u00e4nder ber\u00fccksichtigen. Deshalb ist es auf jeden Fall erforderlich, die/den jeweils zust\u00e4ndigen Datenschutzbeauftragten fr\u00fchzeitig in das konkrete Projekt einzubeziehen. Als ein Ergebnis der Gespr\u00e4che hat sich auch gezeigt, dass zur Kl\u00e4rung detaillierter und \u00fcber den Forschungsbereich hinausgehender Fragen des Datenschutzes bei der automatisierten, videobasierten Verkehrserfassung eine dezidierte Besch\u00e4ftigung einer spezialisierten Anwaltskanzlei zielf\u00fchrend sein kann. Zuk\u00fcnftig ist eine Erweiterung der FAQ um Inhalte zum Datenschutz bei der Videoerhebung in nichtwissenschaftlichen Projekten geplant.","title":"Vorwort"},{"location":"overview/dataprivacy/#faq-zum-datenschutz-bei-videobasierter-verkehrserfassung-in-forschungsprojekten","text":"","title":"FAQ zum Datenschutz bei videobasierter Verkehrserfassung in Forschungsprojekten"},{"location":"overview/dataprivacy/#grundsatzliches","text":"Wer ist in einem Projekt zust\u00e4ndig f\u00fcr den Datenschutz und tr\u00e4gt die Verantwortung? Die Auftragnehmenden oder die Auftraggebenden? Bei Forschungsf\u00f6rderung: Die erhebende Institution ist verantwortlich. Hier ist der Forschungsparagraph des jeweiligen Landesdatenschutzgesetzes relevant, der relativ viele Freiheitsgrade beinhaltet. Dementsprechend ist die Freiheit von Forschung und Lehre besonders sch\u00fctzenswert und ist mit dem Recht auf Schutz personenbezogener Daten abzuw\u00e4gen. Grundsatz dabei ist immer die weitgehende Minimierung der Eingriffe in das Pers\u00f6nlichkeitsrecht. Bei Forschungsauftr\u00e4gen: Hier kommt es darauf an, in wessen Interesse die Datenerhebung liegt. Wenn die Forschungsmethodik durch die Auftraggebenden vorgegeben wird oder die Forschungsfrage nicht mit alternativen Erhebungsdesigns beantwortet werden kann (z.B. falls Videobeobachtungen in geringer Aufl\u00f6sung ohne die Erfassung personenbezogener Daten daf\u00fcr nicht ausreichen), liegt die Verantwortung aus rechtlicher Sicht allein beim Auftraggeber. Dies gilt auch, wenn die Auftragnehmenden die (zu diesem Zeitpunkt bereits anonymisierten) Daten \u00fcber den Projektgegenstand hinaus publizieren (es z\u00e4hlt der initiale Zweck der Datenerfassung). In diesem Fall sind die Auftraggebenden nicht nur verpflichtet, nach einem Datenschutzkonzept zu fragen. Sie sind auch dazu verpflichtet, Vorgaben f\u00fcr den Datenschutz in der Ausschreibung zu formulieren und Angebote darauf zu pr\u00fcfen, dass diese eigenen Vorgaben des Datenschutzes eingehalten werden. Wenn die Ausschreibung und die zu beantwortenden Forschungsfragen es zulassen, dass die Auftragnehmenden das Erhebungsdesign frei w\u00e4hlen, gilt die gemeinsame Verantwortlichkeit der Auftraggebenden und Auftragnehmenden gem\u00e4\u00df Art. 26 DSGVO (\u201cgemeinsames Interesse an der Datenauswertung\u201d / Gemeinsame Verantwortlichkeit). Ist dar\u00fcber hinaus eine Instanz (z. B. Stra\u00dfenbaulasttr\u00e4ger) befugt, eine Messung aus Gr\u00fcnden des Datenschutzes zu verbieten? Nein, weder der Stra\u00dfenbaulasttr\u00e4ger, noch eine Gebietsk\u00f6rperschaft oder der entsprechende Landesdatenschutzbeauftragte d\u00fcrfen die Datenerhebung auf \u00f6ffentlichen Fl\u00e4chen aus Gr\u00fcnden des Datenschutzes erlauben oder verbieten. Die Einhaltung des Datenschutzes liegt wie bereits beschrieben entweder in der Verantwortung der Forschungseinrichtung oder des Auftraggebers. Achtung! Neben dem Datenschutz k\u00f6nnten jedoch der Stra\u00dfenbaulasttr\u00e4ger bzw. die Gebietsk\u00f6rperschaft aufgrund ihrer Verkehrssicherungspflicht Einw\u00e4nde haben oder aufgrund der Montage von Kameras an Einbauten oder B\u00e4umen in deren Zust\u00e4ndigkeit. Welche sind dabei die relevanten Rechtsvorschriften und wo/f\u00fcr wen gelten sie? Gibt es bereichsspezifische Regelungen? Grunds\u00e4tzlich gilt zun\u00e4chst immer die europ\u00e4ische DSGVO, die jedoch gewisse Punkte offenl\u00e4sst, die wiederum von den Gesetzen nachgeordneter Gebietsk\u00f6rperschaften geregelt werden. F\u00fcr die Privatwirtschaft und Institutionen des Bundes gilt dabei spezialgesetzlich das BDSG. F\u00fcr \u00f6ffentliche Einrichtungen der Bundesl\u00e4nder (also die meisten Hochschulen), der Landkreise und kreisfreien St\u00e4dte gilt das jeweilige Landesdatenschutzgesetz, welches sich in der Regel bzgl. Forschung nicht stark vom BDSG unterscheidet. Der Geltungsbereich bereichsspezifischer Regelungen wird also durch die rechtliche Zuordnung der Institution zu einer Gebietsk\u00f6rperschaft bestimmt und nicht durch den Ort der Datenerhebung. Wenn mehrere Institutionen gemeinsam f\u00fcr den Datenschutz verantwortlich sind, gilt f\u00fcr jede das f\u00fcr sie relevante Gesetz bzw. die Gesetze. Gibt es Unterschiede je nach Erhebungszweck bzw. Zweck des Projekts? Es gibt Unterschiede zwischen Datenerhebungen zum Zweck der Forschung und anderen Zwecken wie Wirtschaft oder Verwaltung (z.B. Wahrung der \u00f6ffentlichen Sicherheit) sowie in der Verantwortung der Beteiligten. Ansonsten sind keine weiteren Unterschiede nach dem Erhebungszweck bekannt. Wie \"dehnbar\" ist das Datenschutzrecht, wie unterschiedlich kann es ausgelegt werden? Es existieren tats\u00e4chlich viele Grauzonen aufgrund der Nichtregulierung spezifischer Einzelf\u00e4lle (zu denen auch die videobasierte, automatisierte Verkehrserfassung f\u00fcr die Forschung und die Planung z\u00e4hlt), die im Falle rechtlicher Streitigkeiten separat bewertet werden m\u00fcssen. Wenn f\u00fcr rechtliche Fragestellungen keine dezidierten Rechtsnormen vorliegen, kann sich ein Gericht einerseits auf Pr\u00e4zedenzurteile berufen. Zur videobasierten Verkehrserfassung sind bisher jedoch nur wenige Urteile bekannt (siehe Frage \"Gibt es Pr\u00e4zedenz-Urteile zur videobasierten Verkehrserfassung?\"). Andererseits ist eine rechtliche Anerkennung niedergeschriebener Branchenstandards m\u00f6glich, die in Zusammenarbeit zwischen den datenerhebenden Institutionen und den Aufsichtsbeh\u00f6rden erstellt wurden. Ein Beispiel daf\u00fcr ist der Arbeitskreis \"Datenschutz\" f\u00fcr Gesundheitsdaten, der in Abstimmung mit den Aufsichtsbeh\u00f6rden ein Rahmenkonzept entwickelt hat, auf das sich im Falle rechtlicher Streitigkeiten berufen werden kann. Zur Schaffung einer gewissen Rechtssicherheit bei den Details der videobasierten Verkehrserhebung f\u00fcr die Forschung und Planung kann ein \u00e4quivalentes Vorgehen sinnvoll sein. Gibt es Pr\u00e4zedenz-Urteile zur videobasierten Verkehrserfassung? Urteile zur Verkehrserfassung per Video zum Zweck der allgemeinen Verkehrsdatenerhebung f\u00fcr oder zur Unterst\u00fctzung von Forschung oder Verwaltungst\u00e4tigkeiten sind uns nicht bekannt. Folgende Urteile zum Datenschutz bei der Bilderfassung im Stra\u00dfenverkehr sind dar\u00fcber hinaus bekannt: OLG Frankfurt, 06.11.2019 - 2 Ss - OWi 942/19 Das OLG Frankfurt hat 2019 entschieden, dass private Dienstleister keine Verkehrs\u00fcberwachungen durf\u00fchren d\u00fcrfen und entsprechende Bu\u00dfgeldbescheide gesetzeswidrig sind. BGH 15.05.2018 \u2013 VI ZR 233/17 Der Bundesgerichtshof hat 2018 das sogenannte Dashcam-Urteil gef\u00e4llt, nachdem es f\u00fcr private Personen datenschutzrechtlich unzul\u00e4ssig ist, personenbezogene Merkmale mit einer Kamera aus einem fahrenden Auto \u201cpermanent und anlasslos\u201d aufzuzeichnen. Kameras mit Ringspeicher, die das Geschehen nur im Falle eines Unfalls permanent speichern, w\u00e4ren demnach zul\u00e4ssig. Im entsprechenden Fall hat das Gericht die permanente Videoaufzeichnung dennoch als Beweismittel zugelassen. Muss eine Verkehrskamera als solche erkennbar und ausgewiesen sein oder darf man \"verdeckt\" filmen? Falls keine personenbezogenen Daten erfasst werden, ist keine Information der Verkehrsbeteiligten notwendig. Falls personenbezogene Daten erfasst werden, ist grunds\u00e4tzlich eine Einverst\u00e4ndniserkl\u00e4rung der jeweiligen Personen einzuholen. Da dies bei Videobeobachtungen im Verkehrsraum meist nicht m\u00f6glich ist, soll stattdessen eine gute erkennbare Information der Verkehrsteilnehmenden erfolgen (z.B. \u00fcber ein auch f\u00fcr Kfz-F\u00fchrende gut lesbares Schild \u201cVerkehrserhebung\u201d) und Detailinformationen zur Erhebung an bzw. in der N\u00e4he der Kamera (mind. erhebende Institution und Kontaktinformationen, weitere Details z.B. auf einer Website, die per QR-Code erreichbar ist). Falls personenbezogene Daten erfasst werden und im Rahmen von Forschungsprojekten eine Information der Verkehrsteilnehmenden \u00fcber die Verkehrserhebung die Verl\u00e4sslichkeit der angestrebten Ergebnisse beeintr\u00e4chtigen w\u00fcrde (z.B. bei der Untersuchung regelkonformen oder sicherheitsrelevanten Verkehrsverhaltens), kann in Einzelf\u00e4llen darauf verzichtet werden. Grundlage hierf\u00fcr ist der Forschungsparagraph des jeweiligen Landesdatenschutzgesetzes, nach dem die Freiheit von Forschung und Lehre besonders sch\u00fctzenswert ist und das Pers\u00f6nlichkeitsrecht \u00fcberwiegen kann. Dieses Vorgehen sollte jedoch ausf\u00fchrlich begr\u00fcndet und von Beginn an dokumentiert werden, um etwaigen Klagen durch Verkehrsteilnehmende vor Gericht standzuhalten. Eine Abstimmung mit der zust\u00e4ndigen Stelle f\u00fcr Informationssicherheit wird dringend empfohlen.","title":"Grunds\u00e4tzliches"},{"location":"overview/dataprivacy/#vermeidung-der-erfassung-personenbezogener-daten","text":"Wie sind personenbezogene Daten definiert? Z\u00e4hlen dazu neben Gesicht und Nummernschild auch die Gr\u00f6\u00dfe der Person, die Art ihres Gangs, die Art und Farbe ihrer Kleidung oder auff\u00e4llige Frisuren? Laut Art. 4 Abs. 1 DSGVO sind alle Namen, Kennnummern, Standortdaten und Merkmale von nat\u00fcrlichen Personen als \u201cpersonenbezogene Daten\u201d definiert, die R\u00fcckschl\u00fcsse auf deren Identit\u00e4t erlauben. Es gilt als grunds\u00e4tzlich anerkannt, dass im Bereich des Stra\u00dfenverkehrs das Kfz-Kennzeichen und das Gesicht einer Person als personenbezogene Daten gelten. In Einzelf\u00e4llen k\u00f6nnte auch die Kombination anderer Merkmale wie Gr\u00f6\u00dfe, Kleidung und Frisur mit weiteren Informationen (z.B. dem Standort) die Identifikation einer Person erm\u00f6glichen. Hier liegt die Beweispflicht jedoch bei potentiellen Klagenden und die Identifikation der Person im Bild/Video muss beispielhaft durch unabh\u00e4ngige Personen nachgewiesen werden (d.h. es reicht nicht aus, wenn die Person selbst oder eine ihr bekannte Person sie im Bild/Video identifizieren kann). Es sind Klagen und Urteile zur Identifikation von Personen auf Fotographien des \u00f6ffentlichen Raums bekannt, in denen f\u00fcr die Kl\u00e4ger entschieden wurde \u2013 diese sind jedoch nicht einfach auf Videoerhebungen im Stra\u00dfenverkehr zum Zwecke der allgemeinen Verkehrsdatenerfassung \u00fcbertragbar. Welche Entfernung zur Kamera z\u00e4hlt f\u00fcr die Datenschutzkonformit\u00e4t? Die Kamera ist grunds\u00e4tzlich so zu konfigurieren, dass nur der relevante Untersuchungsbereich erfasst wird. Wenn dies technisch nicht m\u00f6glich ist, dann muss im Nachgang eine Sichtung und L\u00f6schung erfolgen. Dies gilt z. B. auch f\u00fcr F\u00e4lle, in denen eine Person sich aktiv auf H\u00f6he der Kamera begibt, um diese aus der N\u00e4he zu betrachten und dadurch selbst deutlich erkennbar wird. Sichtung und L\u00f6schung k\u00f6nnen auch automatisiert im Nachgang erfolgen (technische Ma\u00dfnahme, falls z.B. Objekte automatisiert detektiert werden und ein Grenzwert f\u00fcr die Gr\u00f6\u00dfe des erfassten Objekts \u00fcberschritten wird). Reicht es aus, in den Videos sichtbare Gesichter und Kennzeichen zu verpixeln? Wann und wie muss das geschehen? Grunds\u00e4tzlich sollte wenn m\u00f6glich bereits durch niedrige Aufl\u00f6sung und De-Fokussierung die Erfassung von Kennzeichen und Gesichtern vermieden werden. Wenn dies f\u00fcr bestimmte, nicht interessierende Bildbereiche nicht m\u00f6glich ist, kann als erste Alternative eine Nicht-Erfassung (z. B. Schw\u00e4rzung) dieser Bildbereiche bereits im Kamerasystem gepr\u00fcft und technisch umgesetzt werden. Wenn auch dies nicht m\u00f6glich ist, k\u00f6nnen die entsprechenden Bildbereiche direkt auf der Kamera live und permanent verpixelt und somit anonymisiert werden. Diese Methode setzt eine automatisierte Detektion des Kennzeichens bzw. Gesichts voraus. Wenn daraus keine weiteren Daten abgeleitet und gespeichert oder \u00fcbermittelt werden (z.B. das Kennzeichen als Klartext), gilt diese Methode als anerkannt, da sie dem Stand der Technik entspricht. Vor dem Einsatz dieser Methode wird jedoch dringend empfohlen, die Konformit\u00e4t mit der entsprechend geltenden Rechtsgrundlage zu pr\u00fcfen. Das zeitversetzte, nachtr\u00e4gliche (manuelle oder automatische) Verpixeln von Kennzeichen oder Gesichtern bei der Sichtung/Auswertung des Videomaterials sollte auf Einzelf\u00e4lle beschr\u00e4nkt bleiben (siehe Frage \"Welche Entfernung zur Kamera z\u00e4hlt f\u00fcr die Datenschutzkonformit\u00e4t?\"). Sollten die zuvor genannten Ma\u00dfnahmen nicht zielf\u00fchrend sein und sind dennoch geh\u00e4uft Kennzeichen oder Gesichter in den Videos zu vermuten, kann nicht mehr von einer anonymen Erhebung (Vermeidung der Erhebung personenbezogener Daten) gesprochen werden. In diesem Fall m\u00fcssen die nachfolgend beschriebenen Rahmenbedingungen f\u00fcr die Erhebung personenbezogener Daten eingehalten werden.","title":"Vermeidung der Erfassung personenbezogener Daten"},{"location":"overview/dataprivacy/#ablauf-in-fallen-in-denen-die-erfassung-personenbezogener-daten-nicht-vermieden-werden-kann","text":"Unter welchen Umst\u00e4nden d\u00fcrfen personenbezogene Daten erhoben werden? Grunds\u00e4tzlich ist zu pr\u00fcfen, ob f\u00fcr den Erhebungszweck auch eine Videobeobachtung ohne die Erfassung personenbezogener Daten (bzw. eine Methode g\u00e4nzlich ohne Videobeobachtung) m\u00f6glich ist. Wenn dies nicht m\u00f6glich ist und das Interesse der Allgemeinheit an den Ergebnissen der Videobeobachtung die Interessen der einzelnen Personen (deren Datenschutz) \u00fcberwiegt, ist eine videobasierte Erfassung personenbezogener Daten grunds\u00e4tzlich denkbar. Dabei muss die Zustimmung der betroffenen Personen eingeholt werden oder zumindest eine Information der betroffenen Personen erfolgen (siehe Frage \u201cMuss eine Verkehrskamera als solche erkennbar und ausgewiesen sein oder darf man \"verdeckt\" filmen?\u201d). Was beinhaltet ein Datenschutzkonzept und in welchen F\u00e4llen braucht man es? Wer muss dieses \"absegnen\"? Die Erstellung eines Datenschutzkonzepts ist nicht in allen F\u00e4llen n\u00f6tig, in denen die Erhebung personenbezogener Daten geplant ist. Es ist jedoch sinnvoll, ein solches Konzept zu erstellen, da eine Rechenschaftspflicht besteht, d.h. eine Dokumentation zu f\u00fchren ist. In der DSGVO und in den Landesdatenschutzgesetzen ist f\u00fcr viele Aspekte auch grunds\u00e4tzlich geregelt, ob und wie personenbezogene Daten erhoben werden d\u00fcrfen. Wenn die Konformit\u00e4t mit diesen Rechtsgrundlagen eingehalten wird, reicht es aus, dies im Nachgang belegen zu k\u00f6nnen. Falls diesbez\u00fcglich Unklarheiten existieren, empfiehlt sich die R\u00fccksprache mit Datenschutzfachleuten (z.B. den zust\u00e4ndigen Beauftragten f\u00fcr Informationssicherheit und dem/der Datenschutzbeauftragten). In Zusammenarbeit mit diesen kann au\u00dferdem in F\u00e4llen, in denen vereinzelte Grunds\u00e4tze der Verarbeitung personenbezogener Daten nicht eingehalten werden k\u00f6nnen, eine sogenannte \u201cDatenschutzfolgenabsch\u00e4tzung\u201d erarbeitet werden. In dieser werden Erhebungszweck und -design, der Umgang mit den Daten sowie Gr\u00fcnde f\u00fcr die vereinzelten Abweichungen von den Rechtsgrundlagen detailliert erl\u00e4utert. Sind die Daten bereits w\u00e4hrend der Messung vor Diebstahl zu sch\u00fctzen? Ist es datenschutzkonform m\u00f6glich, einen externen Zugang auf die Kamera w\u00e4hrend der Messung herzustellen? Wenn personenbezogenen Daten erfasst werden, muss bereits w\u00e4hrend der Erfassung technisch sichergestellt werden, dass unbefugte Personen keinen Zugriff auf diese Daten haben. Daher sollten die Daten auf Kameras im Verkehrsraum nach dem aktuellen Stand der Technik gesch\u00fctzt werden. Momentan k\u00f6nnte dies durch ein sicheres Passwort erfolgen. Falls personenbezogene Daten \u00fcbermittelt werden, sind verschl\u00fcsselte Verbindungen zu empfehlen (z. B. per SSH, HTTPS, VPN). Unter welchen Umst\u00e4nden darf man die Videos speichern und aufheben? D\u00fcrfen die Videos f\u00fcr einen anderen als f\u00fcr den urspr\u00fcnglichen Zweck verwendet werden? Personenbezogene Daten m\u00fcssen so gespeichert werden, dass ausschlie\u00dflich befugte Personen Zugriff zu diesen haben. Au\u00dferdem d\u00fcrfen sie grunds\u00e4tzlich nur so lange gespeichert werden bis deren spezifischer Erhebungszweck erf\u00fcllt wurde (also z.B. die spezifische Forschungsfrage innerhalb eines Projekts beantwortet wurde). Anschlie\u00dfend m\u00fcssen die Daten gel\u00f6scht oder um die Eigenschaften reduziert werden, die einen Personenbezug erm\u00f6glichen (bei Videos z.B. Verringerung der Aufl\u00f6sung). Personenbezogene Daten d\u00fcrfen nur dann auch nach Erf\u00fcllung des spezifischen Erhebungszwecks vorgehalten werden, wenn die betroffenen Personen dem ausdr\u00fccklich zugestimmt haben. Vor der Verwendung gespeicherter personenbezogener Daten f\u00fcr weitere Zwecke ist jedoch erneut die Konformit\u00e4t des Zwecks sowohl mit den Rechtsgrundlagen als auch mit dem Wortlaut der Einwilligung der betroffenen Personen zu pr\u00fcfen. Darf ich die Verkehrsvideos weitergeben? Was muss ich dabei beachten? (Stichworte \"Nachnutzbarkeit\" und \"Datensparsamkeit\") Wenn es sich um personenbezogene Daten handelt und der \u00f6ffentliche F\u00f6rder- oder Auftraggeber f\u00fcr den Datenschutz verantwortlich ist, liegt diese Entscheidung bei ihm (siehe Frage \"Wer ist in einem Projekt zust\u00e4ndig f\u00fcr den Datenschutz und tr\u00e4gt die Verantwortung? Die Auftragnehmenden oder die Auftraggebenden?\"). Wenn der F\u00f6rder- oder Auftragnehmer selbst f\u00fcr den Datenschutz verantwortlich ist und die Anfrage von \u00f6ffentlichen Stellen (z.B. Gebietsk\u00f6rperschaft, Beh\u00f6rde, Universit\u00e4t) kommt, ist die anfragende \u00f6ffentliche Stelle auch f\u00fcr die \u00dcbermittlung verantwortlich. Das bedeutet, dass die anfragende Stelle sicherstellen muss, dass die \u00dcbermittlung der Daten gerechtfertigt ist. Einer \u00dcbermittlung an eine \u00f6ffentliche Stelle steht also nichts im Wege. Dies gilt auch, falls die \u00f6ffentliche Stelle die Daten f\u00fcr einen anderen als den urspr\u00fcnglichen Erhebungszweck verarbeiten m\u00f6chte (diese \u00f6ffentliche Stelle muss dann selbst pr\u00fcfen, ob dies rechtskonform ist). Vor einer \u00dcbermittlung an eine nicht\u00f6ffentliche Stelle (z.B. an ein Ingenieurb\u00fcro) empfehlen wir dringend die Konsultation von Fachleuten f\u00fcr Datenschutz. Die \u00dcbermittlung anonymisierter Verkehrsvideos stellt aus datenschutzrechtlicher Sicht nat\u00fcrlich kein Problem dar und kann aus dieser Hinsicht an alle Stellen erfolgen.","title":"Ablauf in F\u00e4llen, in denen die Erfassung personenbezogener Daten nicht vermieden werden kann"},{"location":"overview/gettingstarted/","text":"Getting Started \u00b6 All OTC Modules are integrated in a workflow The Framework of OpenTrafficCam's Core Functions","title":"Getting Started"},{"location":"overview/gettingstarted/#getting-started","text":"All OTC Modules are integrated in a workflow The Framework of OpenTrafficCam's Core Functions","title":"Getting Started"},{"location":"overview/usecases/trafficcounts/","text":"Traffic Counts \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Traffic Counts"},{"location":"overview/usecases/trafficcounts/#traffic-counts","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Traffic Counts"},{"location":"overview/usecases/trafficsafety/","text":"Traffic Safety \u00b6 The rest of the iceberg \u00b6 Traditionally, accident data has primarily been used to analyse the road safety of a traffic facility. However, this is a reactive approach, which means that accident occurrences must first be observed over many years before measures can be taken to improve road safety. In addition, the accident statistics show a certain number of unreported cases and there is only limited information on the sequence of events that led to the accident. Assuming that accidents are only the tip of the iceberg, traffic flow can also be observed and analysed in terms of conflicts or near misses in a complementary or supplementary manner. Near misses or traffic conflicts are defined by Amundsen and Hyd\u00e9n (1977) as \"observable situations in which two or more road users approach each other in space and time to such an extent that there is a risk of collision if their movements remain unchanged\". In a simplified way, the traffic flow can be divided into the following hierarchy of severity: Undisturbed passages - Interactions - Near misses/Conflicts - Accidents. The near misses (similar to accidents) can be divided into further severity levels with regard to the proximity to a collision, but also with regard to the severity of the resulting collision (similar to accident severity). PET - easy peasy \u00b6 A number of Surrogate Measures of Safety have been developed as objective indicators to classify an interaction as a near miss and to assess its severity (primarily in terms of proximity to a collision). One commonly used indicator is PET (Post Encroachment Time), which is a rather simple safety indicator. It describes the time period between the first road user leaving the common conflict area and the second one entering it. TTC - complex, but meaningful \u00b6 Another widely applied safety indicator in surrogate analysis is Time To Collision (TTC). It describes the remaining time to a collision between two road users at a certain point in time of their interaction and under the assumption that neither of the parties involved will take an evasive action. The lower the TTC, the more severe the conflict between road users. In case of a collision the TTC equals zero. A TTC can only be calculated if the two interacting road users are on a collision course. If the collision course is left due to an evasive action, the TTC for the time from the start of the evasive action is calculated on the basis of the hypotethic trajectories without the evasive action. Such hypotethic trajectories can be approximated by various methods of trajectory prediction. The simplest (but in some cases also the most unrealistic) method for trajectory prediction is to assume continued motion with constant velocity vectors. The TTC is calculated for each moment of interaction. This results in a TTC curve. Interactions and also periods of interactions are often categorised according to whether the TTC is below a threshold value (e.g. 1.5 sec). The Minimum TTC, the Time Exposed TTC and the Time Integrated TTC are used as aggregated indicators. In addition to road safety analysis, TTC is also used for active accident avoidance, especially for decision-making in algorithms of autonomous vehicles. Best practices \u00b6 In order to enable comparisons, e.g. with other traffic facilities, safety indicators like PET and TTC are related to suitable exposure units, such as traffic volume or the total number of interactions in the observation period. For comparisons one also needs statistcal significance of the frequency of conflicts at every individual location. To achieve this one must capture trajectories over a sufficient period of time. In most constellations, conflicts do occur more frequently than accidents that require observation periods of several years. However, recent studis show, that capturing videos over weeks or at least multiple days is the minimum required duration for conflict analyses. One mistake that also should be avoided is to raise the threshold for categorising encounters as conflicts, as this means measuring exposure, but not road safety. It\u00b4s all about validity \u00b6 Exposure based conflict frequencies can then be compared over different road sections or intersections or to known mean values. In addition, in the past, fixed factors for estimating the number of accidents were derived from the number of conflicts, which, however, could often not be confirmed by other researchers. To overcome fixed coefficients, a new approach taking into account variability of conflict severity based on Extreme Value Theory (EVT) is now state of the art, that has also been applied in stock market and flood protection earlier. Conflicts are even more valid with crash data when it comes to relative safety asssessment, e.g. ranking a number of road sites by traffic safety, which should deliver nearly the same results as using crash data from several years. This relative validity is sufficient for many applications in road safety work. Another advantage of the safety analysis of trajectories is that the entire movements of the road users leading to the conflict are known. For example, differences in movement patterns between conflicts and other encounters can be analyzed and suitable measures for increasing road safety can be derived from them. In order to avoid accidents, however, a certain process validity between conflicts and accidents is also required here. How widespread is the method? \u00b6 Several other European countries developed their own dedicated traffic conflict techniques many years ago and conflict analysis already plays an important role in their road safety assessment tools. In some other countries there appear to be concerns about the conflict technique from earlier days, as the severity of the conflicts was assessed by local human observers and was therefore largely subjective. Against the background of Vision Zero (zero traffic fatalities) and the developments in computer technology and AI, we advocate the use of proactive safety assessments based on the trajectories of road users. This can save lives and prevent many accidents. Nonetheless, there is still a need for further research into data collection, indicators, accident prediction and validation. In addition, clarification is required about the strengths and weaknesses of the conflict technology compared to the accident analysis and the need for further proactive safety tools. What\u00b4s next for us? \u00b6 Forthe next years it is planned to integrate the conflict analysis functionalities in OpenTrafficCam, to automate the processes as best as possible and still make them available to all non-programmers in order to enable both basic research and application studies for traffic experts.","title":"Traffic Safety"},{"location":"overview/usecases/trafficsafety/#traffic-safety","text":"","title":"Traffic Safety"},{"location":"overview/usecases/trafficsafety/#the-rest-of-the-iceberg","text":"Traditionally, accident data has primarily been used to analyse the road safety of a traffic facility. However, this is a reactive approach, which means that accident occurrences must first be observed over many years before measures can be taken to improve road safety. In addition, the accident statistics show a certain number of unreported cases and there is only limited information on the sequence of events that led to the accident. Assuming that accidents are only the tip of the iceberg, traffic flow can also be observed and analysed in terms of conflicts or near misses in a complementary or supplementary manner. Near misses or traffic conflicts are defined by Amundsen and Hyd\u00e9n (1977) as \"observable situations in which two or more road users approach each other in space and time to such an extent that there is a risk of collision if their movements remain unchanged\". In a simplified way, the traffic flow can be divided into the following hierarchy of severity: Undisturbed passages - Interactions - Near misses/Conflicts - Accidents. The near misses (similar to accidents) can be divided into further severity levels with regard to the proximity to a collision, but also with regard to the severity of the resulting collision (similar to accident severity).","title":"The rest of the iceberg"},{"location":"overview/usecases/trafficsafety/#pet-easy-peasy","text":"A number of Surrogate Measures of Safety have been developed as objective indicators to classify an interaction as a near miss and to assess its severity (primarily in terms of proximity to a collision). One commonly used indicator is PET (Post Encroachment Time), which is a rather simple safety indicator. It describes the time period between the first road user leaving the common conflict area and the second one entering it.","title":"PET - easy peasy"},{"location":"overview/usecases/trafficsafety/#ttc-complex-but-meaningful","text":"Another widely applied safety indicator in surrogate analysis is Time To Collision (TTC). It describes the remaining time to a collision between two road users at a certain point in time of their interaction and under the assumption that neither of the parties involved will take an evasive action. The lower the TTC, the more severe the conflict between road users. In case of a collision the TTC equals zero. A TTC can only be calculated if the two interacting road users are on a collision course. If the collision course is left due to an evasive action, the TTC for the time from the start of the evasive action is calculated on the basis of the hypotethic trajectories without the evasive action. Such hypotethic trajectories can be approximated by various methods of trajectory prediction. The simplest (but in some cases also the most unrealistic) method for trajectory prediction is to assume continued motion with constant velocity vectors. The TTC is calculated for each moment of interaction. This results in a TTC curve. Interactions and also periods of interactions are often categorised according to whether the TTC is below a threshold value (e.g. 1.5 sec). The Minimum TTC, the Time Exposed TTC and the Time Integrated TTC are used as aggregated indicators. In addition to road safety analysis, TTC is also used for active accident avoidance, especially for decision-making in algorithms of autonomous vehicles.","title":"TTC - complex, but meaningful"},{"location":"overview/usecases/trafficsafety/#best-practices","text":"In order to enable comparisons, e.g. with other traffic facilities, safety indicators like PET and TTC are related to suitable exposure units, such as traffic volume or the total number of interactions in the observation period. For comparisons one also needs statistcal significance of the frequency of conflicts at every individual location. To achieve this one must capture trajectories over a sufficient period of time. In most constellations, conflicts do occur more frequently than accidents that require observation periods of several years. However, recent studis show, that capturing videos over weeks or at least multiple days is the minimum required duration for conflict analyses. One mistake that also should be avoided is to raise the threshold for categorising encounters as conflicts, as this means measuring exposure, but not road safety.","title":"Best practices"},{"location":"overview/usecases/trafficsafety/#it-s-all-about-validity","text":"Exposure based conflict frequencies can then be compared over different road sections or intersections or to known mean values. In addition, in the past, fixed factors for estimating the number of accidents were derived from the number of conflicts, which, however, could often not be confirmed by other researchers. To overcome fixed coefficients, a new approach taking into account variability of conflict severity based on Extreme Value Theory (EVT) is now state of the art, that has also been applied in stock market and flood protection earlier. Conflicts are even more valid with crash data when it comes to relative safety asssessment, e.g. ranking a number of road sites by traffic safety, which should deliver nearly the same results as using crash data from several years. This relative validity is sufficient for many applications in road safety work. Another advantage of the safety analysis of trajectories is that the entire movements of the road users leading to the conflict are known. For example, differences in movement patterns between conflicts and other encounters can be analyzed and suitable measures for increasing road safety can be derived from them. In order to avoid accidents, however, a certain process validity between conflicts and accidents is also required here.","title":"It\u00b4s all about validity"},{"location":"overview/usecases/trafficsafety/#how-widespread-is-the-method","text":"Several other European countries developed their own dedicated traffic conflict techniques many years ago and conflict analysis already plays an important role in their road safety assessment tools. In some other countries there appear to be concerns about the conflict technique from earlier days, as the severity of the conflicts was assessed by local human observers and was therefore largely subjective. Against the background of Vision Zero (zero traffic fatalities) and the developments in computer technology and AI, we advocate the use of proactive safety assessments based on the trajectories of road users. This can save lives and prevent many accidents. Nonetheless, there is still a need for further research into data collection, indicators, accident prediction and validation. In addition, clarification is required about the strengths and weaknesses of the conflict technology compared to the accident analysis and the need for further proactive safety tools.","title":"How widespread is the method?"},{"location":"overview/usecases/trafficsafety/#what-s-next-for-us","text":"Forthe next years it is planned to integrate the conflict analysis functionalities in OpenTrafficCam, to automate the processes as best as possible and still make them available to all non-programmers in order to enable both basic research and application studies for traffic experts.","title":"What\u00b4s next for us?"},{"location":"overview/usecases/vehiclespeeds/","text":"Vehicle Speeds \u00b6 Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Vehicle Speeds"},{"location":"overview/usecases/vehiclespeeds/#vehicle-speeds","text":"Coming soon Unfortunately, there is no content here yet. But we are currently working on completing this website.","title":"Vehicle Speeds"}]}