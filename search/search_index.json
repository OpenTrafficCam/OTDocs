{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"live/","title":"LIVE","text":""},{"location":"live/#opentrafficcam-live-hoyerswerda","title":"OpenTrafficCam LIVE Hoyerswerda","text":"<p>OpenTrafficCam wird in zahlreichen Projekten in Forschung und Praxis erfolgreich eingesetzt. Dabei wird der Stra\u00dfenverkehr tempor\u00e4r erfasst und anschlie\u00dfend automatisiert analysiert.</p> <p>Im Rahmen einer mFund-F\u00f6rderung wurde die Basis f\u00fcr die Live-Prozessierung der erfassten Verkehrsvideos gelegt. Aufbauend auf diesen Arbeiten erfolgt nun eine Implementierung vor Ort in Hoyerswerda. OpenTrafficCam LIVE wird dabei um die spezifischen Anforderungen der TU Dresden und der Stadt Hoyerswerda erweitert. Diese umfassen sowohl technischen Anforderungen im Hintergrund als auch neue Auswertungen und Darstellungen der Verkehrsdaten.</p> <p>Im Ergebnis werden mit OpenTrafficCam LIVE der Stra\u00dfenverkehr in Hoyerswerda permanent erfasst und die Videostreams in Echtzeit verkehrlich analysiert. Dadurch wird es m\u00f6glich, Verkehrsdaten nicht nur retrospektiv auszuwerten, sondern unmittelbar f\u00fcr Verkehrsmanagement, \u00d6ffentlichkeitsinformation oder vernetzte Systeme nutzbar zu machen.</p> <p>Kontaktieren Sie uns</p>"},{"location":"live/#ziele","title":"Ziele","text":"<p>Mit OpenTrafficCam LIVE Hoyerswerda entwickeln wir ein Reallabor f\u00fcr den Stra\u00dfenverkehr. Damit sollen Verkehrsbewegungen automatisiert, permanent und in Echtzeit erfasst und analysiert werden. Dabei steht nicht nur die reine Z\u00e4hlung Verkehrsteilnehmender im Fokus \u2013 sondern ein tiefes Verst\u00e4ndnis f\u00fcr Verhalten, Nutzungsmuster und Sicherheitsaspekte im Stra\u00dfenverkehr.</p> <p>So soll OpenTrafficCam LIVE Hoyerswerda Forschung und Entwicklung in folgenden Bereichen erm\u00f6glichen:</p> <ul> <li> <p>Automatisierte Verkehrserfassung,</p> </li> <li> <p>Verhaltens- und Sicherheitsanalysen im Stra\u00dfenraum,</p> </li> <li> <p>smartes, situationsabh\u00e4ngiges Verkehrsmanagement sowie,</p> </li> <li> <p>automatisiertes und vernetztes Fahren.</p> </li> </ul> <p>Gleichzeitig soll das System wertvolle Informationen f\u00fcr die Verkehrsplanung und -lenkung in Hoyerswerda liefern, unter anderem in den Bereichen Verkehrsmodellierung und Verkehrssicherheit.</p> <p>Auch f\u00fcr andere St\u00e4dte und Verwaltungen wird OpenTrafficCam LIVE als Open Source-Alternative zu propriet\u00e4ren Anbietern verf\u00fcgbar sein.</p> <p>OpenTrafficCam LIVE soll als offenes System Transparenz schaffen, digitale Souver\u00e4nit\u00e4t st\u00e4rken und f\u00fcr Forschung und Praxis gleicherma\u00dfen nutzbar sein.</p>"},{"location":"live/#umsetzung","title":"Umsetzung","text":""},{"location":"live/#technische-umsetzung","title":"Technische Umsetzung","text":"<p>Der Stra\u00dfenverkehr wird mit OTCamera aufgezeichnet und die Videostreams werden per Mobilfunk (LTE/5G) an das Rechenzentrum der TU Dresden \u00fcbertragen. Dort erfolgen in Echtzeit mit OTVision KI-basierte Detektion und Tracking sowie mit OTAnalytics die Bestimmung von Verkehrskennwerten. Die ermittelten Informationen werden per Web-Dashboard visualisiert und stehen f\u00fcr Forschung, Entwicklung und lokale Verkehrsplanung zur Verf\u00fcgung.</p> <p>Perspektivisch sollen die Live-Informationen f\u00fcr Forschung und Entwicklung im Bereich intelligentes Verkehrsmanagement verwendet werden (zum Beispiel dynamische LSA-Steuerung).</p> <p>Das System im \u00dcberblick</p> <p></p>"},{"location":"live/#untersuchungsbereiche","title":"Untersuchungsbereiche","text":"<p>Das Reallabor umfasst zwei Untersuchungsbereiche in der Neustadt Hoyerswerdas mit 8 Messstellen und 19 Kamerasystemen.</p> <p>Untersuchungsbereiche und Messstellen</p> UntersuchungsbereicheMessstellen Neustadt OstMessstellen Neustadt West <p></p> <p></p> <p>Messstellen:</p> <ul> <li>\ud83d\udea6 Lichtsignalanlage Knotenpunkt Claus-vonStauffenberg-Stra\u00dfe / Maria-Grollmu\u00df-Stra\u00dfe / Erich-Weinert-Stra\u00dfe / K\u00e4the-Niederkirchner-Stra\u00dfe</li> <li>\ud83d\udee3\ufe0f Zufahrt zu dieser Lichtsignalanlage aus Richtung Osten (Maria-Grollmu\u00df-Stra\u00dfe)</li> <li>\ud83c\udd7f\ufe0f Ein-/Ausfahrt des Parkplatzes zum Einkaufscenter \"Treff 8\"</li> <li>\ud83d\ude8c Bushaltestelle \"Klinikum\"</li> <li>\ud83d\udeb6 Fu\u00dfg\u00e4nger-Lichtsignalanlage zwischen \"Lausitzer Seenland Klinikum\" und Einkaufscenter \"Treff 8\"</li> </ul> <p></p> <p>Messstellen:</p> <ul> <li>\ud83d\udea6 Lichtsignalanlage Knotenpunkt B97 Elsterstra\u00dfe / Albert-Einstein-Stra\u00dfe / Alte Berliner Stra\u00dfe</li> <li>\ud83d\ude8c Bushaltestelle \"Albert-Einstein-Stra\u00dfe\"</li> <li>\ud83d\udeb6 Verkehrsinsel \"Albert-Einstein-Stra\u00dfe\"</li> </ul>"},{"location":"live/#projektfortschritt","title":"Projektfortschritt","text":"04/2025: Projektstart <p>Im April 2025 begann das Projekt zur Weiterentwicklung von OpenTrafficCam zu einem Live-System.</p> 06/2025: Kickoff und Testmessung <p>Im Juni 2026 fanden das Kickoff mit der Stadt Hoyerswerda und eine tempor\u00e4re Testmessung im Untersuchungsbereich \"Hoyerswerda Neustadt Ost\" mit 21 Kamerasystemen statt.</p> <p>Impressionen</p> <ul> <li> <p> Anbringung OTCameras </p> </li> <li> <p> Montierte OTCameras </p> </li> <li> <p> Kickoff 1/2 (Quelle: Wochenkurier/Peter Aswendt) </p> </li> <li> <p> Kickoff 2/2 (Quelle: SZ/Juliane Mietzsch) </p> </li> </ul> <p>Presseberichte</p> <p>Radio Lausitz: \"Hoyerswerda wird zum Labor f\u00fcr Verkehrsforscher\"</p> <p>Wochenkurier: \"Smarter Verkehr in Hoyerswerda\"</p> <p>S\u00e4chsische Zeitung: \"Verkehrsforscher \u00fcberwachen die Klinikums-Kreuzung in Hoyerswerda\"</p> <p>Lausitzwelle: \"Kameras beobachten Verkehrslage\"</p> <p> </p> <p>09/2026: Go-Live</p>"},{"location":"live/#beteiligte","title":"Beteiligte","text":"<ul> <li> <p>F\u00f6rdergeber</p> <p>Bundesministerium f\u00fcr Wirtschaft und Energie</p> <p>Land Sachsen</p> <p></p> </li> <li> <p>F\u00f6rdernehmer und Auftraggeber</p> <p>TU Dresden</p> <p>Professur f\u00fcr Mobilit\u00e4tssystemplanung</p> <p></p> </li> <li> <p>Auftragnehmer</p> <p>platomo GmbH</p> <p></p> </li> <li> <p>Kommunaler Partner</p> <p>Stadt Hoyerswerda</p> <p></p> </li> </ul>"},{"location":"team/","title":"Team","text":""},{"location":"team/#entwicklung","title":"Entwicklung","text":"<p>Das Team rund um OpenTrafficCam besteht aus Softwareentwicklern und Ingenieur:innen aus den Bereichen Verkehr und Fotogrammetrie.</p> <p>Die Grundidee zu OpenTrafficCam entstand 2017 w\u00e4hrend der freiberuflichen T\u00e4tigkeiten der platomo Gr\u00fcnder. Der Markt der Verkehrserfassungsger\u00e4te war gepr\u00e4gt von eher geschlossenen und teuren L\u00f6sungen, die den Anforderungen der wissenschaftlichen und planerischen Projekte nicht gen\u00fcgte.</p> <p>Mit dem Raspberry Pi war schon l\u00e4nger eine offene und gut dokumentierte Hardwareplattform verf\u00fcgbar, auf der wir unsere ersten Kameraprototypen aufbauten.</p> <p>Gleichzeitig entwickelte sich die KI-basierte Objekterkennung \u00e4u\u00dferst dynamisch und es entstanden mehrere Open Source Modelle. Diese Modelle haben wir in unsere ersten Softwareprototypen integriert und die entstanden Detektionen mit Trackingalgorithmen kombiniert. F\u00fcr unsere damaligen Projekte haben wir eine Analysesoftware geschrieben, um die passenden Verkehrsdaten zu generieren, aufzubereiten und zu visualisieren.</p> <p>Mit den ersten Prototypen haben wir gemeinsam mit der TU Dresden im mFUND eine F\u00f6rderung beim Bundesministerium f\u00fcr Digitales und Verkehr beantragt und erhalten.</p> <p>Seit 2021 entwickeln wir im Rahmen einer gr\u00f6\u00dferen F\u00f6rderung (auch im mFUND) die OpenTrafficCam zu einem Open Source Verkehrserfassungssystem weiter, welches einerseits frei nutzbar sein soll und andererseits durch zus\u00e4tzliche Dienste das Gesch\u00e4ftsmodell der platomo GmbH bildet.</p> <p>Durch die Kombination von Praxispartner und Forschungseinrichtung k\u00f6nnen wir die Fragestellungen im Laufe der Entwicklung aus unterschiedlichen Sichtweisen beleuchten und l\u00f6sen.</p> <p>OpenTrafficCam wird von der platomo GmbH aus Karlsruhe und der TU Dresden entwickelt.</p> <ul> <li> <p></p> </li> <li> <p></p> </li> </ul>"},{"location":"team/#forderung","title":"F\u00f6rderung","text":"<p>Das Projekt OpenTrafficCam_live wird in der F\u00f6rderlinie mFUND mit insgesamt 973.140 Euro durch das Bundesministerium f\u00fcr Digitales und Verkehr gef\u00f6rdert.</p> <p>\u00dcber den mFUND des BMDV:</p> <p>Im Rahmen der Innovationsinitiative mFUND f\u00f6rdert das BMDV seit 2016 datenbasierte Forschungs- und Entwicklungsprojekte f\u00fcr die digitale und vernetzte Mobilit\u00e4t der Zukunft. Die Projektf\u00f6rderung wird erg\u00e4nzt durch eine aktive fachliche Vernetzung zwischen Akteuren aus Politik, Wirtschaft, Verwaltung und Forschung und durch die Bereitstellung von offenen Daten auf der Mobilithek. Weitere Informationen finden Sie unter www.mfund.de</p> <ul> <li> <p></p> </li> <li> <p></p> </li> </ul> <p>Zudem werden Teile der Weiterentwicklung von OpenTrafficCam von der Deutschen Forschungsgemeinschaft (DFG) gef\u00f6rdert. Konkret erfolgt im Rahmen des Projekts NFDI4ing die Entwicklung von OTGroundTruther,  einem Tool zur manuellen Erzeugung von Referenzdatens\u00e4tzen. Mit diesen Referenzdatens\u00e4tzen k\u00f6nnen die mit OpenTrafficCam automatisiert abgeleiteten Verkehrskennwerte validiert werden.</p> <ul> <li> <p></p> </li> <li> <p></p> </li> </ul>"},{"location":"use-cases/","title":"Use Cases","text":"<p>OpenTrafficCam ist das einzige Open Source Gesamtsystem zur automatischen Analyse von Verkehr. Entwickelt von Informatikern (ja bisher nur M\u00e4nner :() und Verkehrsingenieur:innen f\u00fcr Verkehrsingenieur:innen. #less-sales-more-truth #lessbullshitbingo Die Verkehrsanalyse mit OpenTrafficCam ist in drei aufeinander abgestimmte Module unterteilt. Die Module k\u00f6nnen auch einzeln eingesetzt werden. Das mobile Kamerasystem OTCamera zeichnet datenschutzkonform Verkehrsvideos auf. Die Software OTVision detektiert Verkehrsteilnehmende in Videos von vielen g\u00e4ngigen Verkehrskameras - nicht nur von OTCamera. OTAnalytics analysiert die Bewegungen der detektierten Verkehrsteilnehmenden und aggregiert sie zu verkehrlichen Kenngr\u00f6\u00dfen wie Verkehrsst\u00e4rken, Zeitl\u00fccken, Geschwindigkeiten, \u2026</p>"},{"location":"use-cases/#datenschutz","title":"Datenschutz","text":"<p>Durch bewusste Fokussierung auf den Nahbereich und einen typischerweise gro\u00dfen Abstand zu den Verkehrsteilnehmenden (&gt; 3m) werden Verkehrsteilnehmende nur unscharf aufgezeichnet. In Kombination mit einer geringen Aufl\u00f6sung werden typische Datenschutzprobleme, wie erkennbare Gesichert oder Kennzeichen, gel\u00f6st. Die eingesetzten Modelle zur Detektion der Verkehrsteilnehmenden ber\u00fccksichtigen dies ohne wesentliche Einbu\u00dfen bei der Erfassungsgenauigkeit. Zus\u00e4tzlich kann das Sichtfeld individuell an den aufzuzeichnenden Ort angepasst werden, um nur relevante Bereiche aufzuzeichnen.</p> <p>Tell Me More</p>"},{"location":"use-cases/#kurzzeitzahlung","title":"Kurzzeitz\u00e4hlung","text":"<p>OpenTrafficCam kann zur Kurzzeitz\u00e4hlung an Knotenpunkten und Querschnitten eingesetzt werden. Die Kameras werden an vorhandener Infrastruktur montiert und anschlie\u00dfend Knotenstromz\u00e4hlungen und Querschnittsz\u00e4hlungen durchgef\u00fchrt. Der automatisierte Prozess ersetzt oder erg\u00e4nzt geschultes Z\u00e4hlpersonal. Die Z\u00e4hldaten werden in gut strukturierten und \u00fcbersichtlichen CSV- und Excel-Datens\u00e4tzen exportiert. Dabei l\u00e4sst sich die zeitliche Aggregierung frei einstellen (z.B. 5 oder 15 Minuten). Das erm\u00f6glicht eine einfache Visualisierung der Daten und eine effiziente Integration in bestehende Prozesse der Ingenieurb\u00fcros und Kommunen. Mit den offenen Modellen lassen sich Fu\u00df-, Rad-, Leicht- und Schwerverkehr unterscheiden. Mit den Pro-Modellen von platomo wird die Erkennungsgenauigkeit- und rate deutlich verbessert und es werden insgesamt 18 verschiedene Klassen erkannt. Diese lassen sich z.B. zu Klassen nach RLS-19 oder TLS-2012 8+1 Klassen (inkl. Fu\u00df- und Radverkehr) zusammenfassen. OpenTrafficCam kann auch f\u00fcr Videos verwendet werden, die nicht mit OTCamera aufgezeichnet wurden.</p> <p>Tell Me More</p>"},{"location":"use-cases/#geschwindigkeitsmessung","title":"Geschwindigkeitsmessung","text":"<p>Unser Produkt basiert auf und ist Open Source, denn wir glauben an Offenheit, Zusammenarbeit und Innovation. Durch die Verwendung von Open Source-Technologien erm\u00f6glichen wir nicht nur eine transparente Entwicklung, sondern f\u00f6rdern auch die Teilnahme der Gemeinschaft. Dies schafft Vertrauen, Sicherheit und erm\u00f6glicht individuelle Anpassungen. Mit Open Source gestalten wir eine kollaborative und zukunftsorientierte L\u00f6sung f\u00fcr unsere Nutzenden.</p> <p>Tell Me More</p>"},{"location":"OTAnalytics/","title":"Features","text":"<p>The brain of OpenTrafficCam.</p> <p>OTAnalytics is a set of self developed algorithms to gather traffic measures based on trajectories of road users.</p>"},{"location":"OTAnalytics/#key-features","title":"Key features","text":"<ul> <li>Generation of traffic flow matrices and node flow diagrams</li> <li>Measurement of time gaps, speeds and accelerations</li> <li>Long-term: Analysis of near-accidents (e.g. TTC, PET)</li> <li>Visualization of traffic data</li> <li>Import of trajectories from other systems (e.g. DataFromSky)</li> <li>Runs on any Windows laptop</li> </ul>"},{"location":"OTAnalytics/#content-of-documentation","title":"Content of documentation","text":"<p>Most of the code will be developed by the OpenTrafficCam contributors (based on python libraries like NumPy, pandas, tkinter and OpenCV).</p> <p>Early prototype</p> <p>The currently available version of OTAnalytics described in this documentation is in early prototype stage. This version works and offers an impression of the functionality. \u00a0</p> <p>It is partially tested on Windows 10.</p> <p>We are currently working on a complete re-implementation for a stable release. It will also be tested on Mac and Linux.</p> <p>Stay tuned :)</p>"},{"location":"OTAnalytics/installation/","title":"Installation","text":"<p>READ THIS BEFORE YOU INSTALL AND USE THE SOFTWARE</p> <p>License</p> <p>This software is licensed under the GNU General Public License Version 3 (GNU GPL-3.0).</p> <p>A full copy of the license is included in the code repository as LICENSE file and can also be viewed online: LICENSE file on Github.</p> <p>Please respect this license!</p> <p>Disclaimer</p> <p>Although OTAnalytics has been widely used in many transportation-related projects, it is still under active development. Features and functionality may change at any time. Particularly the user interface and the data formats should not be considered stable.</p> <p>This software is provided \"as is,\" without any express or implied warranties or liabilities. Use is at your own risk. Neither the authors nor contributors assume any responsibility for damages resulting from the use of this software. For more details, refer to Sections 15 and 16 of the GNU GPL-3.0.</p>"},{"location":"OTAnalytics/installation/#hardware-requirements","title":"Hardware requirements","text":"<p>OTAnalytics runs on most modern desktop pcs and laptops (e.g. with an i5 processor and 8 GB of RAM).</p> <p>Please note:</p> <ul> <li>More RAM allows you to load more tracks of road users simultaneously.</li> <li>A faster processor will improve the runtime of the analyses and make the user interface more responsive.</li> </ul>"},{"location":"OTAnalytics/installation/#install-python-311","title":"Install Python 3.11","text":"<p>If not done yet, visit www.python.org/downloads/ and install Python 3.11.x for your platform.</p> <p>Please make sure to include at least the following boxes:</p> WindowsmacOS / Linux <ul> <li>Add Python to PATH</li> <li>pip</li> <li>tcl/tk</li> <li>Add python to environment variables</li> <li>Precompile standard library</li> </ul> <ul> <li>GUI applications</li> <li>UNIX command-line tools</li> <li>Shell profile updater</li> <li>Install or upgrade pip</li> </ul>"},{"location":"OTAnalytics/installation/#install-otanalytics","title":"Install OTAnalytics","text":"<p>Visit OTAnalytics latest releases on GitHub,  scroll down to the list of assets and download the correct \"otanalytics\" <code>.zip</code> file for your platform.</p> <p>Unzip the file to a location of your choice (preferably local).</p> <p>Open a terminal window (or command prompt), navigate to the unzipped folder and run the install script as follows:</p> WindowsmacOS / Linux <pre><code>.\\install.cmd\n</code></pre> <pre><code>sh ./install.sh\n</code></pre> <p>Wait until the installation of dependencies is complete - this could take some time.</p>"},{"location":"OTAnalytics/installation/#run-otanalytics","title":"Run OTAnalytics","text":"<p>If you left the terminal window, open it and navigate to the unzipped folder, again. Run the start script as follows:</p> WindowsmacOS / Linux <pre><code>.\\start_gui.cmd\n</code></pre> <pre><code>sh ./start_gui.sh\n</code></pre>"},{"location":"OTAnalytics/installation/#what-else","title":"What else?","text":"<p>If you encounter problems, have questions or like to request features, please open an issue on GitHub.</p> <p>We welcome code contributions (e.g. fixing bugs or adding features) by the community. If you consider contributing, please check the contribute section of this documentation first.</p> <p>If you like to receive professional support, please have a look at our services or contact us for more information.</p>"},{"location":"OTAnalytics/otconfig/","title":"OTConfig","text":"<p>OTConfig is a program for configuring video-based traffic analysis (e.g. traffic counts, speed measurements) with the OpenTrafficCam pipeline (OTVision and OTAnalytics). It has a subset of the functionality OTAnalytics has. The configuration consists of three steps:</p> <ol> <li>Project setup</li> <li>Definition of the traffic flows to be analyzed</li> <li>Export of the configuration file</li> </ol> <p>The results of the OpenTrafficCam pipeline are raw data on the individual traffic movements as well as count values for the individual traffic flows. Further evaluations, such as speed or time requirement analyses, are carried out in downstream processes upon customer request. The relevant information is transmitted individually and any desired aggregations of vehicle classes are agreed individually and subsequently implemented by platomo.</p> <p>Traffic flows are defined by using detectors (sections). Each flow consists of exactly one start and one end section (from-to relationship). Sections can be defined in the form of lines with any number of support points as well as areas.</p> <p>Info</p> <p>A separate configuration must be created for each camera view. Due to the different positions of the sections and flows, videos with different views cannot be configured at the same time.</p>"},{"location":"OTAnalytics/otconfig/#why-otconfig","title":"Why OTConfig?","text":"<p>If the configuration and execution of the OpenTrafficCam pipeline (processing or analysis) are not to be carried out in the same step or in the same institution, it must be ensured that the configured pipeline is also executed later as desired. For this purpose, we have developed the OTConfig tool for our customers to make the external processing of videos through our infrastructure as efficient and smooth as possible.</p> <p>With OTConfig, the customer can specify all the information required for execution and preconfigure the OpenTrafficCam pipeline accordingly. This information is then saved in an otconfig file. This ensures that all relevant information is transferred to us in a structured manner and applied correctly in the OpenTrafficCam pipeline.</p> <p>Info</p> <p>The transmitted information is validated again by trained personnel before each execution of the pipeline and, if necessary, checked for plausibility. In particular, the position of the sections is optimized to ensure the best possible result.</p>"},{"location":"OTAnalytics/usage-ui/","title":"Usage GUI","text":"<p>After processing videos with OTVision\u00b4s tracking module (OTVision track), you can use OTAnalytics for analysing traffic data. The OTAnalytics Graphical User Interface (GUI) enables you to configure analyses, extract valuable insights from your data, and export the results efficiently.</p>"},{"location":"OTAnalytics/usage-ui/#terminology","title":"Terminology","text":"<p>Vehicles and pedestrians are detected in the individual frames of the video using detect in OTVision. In a single frame of a video, each detected object (Detection) is represented by a Bounding Box,  which is the rectangular area surrounding the detected vehicle or pedestrian together with its classification (e.g., car, bike, pedestrian).</p> <p>The tracker in OTVision links consecutive detections, or bounding boxes, to form the Track of a Road User.</p> <p>A track is therefore composed of a series of detections, each with its own bounding box and classification. Ideally, all detections within a track share the same classification. However, in practice, a track may include detections with varying classifications (e.g., truck and truck_with_trailer). This typically occurs when a road user is poorly visible in some of the frames (far away, partially obscured). OTAnalytics assigns a single Track Classification to each track, as a road user can have only one classification in the real world (e.g., a vehicle is either a car or a bus).</p> <p>A track's trajectory is represented by a representative point from each bounding box associated with the track. This point is called the Track Point. The trajectory is formed by connecting the track points collected across successive frames.</p> <p>Offset examples</p> 0.1, 0.10.8, 0.3 <p></p> <p></p> <p>The position of the track point within the bounding box can be configured using the Offset attribute. This offset is defined separately for the x- and y-axes, with values ranging from <code>0</code> to <code>1</code>. Origin <code>(0, 0)</code> is the top left-hand corner of the bounding box. These values determine the relative position of the track point within the bounding box, providing flexibility in track point placement.</p> <p>To analyze the tracks, OTAnalytics provides Sections and Flows. Imagine sections as the digital equivalent of physical detectors used for traffic measurement, sometimes, also called virtual inductive loops (not to be confused with the detection of bounding boxes in OTVision). Each time a track's trajectory intersects a section, an Event is generated. Each event contains detailed information, including the track's identifier, its position in the frame, and the timestamp of the intersection.</p> <p>Flows define traffic flows to be analyzed. As explained in Flows, a flow is defined by two sections: a starting section and an ending section. Tracks that intersect both sections of a flow in the defined order can be assigned to that flow. To achieve this, the events belonging to a track are sorted chronologically based on their time of occurrence. The assignment of a track to a flow is called Track Assignment.</p> <p>Info</p> <p>The pair of events with the longest time interval between them is used to assign the track to the flow corresponding to the sections of both events.</p>"},{"location":"OTAnalytics/usage-ui/#user-interface","title":"User Interface","text":"<p>The user interface is divided into four main areas: the Configuration Bar, Workspace, Visualization Layers, and Visualization Filters.</p> <p>The Configuration Bar consists of four parts:</p> <ol> <li>Project: Manage project metadata and save or load <code>.otconfig</code> or <code>.otflow</code> files.</li> <li>Tracks/Video: Add input files, where only track files can be added, while video files can be both added and removed. An overview of loaded video files is also displayed.</li> <li>Sections/Flows: Create, edit, and delete sections and flows. The flow names are shown with numbers indicating the number of assigned tracks per flow.</li> <li>Analysis: Export of analyzed traffic data.</li> </ol> <p>The Workspace consists of the Canvas and the Track Statistics. The canvas displays the tracks and video frames. It is also where the geometry of sections can be created or edited. Below the workspace, the Track Statistics provides key statistics about the displayed tracks.</p> <p>By choosing the Visualization Layers, that are located right to the workspace, one can customize how tracks are displayed.</p> <p>With the Visualization Filters located below the workspace, one can adjust which tracks are displayed.</p> <p></p>"},{"location":"OTAnalytics/usage-ui/#setup-the-project","title":"Setup the project","text":"<p>To make the handling and reproducibility of analyses easier, all related data is organized within a project. Setting up a project ensures that all necessary information, such as videos, tracks, and configurations, is stored in one centralized place. This simplifies workflows and maintains consistency across analyses.</p>"},{"location":"OTAnalytics/usage-ui/#enter-project-information","title":"Enter project information","text":"<p>The project must first be named. The project name is entered in the corresponding Name field.</p> <p>Info</p> <p>It is recommended to choose a unique name (e.g. name of the measuring point, camera number; or a combination of several unique details).</p> <p>The start time (date and time) of the first video must then be entered in the corresponding Start date fields. It is possible to enter in ISO 8601 format (YYYY-MM-DD) or German date format (DD.MM.YYYY).</p> <p>Info</p> <p>OTAnalytics assumes that all selected videos are contiguous in time. If the videos to be processed have a time gap (e.g., videos from three days from 6:00 a.m. to 10:00 p.m.), several projects with chronologically contiguous videos must be created.</p>"},{"location":"OTAnalytics/usage-ui/#load-track-files","title":"Load track files","text":"<p>To analyze tracks in OTAnalytics, you must load the track files (.ottrk). This can be done using the Add tracks... button, which opens a file browser to select the respective files. Multiple track files can be added at once. If a corresponding video file exists, OTAnalytics will automatically load it alongside the track file. If no associated video is found, the track file cannot be loaded.</p> <p>Alternatively, you can add videos only. This is particularly useful for configuring an analysis handled by the platomo team. To add videos, use the Add... button, and to remove them from the configuration, use the Remove button.</p> <p>Tip</p> <p>Added videos are displayed in alphabetical order in the \u201cVideos\u201d overview panel and are later processed in this order. The videos should therefore be named in such a way that the chronological order matches the alphabetical order. Additional zero-padding (e.g., <code>01</code>, <code>02</code>) is helpful if you want to ensure correct alphabetical sorting.</p> <p>When you click a video in the configuration bar, the first image of the video is displayed as the background image. This allows you to check whether all videos have been selected correctly.</p>"},{"location":"OTAnalytics/usage-ui/#save-and-open-the-project","title":"Save and open the project","text":"<p>Once you added all required tracks and videos and created all sections and flows, the project can be saved.</p> <ol> <li>Click on the Save as... button in the Project section of the configuration bar.</li> <li>Select a suitable filename in the file browser and save it in the same folder as the videos.</li> </ol> <p>You can save the project in two ways:</p> <p>An <code>.otconfig</code> file stores all information necessary to reload the project in OTAnalytics. This is the default option.</p> <p>An <code>.otflow</code> file stores only information about sections and flows. Hence, it can be reused / imported into another project with different videos of the same measurement site.</p> <p>Tip</p> <p>We recommend regularly saving the progress of the project while it is still being processed. This prevents possible loss of data. The Save button automatically saves the file to the last selected location. If it is colored in orange, changes have been made.</p> <p>We also recomment to store all project related files (videos, <code>.otdet</code>, <code>.ottrk</code> and <code>.otconfig</code>) in the same folder.</p> <p>When you return later, you can reopen the project using the Open ... button.</p>"},{"location":"OTAnalytics/usage-ui/#configure-traffic-analysis","title":"Configure traffic analysis","text":"<p>To perform traffic analyses with OTAnalytics, sections and flows are needed. A traffic flow depicts the directional travel relationship between two sections. Each flow always consists of two sections: A start section and an end section. Before defining flows, you must create the sections. The flows can then be assigned to the sections.</p>"},{"location":"OTAnalytics/usage-ui/#sections","title":"Sections","text":"<p>Sections can consist of two or more support points (shown as a circle). They are drawn directly on the background image in OTAnalytics. To do this, the Sections tab must first be selected in the Sections/Flows section.</p> <p>Tip</p> <p>Both line sections and area sections can be created. If no occupancy durations (e.g., of parking areas) are analyzed, we recommend using line sections.</p> <p>The following example is limited to line sections. However, the procedure described can also be applied to area sections. The only difference is that the polygon is automatically closed when the add mode is exited.</p>"},{"location":"OTAnalytics/usage-ui/#add-sections","title":"Add sections","text":"<p>A new line section is added in the following steps:</p> <ol> <li>Left-click on the Add line button, which starts the add mode.</li> <li>Set the first point by left-clicking at the correct position in the video image. The point is now fixed. Further points can be added to the section by moving the mouse and left-clicking again.    A section must consist of at least two points.</li> <li>Once the desired length and shape of a section has been reached, right-click or press the Enter key to exit Add mode.    (Pressing the Escape key cancels add mode without saving the previously created section.)</li> <li>A pop-up window opens. Enter the name for the section in this window and confirm. The name is reused in the analysis.</li> <li>The created section appears with the assigned name in the sections part in the configuration bar.</li> <li>Repeat the process to add further sections.</li> </ol> <p>Warning</p> <p>A name can only be assigned once, duplicate names of several sections are not possible. We recommend including the approximate compass direction of the geographical location (e.g. north, north-east) in the name of the section.</p> <p></p>"},{"location":"OTAnalytics/usage-ui/#select-sections","title":"Select sections","text":"<p>To select a specific section, click on its name in the list of sections. The selected section is now highlighted both in the list and on the canvas.</p> <p>You can also select multiple sections with left-click + ctrl on Windows or cmd on macOS. Deselect a single with right-click + ctrl on Windows or cmd on macOS or deselect all sections with right-click into a empty place in the list.</p>"},{"location":"OTAnalytics/usage-ui/#change-the-geometry-of-a-section","title":"Change the geometry of a section","text":"<p>The geometry of sections that have already been created can be changed using the following procedure:</p> <ol> <li>First select the section to be changed in the list.</li> <li>Then start the change mode by left-clicking on the Edit button. The supporting points are now visible.</li> <li>Support points can now be selected by left-clicking and reset as in add mode.</li> <li>The + button can be used to add another support point.</li> </ol> <p>The selected point \u201csticks\u201d to the mouse pointer until the left mouse button is pressed. The old geometry is displayed as a dashed reference.</p> <ol> <li>Right-click to exit change mode and save the changes.</li> <li>Pressing the Escape button exits the change mode without saving the changes to the geometry.</li> </ol>"},{"location":"OTAnalytics/usage-ui/#change-attributes-of-a-section","title":"Change attributes of a section","text":"<p>The attributes (name and offset) of a section can be changed by clicking on the Properties button. To do this, the desired section must first be selected from the list.</p> <p>The bounding box offset defines, which reference point of the bounding boxes of the track is used to trigger an event.</p> <p>Tip</p> <p>Since the bounding box offset can be individually configured for each section, tracks intersecting different sections can be finely tuned. This helps to increase the recording accuracy of the section or to fulfil special use cases (e.g. if the front of a road user is to be recorded).</p>"},{"location":"OTAnalytics/usage-ui/#remove-sections","title":"Remove sections","text":"<p>A section can be removed by clicking on the Remove button. To do this, the desired section must be removed from any flows and it must be selected in the list. You can use multiselect to remove multiple sections at once.</p>"},{"location":"OTAnalytics/usage-ui/#flows","title":"Flows","text":"<p>A flow always consists of exactly two sections: a start section and an end section. To edit flows, the Flows tab must first be selected in the Sections/Flows section.</p>"},{"location":"OTAnalytics/usage-ui/#add-flows","title":"Add flows","text":"<p>A new flow is added in the following steps:</p> <ol> <li>Left-click on the Add button. A pop-up window opens. The appropriate sections can be selected from the drop-down menu.</li> <li>The start point of the flow is defined as the first section.</li> <li>The endpoint of the flow is defined as the second section.</li> <li>The name is automatically set in the Name field after selecting the two sections. This can be changed as required. The name is reused in the analysis.</li> <li>If you plan to perform speed measurements, you can enter the distance between the sections (which must be measured independently of OTAnalytics)</li> </ol> <p>Warning</p> <p>A name can only be assigned once, duplicate names of several flows are not possible.</p> <p></p> <p>Tip</p> <p>Entering flows for many sections can be tedious. Fortunately, OTAnalytics can automatically generate flows for all possible combinations of start and end sections. Just hit the Generate button. The naming convention for the flows is <code>{Name of start section} --&gt; {Name of end section}</code>.</p>"},{"location":"OTAnalytics/usage-ui/#select-flows","title":"Select flows","text":"<p>To select a specific flow, click on its name in the list of flows. The selected flow is now highlighted both in the list and on the canvas (by an arrow indicating its direction).</p> <p>You can also select multiple flows with left-click + ctrl on Windows or cmd on macOS. Deselect a single with right-click + ctrl on Windows or cmd on macOS or deselect all sections with right-click into a empty place in the list.</p>"},{"location":"OTAnalytics/usage-ui/#edit-a-flow","title":"Edit a flow","text":"<p>To edit a flow, it must first be selected in the list. Then left-click on the Properties button to open the same pop-up window as when creating a flow. The desired changes can now be made.</p>"},{"location":"OTAnalytics/usage-ui/#remove-flows","title":"Remove flows","text":"<p>To remove a flow, it must first be selected in the list. Left-click on the Remove button to remove the selected flow. You can use multiselect to remove multiple flows at once.</p>"},{"location":"OTAnalytics/usage-ui/#visualization-layers","title":"Visualization layers","text":"<p>Utilizing the different visualization layers helps you to explore the tracks and to refine traffic analysis by offering specific views of the data. Tracks are consistently displayed in the same colors, which are determined by their classification. The visualization layers are organized into groups.</p> <p>Warning</p> <p>Some visualization layers are not updated automatically, when you change the configuration of the project or the sections/flows.</p> <p>If the visualization is outdated, just hit the Update flow highlighting button - calculations will be run and visulalization will be updated.</p>"},{"location":"OTAnalytics/usage-ui/#background","title":"Background","text":"<p>This layer shows a frame of the video as a background image. The currently shown frame can be configured using the filter. If a date/time filter is set, the frame at the end of the range is shown. Otherwise, the first frame of the selected video is shown.</p>"},{"location":"OTAnalytics/usage-ui/#show-tracks","title":"Show tracks","text":"<p>This group of layers offers options to control which tracks are displayed based on their intersection with sections or their assignment to flows.</p> <p>They enable you to check the position of the sections and the definition of the flows and to optimise them iteratively. Checking, if the sections \"catch\" the correct tracks is a crucial step to ensure a good quality of each traffic analysis.</p> AllIntersecting sectionsNot intersecting sectionsAssigned to flowsNot assigned to flows <p>All tracks are shown.</p> <p></p> <p>Tracks intersecting at least one of the selected sections are shown.</p> <p></p> <p>Tracks intersecting none of the selected sections are shown.</p> <p></p> <p>Tracks assigned to at least one selected flow are shown.</p> <p></p> <p>Tracks assigned to none of the selected flows are shown.</p> <p></p> <p>Tip</p> <p>When checking and optimising sections and flows, also have a look at the track statistics shown below the canvas.</p>"},{"location":"OTAnalytics/usage-ui/#show-start-and-endpoints","title":"Show start- and endpoints","text":"<p>In this group of layers, you can decide if the start- and endpoints of tracks are highlighted. Like in Show tracks, you can display the start- and endpoints based on their intersection with sections or their assignment to flows.</p> <p>They enable you, for example, to check whether some tracks are interrupted and where this may be clustered.</p> AllIntersecting sectionsNot intersecting sectionsAssigned to flowsNot assigned to flows <p>Start- and endpoints of all tracks are highlighted.</p> <p></p> <p>Start- and endpoints of tracks intersecting at least one of the selected sections are highlighted.</p> <p></p> <p>Start- and endpoints of tracks intersecting none of the selected sections are highlighted.</p> <p></p> <p>Start- and endpoints of tracks assigned to at least one selected flow are highlighted.</p> <p></p> <p>Start- and endpoints of tracks assigned to none of the selected flows are highlighted.</p> <p></p>"},{"location":"OTAnalytics/usage-ui/#show-detections-of-current-frame","title":"Show detections of current frame","text":"<p>These layers visualize all detections of a single frame either using a bounding box or using the track point (with the current offset).</p> <p>It can be helpful for an in-depth assessment of the quality of samples of detections and tracks.</p> Bounding boxTrack point <p>The bounding boxes of all detections in the currently visible frame are shown.</p> <p></p> <p>The track points of all detections in the currently visible frame are shown.</p> <p></p>"},{"location":"OTAnalytics/usage-ui/#show-events","title":"Show events","text":"<p>This group of layers highlights the locations of events triggered by the intersection of tracks with sections in a given period or at a given moment.</p> <p>Like the other layers, they provide in-depth data validation. Additionally, the event locations themselves can serve as an interesting focus for traffic analysis.</p> <p>Info</p> <p>The locations of the events are determined based on the bounding box offset configured locally for each individual section.</p> <p>In contrast, a global offset can be set for visualising the tracks.</p> <p>As a result, the event locations may not align with the corresponding tracks.</p> <p>To align the tracks, select a specific section and hit the Update with section offset button.</p> Current filterCurrent frame <p>The locations of all events within the date/time filter range are shown.</p> <p></p> <p>All events present at the moment of the current frame are shown.</p> <p></p>"},{"location":"OTAnalytics/usage-ui/#visualization-filters","title":"Visualization filters","text":"<p>As the number of analyzed video hours grows, distinguishing individual tracks in the visualization becomes increasingly difficult. Filters enable you to reduce the number of displayed track and the part of the track which is shown, making the visualization more focused and easier to manage.</p> <p>Info</p> <p>Filters are applied only to the visualization in the workspace and to the calculation of track statistics displayed below. Analyses and data exports always include all loaded tracks, regardless of any applied filters.</p>"},{"location":"OTAnalytics/usage-ui/#filter-by-time","title":"Filter by Time","text":"<p>Filter tracks based on time and date. Only track points that are within the date/time range will be displayed.</p> <p>You can toggle the filter by checking/unchecking the box left to the Filter by date button. By clicking the Filter by date button, a popup appears where you can enter a custom date/time range.</p> <p>The &lt; and &gt; buttons to the right of the Filter by date button allow you to shift the filter forward or backward by its current duration. For more precise adjustments, use the &lt; and &gt; buttons below to fine-tune the filter's position (Seconds, Frames, Events).</p> <p></p>"},{"location":"OTAnalytics/usage-ui/#filter-by-classification","title":"Filter by Classification","text":"<p>Filter tracks based on their classification (e.g., car, bicyclist).</p> <p>You can toggle the filter by checking/unchecking the box left to the Filter by classification button. By clicking the Filter by classification button, a popup appears where you can select the track classes to be displayed.</p>"},{"location":"OTAnalytics/usage-ui/#analysis-exports","title":"Analysis exports","text":"<p>After all sections and flows have been configured, you can choose an analysis for export.</p> <p>Tip</p> <p>We highly recommend using the different visualization layers and the track statisics below the canvas before exporting analyses. This is how you gain detailed insights into the data and can optimize the configuration. It is important to ensure a good quality of your exports.</p>"},{"location":"OTAnalytics/usage-ui/#traffic-counts","title":"Traffic counts","text":"<p>This is the most commonly used metric for analysing traffic, supporting various use cases such as planning, simulation, and safety analysis.</p> <p>The traffic counts are derived from the assignment of tracks to flows. They can be exported as a <code>.csv</code> file, which can be easily imported into your preferred software. You can specify the start and end date and time, as well as the interval for aggregating traffic counts.</p> <p></p> <p>The <code>.csv</code> file includes the following values:</p> Value Datatype Description Example start time string Timestamp indicating the start of the interval 2023-05-24 11:45:00 start occurrence date string Date at the start of the interval 2023-05-24 start occurrence time string Time at the start of the interval 11:45:00 end time string Timestamp indicating the end of the interval 2023-05-24 12:00:00 end occurrence date string Date at the end of the interval 2023-05-24 end occurrence time string Time at the end of the interval 12:00:00 classification string The classification of the track / road user (e.g., car, bicyclist) bicyclist flow string Direction of traffic flow, specifying origin and destination sections south --&gt; east from section string Name of the section where the track originated south to section string Name of the section where the track was directed east count int Number of tracks / road users recorded during the interval 1"},{"location":"OTAnalytics/usage-ui/#event-list","title":"Event list","text":"<p>Traffic events provide a more granular data source for in-depth use cases, such as speed measurements, occupancy durations, or time gap analysis.</p> <p>The events (tracks intersecting sections) can be exported into a single file in one of the following formats:</p> <ul> <li><code>.csv</code> contains the event data in a format that can be read by many software products.</li> <li><code>.xslx</code> includes the event data along with additional information about the sections used to generate the events for further analysis in MS Excel.</li> <li><code>.otevents</code> is a bzip2-compressed JSON file that contains the event data, additional information about the sections used to generate the events, and metadata collected during processing.</li> </ul> <p>The export includes the following values for each event:</p> Value Datatype Description Example road_user_id string Unique identifier for the track / road user 5f8cd584-f490-4fec-afd0-b55ebf39ab4e#0#102341 road_user_type string Track classification of the road user (e.g., car, pedestrian) car hostname string Name of the camera or device capturing the data OTCamera19 occurrence string Timestamp of the event in date and time format 2023-05-24 11:45:00.000000 frame_number int Frame number of the video corresponding to the event 1 section_id string (optional) Identifier for the section. Not available for the event types enter-scene and leave-scene 1 event_type string Type of event (enter-scene, leave-scene, enter-section, leave-section) enter-section video_name string Name of the video file where the event was recorded OTCamera19_FR20_2023-05-24_11-45-00.mp4 occurrence_sec timestamp Time of the event in seconds since the epoch 1684928700.0 coordinate_px_x float X-coordinate of the road user in pixels 142.60128784179688 coordinate_px_y float Y-coordinate of the road user in pixels 199.48948669433594 vector_px_x float X-component of the road user's movement vector -0.2536773681640625 vector_px_y float Y-component of the road user's movement vector 0.075958251953125 section_name string (optional) Name of the section where the event occurred. Not available for the event types enter-scene and leave-scene north occurrence_day string Day of the event in date format 2023-05-24 occurrence_time string Time of the event in time format 11:45:00.000000"},{"location":"OTAnalytics/usage-ui/#road-user-assignments","title":"Road User Assignments","text":"<p>Use this export in case no aggregation of tracks to traffic counts is wanted, but the assignment strategy of OTAnalytics should be used.</p> <p>All tracks assigned to flows can be exported using the Export road user assignments... Button. The tracks assigned to flows are exported into a <code>.csv</code> file with the following format:</p> Value Datatype Description Example flow_id int Unique identifier for the flow 16 flow_name string Name of the flow north --&gt; east road_user_id string Unique identifier for the track / road user 5f8cd584-f490-4fec-afd0-b55ebf39ab4e#0#102708 max_confidence float Maximum confidence score for the road user's track classification 0.9151925444602966 start_occurrence timestamp Timestamp indicating the starting time of the track 2023-05-24 11:45:07.600000 start_occurrence_day string Date indicating the start of the track 2023-05-24 start_occurrence_time string Time indicating the start of the track 11:45:07.600000 end_occurrence timestamp Timestamp indicating the ending time of the track 2023-05-24 11:45:09.600000 end_occurrence_day string Date indicating the end of the track 2023-05-24 end_occurrence_time string Time indicating the end of the track 11:45:09.600000 start_frame_number int Frame number corresponding to the start of the track 153 end_frame_number int Frame number corresponding to the end of the track 193 start_video_name string Name of the video file where the start of the track was recorded OTCamera19_FR20_2023-05-24_11-45-00.mp4 end_video_name string Name of the video file where the end of the track was recorded OTCamera19_FR20_2023-05-24_11-45-00.mp4 start_section_id int Identifier for the section where the track originated 3 end_section_id int Identifier for the section where the track ended 2 start_section_name string Name of the section where the track started north end_section_name string Name of the section where the track ended east start_coordinate_px_x float X-coordinate of the road user at the start of the track, in pixels 718.1949096679688 start_coordinate_px_y float Y-coordinate of the road user at the start of the track, in pixels 301.2111328125 end_coordinate_px_x float X-coordinate of the road user at the end of the track, in pixels 775.1578369140625 end_coordinate_px_y float Y-coordinate of the road user at the end of the track, in pixels 326.5100158691406 start_vector_px_x float X-component of the road user's movement vector at the start of the track -0.54376220703125 start_vector_px_y float Y-component of the road user's movement vector at the start of the track 1.6538543701171875 end_vector_px_x float X-component of the road user's movement vector at the end of the track 0.179168701171875 end_vector_px_y float Y-component of the road user's movement vector at the end of the track 2.1141113281249773 hostname string Name of the camera or device capturing the road user OTCamera19"},{"location":"OTAnalytics/usage-ui/#track-statistics","title":"Track Statistics","text":"<p>This is a CSV export of the track statistics shown in the workspace below the camera image.</p>"},{"location":"OTAnalytics/usage-ui/#what-else","title":"What else?","text":"<p>If you encounter problems, have questions or like to request features, please open an issue on GitHub.</p> <p>We welcome code contributions (e.g. fixing bugs or adding features) by the community. If you consider contributing, please check the contribute section of this documentation first.</p> <p>If you like to receive professional support, please have a look at our services or contact us for more information.</p>"},{"location":"OTAnalytics/Accuracy/counts/","title":"Traffic counts","text":""},{"location":"OTAnalytics/Accuracy/counts/#why-is-a-high-quality-of-the-counts-so-important","title":"Why is a high quality of the counts so important?","text":"<p>One of the main goals of OpenTrafficCam is automatic traffic counting at road sections and intersections. However, the results of such counts are only suitable for mobility planning if they are of high quality. In the worst case, traffic facilities planned based on inaccurate count data can waste public resources and even endanger the health of their users.</p>"},{"location":"OTAnalytics/Accuracy/counts/#how-to-measure-the-quality-of-the-counts","title":"How to measure the quality of the counts?","text":"<p>Precision and Recall, which are used to evaluate machine learning algorithms, can similarly be used as meaningful metrics to evaluate the quality of automated traffic counts. The precision metric describes the ratio of vehicles counted correctly by OpenTrafficCam to all vehicles counted by OpenTrafficCam including incorrect ones. Whereas the recall metric describes the ratio of vehicles correctly counted by OpenTrafficCam to all vehicles passed in reality including those not counted by OpenTrafficCam.</p> <p>An Example:</p> <ul> <li>100 vehicles pass through a junction</li> <li>80 of them are correctly detected by the system (true positive)</li> <li>20 are therefore not correctly detected (false negative)</li> <li>the system indicates that 110 vehicles passed the intersection</li> <li>the system has thus detected 30 vehicles that were not present in reality (false positive)</li> </ul> 110 got detected 20 got not detected 100 vehicles passed 80 (true positive) 20 (false negative) 30 are not real 30 (false positive) -- (true negative) <p>The prescision (or accuracy) is equal to true positive / (true positive + false positive). In this case the prescision would be 80 / (80 + 30) = 0.73. The recall (or sensitivity) is equal to true positive / (true positive + false negative). In this case the recall would be 80 / (80 + 20) = 0.80. The higher the two measurements are, the higher the quality of the results are.</p>"},{"location":"OTAnalytics/Accuracy/counts/#how-accurate-can-you-count-using-opentrafficcam","title":"How accurate can you count using OpenTrafficCam?","text":"<p>In his diploma thesis OpenTrafficCam contributor Armin Kollascheck investigated the quality of counting motorized traffic that got generated by OpenTrafficCam using YOLOv5x6 (Ultralytics), and a modified version of the IOU-Tracker (Bochinski et al.) for the following eight realistic traffic scenarios.</p> Three-way intersection Four-way intersection (1) Four-way intersection (2) Four-way intersection (2) at night Four-way roundabout Section (1) Section (2) Section (2) at night <p>Armin evaluated the count data automatically generated with OpenTrafficCam against a manually collected and verified ground truth. The sample sizes for determining the precision and recall of the automated traffic counts are 200 vehicles for each scenario. The ground truth of 200 vehicles was always counted manually. The results are shown in the table below.</p> Traffic facility Lighting Camera angle Precision Recall Three-way intersection Day Steep 0.95 0.85 Four-way intersection (1) Day Steep 1.00 0.94 Four-way intersection (2) Day Steep 0.99 0.95 Four-way intersection (2) Night with street lights Steep 0.98 0.95 Four-way roundabout Day Flat 0.63 0.56 Section (1) Day Steep 1.00 0.91 Section (2) Day Steep 1.00 0.97 Section (2) Night without street lights Steep 0.06 0.12"},{"location":"OTAnalytics/Accuracy/counts/#conclusion","title":"Conclusion","text":"<p>The high values for precision and recall in many scenarios show that OpenTrafficCam already works quite well with the official YOLOv5x6 model and a rather simple tracker. Thus, on different traffic facilities, counting qualities were achieved that probably exceed those of manual on-site counts. And even video recordings at night with street-side lighting seem to be counted correct with OpenTrafficCam.</p> <p>Unsatisfactory results, however, occurred with comparatively flat camera angles or night recordings without street lighting. This shows that the current pipeline is not yet perfect. We aim to further improve OpenTrafficCam to achieve satisfying counting results even under very difficult circumstances.</p>"},{"location":"OTCamera/","title":"Features","text":"<p>The eye of OpenTrafficCam.</p> <p>OTCamera is a mobile camera system for capturing traffic videos.</p> <p>It is intended for DIY enthusiasts. You don't want to build OTCamera yourself or are looking for someone to conduct the entire traffic survey for you? Just drop us a message.</p> <p></p>"},{"location":"OTCamera/#key-features","title":"Key features","text":"<ul> <li>Based on Raspberry Pi Zero W</li> <li>Waterproof case</li> <li>Operation with buttons and/or web ui</li> <li>Continuous recording for up to one week (depending on the external battery)</li> <li>Privacy compliant recording</li> <li>Low hardware costs per camera system (DIY)</li> </ul>"},{"location":"OTCamera/advanced_usage/calibrate/","title":"How to calibrate OTCamera","text":"<p>OTCamera gives you the opportunity to receive distinctive camera parameters. These include intrinsics, extrinsics and distortion coefficients. By using our web browser based program and multiple images of a calibration pattern, like a chessboard you can calculate a camera matrix and distortion coefficients. You will be able to evaluate your calibration with a reprojection error which is used to quantify how closely you can recreate true projection. In other words: You can undistort pictures, trajectories or any other pixel coordinates.</p> <p>Early Prototype</p> <p>The currently available version of the calibration described in this documentation is in early prototype stage. This version works and offers an impression of the functionality.  </p>"},{"location":"OTCamera/advanced_usage/calibrate/#first-steps","title":"First steps","text":"<ul> <li>First you need to download a chessboard pattern (you can search for calibration board).</li> <li>The pattern size and dimension will play no big role for now, so feel free to use any chessboard.</li> <li>Attach your printout to a solid object. The image should be as flat as possible.</li> <li>For best results its unavoidable to let your calibration pattern be manufactured by professionals.</li> </ul>"},{"location":"OTCamera/advanced_usage/calibrate/#procedure","title":"Procedure","text":"<ul> <li>Start the calibration application on your Raspberry Pi.</li> <li>Create a new calibration with a self-chosen CamID.</li> <li>Enter the number of chessboardcolumns, -rows and the squaresize in mm.</li> <li>Choose a number of wanted successful calibration pictures and resolution.</li> <li>Start the calibrationprocess.</li> <li>By clicking on \"receive coefficients\" your camera parameters will be stored as json-file on your raspberrypi.</li> <li>Evaluate the reprojection error displayed in the userinterface.</li> </ul>"},{"location":"OTCamera/advanced_usage/configration/","title":"Configuration File","text":"<p>Coming soon</p> <p>Unfortunately, there is no content here yet. But we are currently working on completing this site.</p>"},{"location":"OTCamera/gettingstarted/assembly/","title":"Assembly","text":"<p>Coming soon</p> <p>Unfortunately, there is no content here yet. But we are currently working on completing this site.</p>"},{"location":"OTCamera/gettingstarted/firstuse/","title":"Usage","text":"<p>After assembling OTCamera on your own or receiving it from us, you can finally start using it! On this page you will find all the information you need for setting up and mounting OTCamera, recording videos and downloading them. If you follow these instructions, you will be good to got to use OTCamera in the field. The recorded videos can be processed further with OTVision.</p> <p>Safety and security</p> <p>Please be aware that your OTCamera must not fall down. This applies to the place where it is mounted but also during assembly.</p> <p>Furthermore, check your local data protection regulations to guarantee you are operating according to the law. Depending on the position of the camera, it could be possible that you have to adjust the focus or take further measures.</p> <p>Usually you are not allowed to just install an OTCamera in public space and record videos. Please make sure you contact the responsible authority and verify whether you need a permission. </p> <p>OTCamera contains a Li-Ion cell and runs with an USB-battery. The maximum operating temperature is around 60 degrees Celsius.  In warmer regions or on very hot days the ambient temperature in the case may become hotter, especially in direct sunlight.</p>"},{"location":"OTCamera/gettingstarted/firstuse/#required-parts","title":"Required parts","text":"<p>OTCamera consits of various components:</p> <ol> <li>The inner case with th electronic circuit, the switches and the LEDs,</li> <li>An external USB-battery with a cable connected to the inner case,</li> <li>The outer case for weather and theft protection,</li> <li>The ball head with clamp for mounting the OTCamera on a pole.</li> </ol> <p>You may also need the following during mounting:</p> <ol> <li>Mobile phone or tablet to view the preview of OTCamera,</li> <li>A ladder to mount the OTCamera sufficiently high,</li> <li>Tools (screwdriver, cordless screwdriver, hexagon socket),</li> <li>A drop guard, to secure OTCamera and the bracket from falling down,</li> <li>A lock to protect against theft,</li> <li>High-visibility clothing for you and your colleagues,</li> <li>Traffic cones to warn pedestrians,</li> <li>The permit for operating in the field.</li> </ol> <p>Disclaimer</p> <p>For preparing the measurement you may need to think about more things than we listed. This list is just to remind you that you need to think of everything.</p>"},{"location":"OTCamera/gettingstarted/firstuse/#preparation","title":"Preparation","text":""},{"location":"OTCamera/gettingstarted/firstuse/#in-the-office","title":"In the office","text":"<p>Before every measurement you should take care of a few things.</p> <p>Aditionally to the external USB-battery, OTCamera contains an integrated battery. Before use, charge the OTCamera and the external USB-battery. With an 26800 mAh external USB-battery and the integrated battery OTCamera can record up to 3.5 or 4 days in a row. For example, if you install the OTCamera on monday afternoon, you will have recordings of Tuesday, Wednesday and Thursday.</p> <p>Before use, also connect the OTCamera to your local Wi-Fi to update the time of the hardware clock. This is especially necessary if you did not use the OTCamera for a certain time.</p> <p>Practice!</p> <p>We recommend that every person who wants to mount a OTCamera should practice in a save/\"dry\" environment beforehand. This help to prevent anything going wrong out in the field.</p>"},{"location":"OTCamera/gettingstarted/firstuse/#on-site","title":"On site","text":"<p>Once everything is set up, you can start preparing the actual measurement on site.</p> <p>Open the outer case, connect the external USB-battery with the inner case and put the external USB-battery in one of the brackets on the right side. Turn on OTCamera by bringing all the switches in their \"on\" position.</p> <p>Now the left green LED lights up permanently and indicates that OTCamera is booting up. This takes a few tens of seconds. After booted up, all LEDs turn off shortly and then start flashing.</p> <p>The power supply over USB is working, when the green LED blinks every five seconds for two times. Blinking only ones means that the power supply works using the internal battery. Make sure that the USB cable is plugged in correctly on both sides and that the external USB battery is switched on.</p> <p>The yellow LED should blink once every five seconds. It means that Wi-Fi is switched on permanently. On the one hand OTCamera conects with your Wi-Fi in the office and you can access OTCamera from your network. On the other hand OTCamera creates it's own Wi-Fi access point. The SSID of the access point and the password were set during installation.</p> <p>The red LED should blink once every five seconds. It means that the camera is currently recording. Every time the LED blinks a new preview is created.</p> <p>Everything is working just fine when the green LED blinks twice and the yellow and red LED blink once every five seconds.</p> <p>You can now connect to the Wi-Fi access point with your mobile phone or tablet. After connecting to the Wi-Fi, you can call up the IP address of OTCamera in a browser, to see the preview. This is usually 10.10.50.1.</p> Why does my mobile phone not connect? <p>Some mobile phones check whether there internet is accessable via Wi-Fi. If this is not the case, the mobile phone is connected but you cannot access the preview. It helps to change the mobile phone settings that you want to use the connection anyway. Or you can switch off your mobile data and thus force the use of Wi-Fi.</p> <p>We therefore use our own tablet without mobile data for our measurements.</p> <p>You should now see a site with the preview image. The site automatically reloads every five seconds. Below the preview image is a table with the most important information.</p> Feature: Circular buffer <p>If you have already taken several measurements and the memory card is full, at some point the oldest video will be deleted so that recording can continue.</p> <p>However, we would still recommend deleting the videos after each measurement. </p> <p>At the top of the page you will see a green bar, if everything works fine. If the camera does not record, the external power supply is not connected or the \"24/7\" switch is not switched on, a yellow or red bar will appear at the top with a message.</p> Check the time! <p>The camera name and the current time are displayed at the top of the preview image. Check the date and time each time you use OTCamera.</p> <p>If the time seems incorrect, first check if you are connected to the respective OTCamera and receive the correct preview image. Refresh you browser to check, if this is not the preview of the OTCamera you mounted previously.</p> <p>If it is the correct preview and the time is still not correct, there are two possible solutions:</p> <ol> <li>You have not connected the OTCamera to the internet for some time and have to do so in order to update the internal real-time clock.</li> <li>The button cell of the internal real-time clock is empty and has to be replaced.</li> </ol> <p>In doubt, hold your mobile phone in the camera image so that you can see the time in the video later. If you continue recording with wrong time, we recommend recording 24/7 to avoid missing data.</p> <p>You can now use the right-hand switch \"24/7\" to set whether OTCamera should record permanently or only at the configured time of day.</p> <p>When you switch off the Wi-Fi switch, the yellow LED flashes faster, indicating that the Wi-Fi is about to turn off. The time until it switches off can be set and is by default 15 minutes.</p> <p>Now you can close the outer case. Be careful not to pinch the USB cable. You do not need to open the outer case again.</p> <p>Always have extra USB cables with you :)</p> <p>Now you can mount the OTCamera.</p>"},{"location":"OTCamera/gettingstarted/firstuse/#mounting","title":"Mounting","text":"<p>Bring a buddy</p> <p>For mounting the OTCamera in the field its best to get some help. We recommend to do the whole mounting process with two persons.</p> <p>First mount the bracket with the ball head. Check that the ball head is firmly in place. The screws on the ball head should be tightened so that it cannot move. On the back of the outer case you can find a mounting plate that you can slide onto the ball head. The mounting plate is clamped to the ball head with a screw so that OTCamera is firmly attached to it.</p> <p>Danger</p> <p>Make sure that nothing falls down, nobody gets hurt and nothing gets damaged. For this purpose, always use and install a drop protection.</p> <p>Now you can loosen the big screw on the ball head a bit and align the OTCamera to your needs. You can check the alignment with your mobile phone. It is best if two people do this. One person aligns the camera and the other checks the preview.</p> <p>Always wait a few seconds before adjusting OTCamera again, as a preview image is only generated every five seconds.</p> How should I align the camera? <p>We always align OTCamera so that no sky is visible. Otherwise, glare can occur and traffic could no longer be seen. In addition, there should be enough space in all directions, so that OTVision can recognize the trajectories of road users sufficiently.</p> <p>Please be sure to follow these instructions, especially if you want to automatically process the videos with OTVision yourself or have us process them for you.</p> <p>If everything is aligned correctly, you can now lock OTCamera.</p> <p>Check all the values again on the preview page and make sure that everything is displayed correctly. Make sure that all screws are tightened and the fall protection is in place.</p>"},{"location":"OTCamera/gettingstarted/firstuse/#dismounting","title":"Dismounting","text":"<p>When your measurement is finished, you can dismount OTCamera.</p> <p>Basically, just follow the installation steps in reverse order. When dismounting, make sure that nothing can drop (as always!). In particular, opening the screw on the ball head to loosen the mounting plate can cause OTCamera to fall down. Please take special care that this does not happen.</p> <p>When everything is dismounted and you are back on the ground, you can open the outer case and set the three right-hand switches (Boot, Wi-Fi, 24/7) to off.</p> <p>If OTCamera still has power, it will now shut down and indicate this with a rapidly flashing green LED. If the battery has run down in the meantime, OTCamera has already shut down on its own. Nevertheless, set all three switches to \"0\".</p> Turn of all switches! <p>It is sufficient to set the \"Boot\" switch to \"0\". However, we have made the experience that it is better to make sure that every switch is in the off position. Switch on all the switches at the beginning of the measurement and switch them off at the end. Then everything is back in the starting position for the next measurement.</p> <p>When the OTCamera is back in the office, charge the external USB-battery and the OTCamera itself.</p>"},{"location":"OTCamera/gettingstarted/firstuse/#downloading-videos","title":"Downloading videos","text":"<p>After recharging the battery of the OTCamera you can download the recorded videos. You have two options:</p> <ol> <li>Via Wi-Fi and SSH and</li> <li>to an USB flash drive</li> </ol>"},{"location":"OTCamera/gettingstarted/firstuse/#wi-fi","title":"Wi-Fi","text":"<p>Turn on OTCamera as described above. If OTCamera is connected to the Wi-Fi in your office, you can access via SSH using its IP address. This works the same way as during the installation process.</p> <p>Alternatively, you can establish a SSH connection using the access point of OTCamera.</p> <p>By default the recorded videos and logs are stored in the home directory in <code>~/videos</code>. You can download all videos via scp or rsync.</p>"},{"location":"OTCamera/gettingstarted/firstuse/#usb","title":"USB","text":"<p>If you have an OTG USB adapter built into OTCamera, you can also have the videos copied to a flash drive.</p> <p>Turn off OTCamera completely. Plug a flash drive in the USB port of OTCamera and turn on OTCamera using the \"Boot\" switch. During boot-up, OTCamera automatically checks whether a flash drive is plugged in. If this is not the case, the recording function starts (as usual).</p> <p>If a flash drive is plugged in, OTCamera starts in USB copy mode. The yellow LED light blinks to indicate that videos are being copied. If the LED lights up continuously, copying is complete. In addition, the green LED lights up continuously when you can remove the flash drive.</p> <p>OTCamera can then be shut down again by switching off the \"Boot\" switch.</p> <p>The flash drive now contains all videos and a csv file. The csv file gives an overview of all videos on OTCamera (not on the flash drive).</p> <p>When you open the csv file, you can specify that certain videos should be deleted. Just change the corresponding column and insert an <code>x</code> or <code>true</code>.</p> <p>When you save the file and plug the flash drive back into OTCamera,the marked videos will be deleted the next time you boot up.</p>"},{"location":"OTCamera/gettingstarted/firstuse/#next-steps","title":"Next steps","text":"<p>Now you should have all the videos. In the next step you can process them with OTVision.</p>"},{"location":"OTCamera/gettingstarted/installation/","title":"Overview","text":"<p>No matter which version you want to install, you will always use a Raspberry Pi as a base.</p> <p>You will need:</p> <ul> <li>Raspberry Pi 2B / 3B(+) / 4 / Zero W / Zero 2W and power supply</li> <li>We recommend a Zero W or Zero 2W because they draw substantially less power.</li> <li>Micro SD card (a High Endurance version is recommended)</li> <li>SD Card Reader</li> <li>Raspberry Pi Imager.</li> </ul> <p>Warning</p> <p>Even though we have completed the guide ourselves, every PC is different and Raspberry OS can change as well. There are steps described with which you can break your Windows, macOS or Linux on your PC. You should know what you are doing. We are not responsible for any damage that may occur.</p>"},{"location":"OTCamera/gettingstarted/installation/#quick-installer","title":"Quick Installer","text":"<p>We provide a simple quick installer which will do most of the work for you. We're assuming that you'll use an DS3132 RTC module. If you are using a different or no RTC module you should adjust the quick installer.</p> <p>Info</p> <p>If you like, you can also setup the Raspberry Pi and install OTCamera manually. The necessary steps are described on the following pages.</p>"},{"location":"OTCamera/gettingstarted/installation/#prepare-the-sd-card","title":"Prepare the SD-Card","text":"<p>To use the quick installer you'll need to prepare a SD-Card first. If done correctly you should now be able to connect to your Raspberry Pi via SSH.</p>"},{"location":"OTCamera/gettingstarted/installation/#update-and-upgrade-the-system","title":"Update and Upgrade the System","text":"<p>After preparing the SD-Card it is important to update and upgrade the Raspberry Pi before using the quick installer.</p> <p>To do this execute the following command:</p> <pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y &amp;&amp; sudo reboot\n</code></pre>"},{"location":"OTCamera/gettingstarted/installation/#download-and-run-quick-installer","title":"Download and Run Quick Installer","text":"<p>Connect to your Raspberry Pi via SSH again after reboot and download and run the quick installer:</p> <pre><code>wget https://raw.githubusercontent.com/OpenTrafficCam/OTCamera/master/raspi-files/install_otcamera.sh\nsudo bash install_otcamera.sh\n</code></pre> <p>You will be asked for some config values.</p> <p>After the installer complete, simply reboot the Raspberry Pi and the OTCamera software should be up and running.</p> <p>If not, please report your issues using the GitHub issue tracker.</p> <p>Note</p> <p>We are planning to offer a ready-to-use image for the Raspberry Pi, which can be easily installed. The setup will then be even easier. Stay tuned :)</p>"},{"location":"OTCamera/gettingstarted/requirements/","title":"Requirements","text":"<p>You will need some special hardware to build your own OTCamera to record videos.</p> <p>OTCamera is based on a Raspberry Pi Zero W and the official Rapberry Pi Camera Module V2 (we are considering V3 modules as soon as picamera2 is stable).</p> <p>A specially designed PCB (printed circuit board) is needed to connect all additional parts to the Raspberry Pi. The single parts for the PCB as well as basic soldering skills are required to assemble a OTCamera.</p> <p></p> <p>As case a common waterproof explorer case is used in addition to a 3D printed inlay which carries all OTCamera parts.</p> <p>To power everthing up, its best to use an external USB-battery.</p> <p>All in all you need:</p> <ul> <li>Raspberry Pi Zero W</li> <li>Raspberry Pi Camera Module V2</li> <li>Ribbon cable to connect Camera to Raspberry</li> <li>Micro SD card</li> <li>Explorer Cases 2209 (or another waterproof case)</li> <li>OTCamera PCB (+ components) (released soon)</li> <li>OTCamera 3D printed parts (released soon)</li> <li>Adafruit PowerBoost 1000C</li> <li>Adafruit PiRTC - Precise DS3231 Real Time Clock</li> <li>1 cell LiPo battery</li> <li>Four switches</li> <li>Three LEDs (for example Barthelme 6V DC, 9.5mm)</li> <li>Some cables and cable shoes</li> </ul> <p>To assemble everything, you will need a drill, a soldering iron and some basics tools as well.</p>"},{"location":"OTCamera/gettingstarted/installation/hwclock/","title":"Hardware Clock","text":"<p>The Raspberry Pi itself doesn't bring a hardware clock. It's assumed that a Pi always has a internet connection to get the current time. Since we want to use OTCamera in the field and thus may not have a Wi-Fi connection to access internet, we'll need a dedicated hardware clock (i.e. real time clock or RTC). It contains a backup battery (a coin cell) to keep track of time. The Pi will use the hardware clock time to set the system time.</p> <p>There are several RTC chips out there. There are even several RTC modules that are specifically designed to use with a Pi. One of the most precise RTC chips is the DS3231. We are using a Adafruit PiRTC - Precise DS3231 Real Time Clock for Raspberry Pi.</p> <p>Adafruit also provides a good (but little bit outdated) tutorial on how to use a RTC on the Pi. The follwing description is based on that tutorial.</p> <p>First, we'll need to enable I2C to communicate with the RTC. Either use <code>sudo raspi-config</code> and navigate to:</p> <ul> <li>Interface Options \u2192 I2C \u2192 Yes</li> </ul> <p>Or just use the command-line version of raspi-config:</p> <pre><code>sudo raspi-config nonint do_i2c 0\n</code></pre> <p>Let's install and use <code>i2c-tools</code> to see if the RTC works:</p> <pre><code>sudo apt install i2c-tools -y\nsudo i2cdetect -y 1\n</code></pre> <p>You should see an output of several lines containing <code>68</code> in one of them.</p> <p>We now need to add a device tree overlay in <code>/boot/config.txt</code> by adding the highlighted line at end of the file:</p> /boot/config.txt (end of file)<pre><code>...\n# (e.g. for USB device mode) or if USB support is not required.\notg_mode=1\n\n[all]\n\n[pi4]\ndtoverlay=vc4-fkms-v3d\n# Run as fast as firmware / board allows\narm_boost=1\n\n[all]\ngpu_mem=128\n\n# OTCamera\ndtoverlay=disable-bt\ndisable_camera_led=1\ndtparam=act_led_trigger=none\ndtparam=act_led_activelow=on\ndtoverlay=i2c-rtc,ds3231\n</code></pre> I'm not using the DS3231 <p>If you are using a different RTC than the DS3231, check out the Adafruit tutorial.</p> <p>In a nutshell: Use <code>dtoverlay=i2c-rtc,ds1307</code> or <code>dtoverlay=i2c-rtc,pcf8523</code> instead.</p> <p>After rebooting the Pi (<code>sudo reboot</code>) you can run <code>sudo i2cdetect -y 1</code> again. Instead of <code>68</code>there should be a <code>UU</code> instead.</p> <p>Now we'll disable the fake hardware clock:</p> <pre><code>sudo apt remove fake-hwclock -y\nsudo update-rc.d -f fake-hwclock remove\nsudo systemctl disable fake-hwclock\n</code></pre> <p>Additionally, we'll need to comment out some lines in <code>/lib/udev/hwclock-set</code>. So run <code>sudo nano /lib/udev/hwclock-set</code> and add <code>#</code> at the beginning of the lines:</p> /lib/udev/hwclock-set<pre><code>#!/bin/sh\n# Reset the System Clock to UTC if the hardware clock from which it\n# was copied by the kernel was in localtime.\n\ndev=$1\n\n#if [ -e /run/systemd/system ] ; then\n#    exit 0\n#fi\n\n#/sbin/hwclock --rtc=$dev --systz\n/sbin/hwclock --rtc=$dev --hctosys\n</code></pre> <p>The Pi should now be able to communicate with the RTC. Let's try it:</p> <pre><code>sudo hwclock -r\n</code></pre> <p>The Pi should now have automatically synced the time. If not, check if the Pi knows the correct time and sync it once:</p> <pre><code>date\nsudo hwclock -w\n</code></pre> <p>Done :)</p> <p>Your OTCamera should now be able to keep track of time without any USB power connected and without access to the internet. We recommend booting your OTCamera once within Wi-Fi range before each recording to update the time, although the DS3231 is fairly accurate.</p>"},{"location":"OTCamera/gettingstarted/installation/otcamera/","title":"Install OTCamera","text":"<p>Now we are ready to install last missing dependencies and setup OTCamera.</p>"},{"location":"OTCamera/gettingstarted/installation/otcamera/#setup-python-and-dependencies","title":"Setup Python and Dependencies","text":"<p>By default, Raspberry OS light doesn't come with PIP and git installed. We will need it to install required packages.</p> <pre><code>sudo apt install python3-pip git -y\n</code></pre> If you are using RaspberryPi OS legacy <p>Since the latest RaspberryPi OS python3 is the default python version. If you are using (not recommended) an older version of RaspberryPi OS, you need to make python3 your default version.</p> <p>Raspberry OS legacy ships with python 2 and python 3. By default python 2 is used. We want to change that to python 3 by adding two single lines to <code>.bashrc</code>.</p> <pre><code>echo \"alias python='/usr/bin/python3'\" &gt;&gt; ~/.bashrc\necho \"alias pip=pip3\" &gt;&gt; ~/.bashrc\n\nsource ~/.bashrc\n\npython --version\npip --version\n</code></pre> <p>Both commands should state, that they are (using) python 3.(x).</p>"},{"location":"OTCamera/gettingstarted/installation/otcamera/#clone-otcamera","title":"Clone OTCamera","text":"<p>We'll need to download OTCamera using git to get all the code we'll need to run OTCamera.</p> <pre><code>git clone --depth 1 https://github.com/OpenTrafficCam/OTCamera.git\n</code></pre> <p>OTCamera requires additional python packages, which need to be installed.</p> <pre><code>cd OTCamera\npip install -r requirements.txt --upgrade\n</code></pre>"},{"location":"OTCamera/gettingstarted/installation/otcamera/#setup-webserver-for-preview","title":"Setup Webserver for Preview","text":"<p>We are currently using nginx (a small webserver) to serve a small HTML file including a preview of the camera view.</p> <p>To install nginx:</p> <pre><code>sudo apt install nginx -y\n</code></pre> <p>We need to configure nginx to serve the OTCamera GUI. Open the nginx config file <code>/etc/nginx/sites-available/default</code> and edit the webserver root.</p> <pre><code>sudo nano /etc/nginx/sites-available/default\n</code></pre> <p>You should see something like this (there may be some comment lines starting with # which you can ignore).</p> /etc/nginx/sites-available/default<pre><code>server {\n        listen 80 default_server;\n        listen [::]:80 default_server;\n\n        root /var/www/html;\n\n        index index.html index.htm index.nginx-debian.html;\n\n        server_name _;\n\n        location / {\n                try_files $uri $uri/ =404;\n        }\n\n}\n</code></pre> <p>The important line we need to change is highlighted. Replace <code>/var/www/html</code> with the full path to the <code>OTCamera/gui/webfiles</code> folder. If your username is <code>pi</code> it should be <code>/home/pi/OTCamera/webfiles</code>.</p> <p>Restart nginx afterwards to let it know about the new directory:</p> <pre><code>sudo systemctl restart nginx.service\n</code></pre>"},{"location":"OTCamera/gettingstarted/installation/otcamera/#start-otcamera-on-startup","title":"Start OTCamera on startup","text":"<p>Now we want to start OTCamera every time the Raspberry starts. To do so, we will setup and enable a service. Edit <code>otcamera.service</code> inside the OTCamera repository according to your username and path.</p> ./raspi-files/otcamera.service<pre><code>[Unit]\nDescription=This service starts OTCamera and keeps it running\n\n[Service]\nUser=pi\nWorkingDirectory=/home/pi/OTCamera\nRestart=always\nRestartSec=3\nExecStart=/usr/bin/python3 run.py\n\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Now copy the file to <code>/lib/systemd/system</code> and enable it:</p> <pre><code>sudo cp ./raspi-files/otcamera.service /lib/systemd/system\nsudo systemctl daemon-reload\nsystemctl enable otcamera.service\n</code></pre> <p>After rebooting the Raspberry you should be able to access it's Wi-Fi ap and to open the OTCamera status site using the Raspberry's ip address: http://10.10.50.1</p>"},{"location":"OTCamera/gettingstarted/installation/prepare-sd-card/","title":"Prepare the SD Card","text":"<p>Download and install the Raspberry Pi Imager, insert the SD-Card and start the Imager.</p> <p></p> <p>It's recommended to use the Raspberry Pi Imager Advanced Options to do some basic setup. Just press Ctrl+Shift+X on startscreen to access advanced options. In newer Raspberry Pi Imager versions you just need to press the gear symbol.</p> <p>Setup a hostname, enable SSH, either password based or using public-key authentication (recommended!), configure your Wi-Fi credentials and choose the correct locale for Wi-Fi, timezone and keyboard layout. You can also skip the first-run wizard.</p> <p>If you set the default username to anything different than \"pi\" (which is recommended) you'll need to replace \"pi\" in the following documentation with your username. For example: instead of connecting to <code>ssh pi@hostname</code> you'll need <code>ssh username@hostname</code>.</p> How to generate a public key <p>Generate SSH-Keys for password-less connection. On your desktop computer open a command-line terminal (CMD or Powershell on Windows or a bash on Linux) and run</p> <pre><code>ssh-keygen\n</code></pre> <p>to generate a public private key combination. Add the private key to your ssh-agent (you may need to update OpenSSH on Windows).</p> <p></p> <p>Now insert the SD card into your PC. Select Raspberry Pi OS Lite (Legacy, 32-bit) as operating system under \"Raspberry Pi (Other)\". Then select the SD card on which the operating system will be installed.</p> <p>Warning</p> <p>It is possible to select not only the SD card, but also the hard drive or SSD where your operating system or other PC data is stored. Make sure that you really select the right SD card. Most of the time, you can tell by the size.</p> <p>Press the Write button and wait until the Raspberry Pi Imager has completely written and verified your SD card.</p> <p>Tip</p> <p>Sometimes the verification may fail. Try another USB port on your PC or another card reader. If this does not help, maybe the SD card is broken.</p> Setup without Raspberry Pi Imager <ol> <li> <p>Add an empty file named <code>ssh</code> to the boot partition to enable ssh on first boot.</p> </li> <li> <p>Add your Wi-Fi credentials as described in the Raspberry Pi Documentation</p> </li> <li> <p>Connect to the pi using ssh (<code>ssh pi@raspberry</code>)to the Pi using password authentication.</p> <p>Create and edit the needed <code>authorized_keys</code> file.</p> <pre><code>mkdir -p ~/.ssh\nnano ~/.ssh/authorized_keys\n</code></pre> <p>Copy your public key on the host and paste it on the pi, save&amp;close using Ctrl+X - Enter - Y.</p> </li> </ol>"},{"location":"OTCamera/gettingstarted/installation/prepare-sd-card/#next-steps","title":"Next steps","text":"<p>You're now ready to boot your Raspberry Pi to install OTCamera. You can either do it by using our Quick Installer or follwing all steps on the next sites.</p>"},{"location":"OTCamera/gettingstarted/installation/raspberry/","title":"Setup the Raspberry","text":"<p>Now, take the SD-Card and insert it in the Raspberry. You can now boot the Pi on LAN or within your Wi-Fi by connecting it to the power supply. Try to connect to the Pi using a command-line or PowerShell:</p> <pre><code>ssh pi@otcamera01\n</code></pre> <p>If everything is setup correctly, you will be asked to add the host key to the list of known hosts (answer: yes) and you should be connected to your Raspberry Pi.</p> warning: agent returned different signature type ssh-rsa (expected rsa-sha2-512) <p>If you are on Windows you may need to update OpenSSH if you ar getting this error:</p> <pre><code>warning: agent returned different signature type ssh-rsa (expected rsa-sha2-512)\n</code></pre> <p>Warning</p> <p>This guide should run on Windows 10 but you are modifying your system files. Please do not do anything you do not understand! Read the source for more information. We are not liable for any damage you may cause to your system. But we followed the steps ourselves and everything went well.</p> <ol> <li> <p>Download the latest OpenSSH for Windows release here. Depending on your Windows version it is probably <code>OpenSSH-Win64.zip</code>.</p> </li> <li> <p>Open Windows Explorer and navigate to your Download folder. You should see the <code>OpenSSH-Win64.zip</code>. Open an elevated PowerShell via File \u2192 Open Windows PowerShell \u2192 Open Windows PowerShell as administrator.</p> </li> <li> <p>If you copy and paste the following code, you will unzip OpenSSH and overwrite the system's default OpenSSH version.</p> <pre><code># Overwrite windows installed bins\n$openSshBins = (Get-ChildItem 'C:\\WINDOWS\\System32\\OpenSSH\\').Name\nExpand-Archive -Path .\\OpenSSH-Win64.zip -DestinationPath .\ntakeown.exe /a /r /f C:\\Windows\\System32\\OpenSSH\\\nicacls.exe 'C:\\Windows\\System32\\OpenSSH' /grant 'BUILTIN\\Administrators:(OI)(CI)F'\nicacls.exe 'C:\\Windows\\System32\\OpenSSH' /grant 'BUILTIN\\Administrators:F' /t\nStop-Service ssh-agent\n$openSshBins | %{ Copy-Item -Path .\\OpenSSH-Win64\\$_ -Destination C:\\Windows\\System32\\OpenSSH\\ }\nStart-Service ssh-agent\n</code></pre> </li> </ol> <p></p> <p>If you have successfully logged in now, we can configure the Raspberry Pi for OTCamera.</p>"},{"location":"OTCamera/gettingstarted/installation/raspberry/#update","title":"Update","text":"<p>Update the pi by running apt and reboot.</p> <pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y &amp;&amp; sudo reboot\n</code></pre> A new version of configuration file is available <p></p> <p>If you get this message, don't worry. Keep the local version currently installed, since we changed the ssh server configuration using the RPi Imager.</p>"},{"location":"OTCamera/gettingstarted/installation/raspberry/#raspi-config","title":"Raspi Config","text":"<p>Reconnect to your pi (open PowerShell and run <code>ssh pi@otcamera01</code>) and run the raspberry configuration tool.</p> <pre><code>sudo raspi-config\n</code></pre> <p>Change the following settings to appropriate values:</p> <ul> <li>System Options \u2192 Password (if not already done with Raspi Imager choose a new password for security reasons)</li> <li>Interface Options \u2192 I1 Legacy Camera \u2192 yes (since the new camera API is not supported by picamerax)</li> <li>Advanced options \u2192 GL driver \u2192 G1 Legacy (This may take a while, but saves a lot of energy.)</li> </ul> Setup without Raspberry Pi Imager <p>If you did not use the Raspberry Pi Imager, you will need to setup a few more things.</p> <ul> <li>System Options<ul> <li>Hostname</li> </ul> </li> <li>Localization Options<ul> <li>Timezone (Europe/Berlin)</li> <li>WLAN Country (DE)</li> </ul> </li> </ul> <p>Exit the raspi-config selecting \"Finish\" and reboot the Pi afterwards.</p>"},{"location":"OTCamera/gettingstarted/installation/raspberry/#power-saving-options","title":"Power Saving Options","text":"<p>After a reboot we also want to disable the HDMI output for additional power saving. CNX Software made some great power measurements for some Raspberry Pis. We'll need to add a specific line to the file <code>/etc/rc.local</code> in order to deactivate HDMI on every boot.</p> <pre><code>sudo nano /etc/rc.local\n</code></pre> <p>This opens the texteditor nano. We need to insert <code>/usr/bin/tvservice -o</code> in this file as highlighted below. Additionally we'll insert <code>sbin/iw dev wlan0 set power_save off</code> to disable automatic Wi-Fi power saving since we'll deactivate it anyways as soon as we don't need Wi-Fi.</p> /etc/rc.local<pre><code>#!/bin/sh -e\n#\n# rc.local\n#\n# This script is executed at the end of each multiuser runlevel.\n# Make sure that the script will \"exit 0\" on success or any other\n# value on error.\n#\n# In order to enable or disable this script just change the execution\n# bits.\n#\n# By default this script does nothing.\n\n# Print the IP address\n_IP=$(hostname -I) || true\nif [ \"$_IP\" ]; then\n  printf \"My IP address is %s\\n\" \"$_IP\"\nfi\n\n/usr/bin/tvservice -o\n/sbin/iw dev wlan0 set power_save off\n\nexit 0\n</code></pre> <p>Press Ctrl+X and Y and Enter to save the file and exit nano.</p> <p>Additionally we will disable bluetooth and the camera and onboard LED's. Edit <code>/boot/config.txt</code>to do so.</p> <pre><code>sudo nano /boot/config.txt\n</code></pre> <p>The config is quite long. We will add some lines (highlighted) at the end of the file:</p> /boot/config.txt (end of file)<pre><code>...\n# (e.g. for USB device mode) or if USB support is not required.\notg_mode=1\n\n[all]\n\n[pi4]\ndtoverlay=vc4-fkms-v3d\n# Run as fast as firmware / board allows\narm_boost=1\n\n[all]\ngpu_mem=128\n\n# OTCamera\ndtoverlay=disable-bt\ndisable_camera_led=1\ndtparam=act_led_trigger=none\ndtparam=act_led_activelow=on\ndtparam=audio=off\ndisplay_auto_detect=0\ngpio=6,16,18,19=ip\ngpio=16,18,19=pu\ngpio=6=pd\ngpio=5,12,13=op\ngpio=5,12=dl\ngpio=13=dh\n</code></pre> <p>Lines 22 to 27 define the default state of the GPIO pins. If you are using the OTCamera PCB, you want to add those lines. If you are not using it, you may want to adjust settings to your specific setup.</p> <p>Rebooting the Pi activates the new settings.</p> <pre><code>sudo reboot\n</code></pre>"},{"location":"OTCamera/gettingstarted/installation/wifi-ap/","title":"Wi-Fi Accesspoint","text":"<p>In order to access the OTCamera Raspberry Pi in field, we will let the Raspberry create it's own Wi-Fi. First, we'll need to install some packages:</p> <pre><code>sudo apt install hostapd dnsmasq dhcpcd -y\n</code></pre> <p>Now we'll need to configure all three of the newly installed packages. Let's start with hostap.</p> <pre><code>sudo nano /etc/default/hostapd\n</code></pre> <p>We need to modify line 13 to specify a valid <code>hostapd.conf</code>. To do so, insert <code>\"/etc/hostapd/hostapd.conf\"</code> right after <code>DAEMON_CONF=</code> in line 13:</p> /etc/default/hostapd<pre><code># Defaults for hostapd initscript\n#\n# WARNING: The DAEMON_CONF setting has been deprecated and will be removed\n#          in future package releases.\n#\n# See /usr/share/doc/hostapd/README.Debian for information about alternative\n# methods of managing hostapd.\n#\n# Uncomment and set DAEMON_CONF to the absolute path of a hostapd configuration\n# file and hostapd will be started during system boot. An example configuration\n# file can be found at /usr/share/doc/hostapd/examples/hostapd.conf.gz\n#\nDAEMON_CONF=\"/etc/hostapd/hostapd.conf\"\n\n# Additional daemon options to be appended to hostapd command:-\n#       -d   show more debug messages (-dd for even more)\n#       -K   include key data in debug messages\n#       -t   include timestamps in some debug messages\n#\n# Note that -B (daemon mode) and -P (pidfile) options are automatically\n# configured by the init.d script and must not be added to DAEMON_OPTS.\n#\n#DAEMON_OPTS=\"\"\n</code></pre> <p>Now let's edit this hostapd.conf file to configure our access point:</p> /etc/hostapd/hostapd.conf<pre><code>channel=11\nssid=MyOTCameraWifiNetwork\nwpa_passphrase=reallysafepassword\ninterface=uap0\nhw_mode=g\nmacaddr_acl=0\nauth_algs=1\nwpa=2\nwpa_key_mgmt=WPA-PSK\nwpa_pairwise=TKIP\nrsn_pairwise=CCMP\ncountry_code=DE\n</code></pre> <p>If you are planning to connect OTCamera to your own Wi-Fi (e.g. to transfer files in office or to get internet access to update etc.) you must use the same Wi-Fi channel as your office Wi-Fi network (since the Raspberry has just one antanna). You should edit your office Wi-Fi to always use the same channel to avoid automatic channel selection.</p> <p>The name of the Wi-Fi is specified just after <code>ssid=</code> and the password in line 3.</p> <p>Depending on where you will use OTCamera you should set the according contry code in the last line (for us it's Germany --&gt; DE).</p> <p>Save and exit the file.</p> <p>If you will connect to the OTCamera's Wi-Fi your device will need a valid ip address. dhcpcd and dnsmasq will help us doing by adding some lines (12-14) to the end of <code>/etc/dhcpcd.conf</code>:</p> /etc/dhcpcd.conf<pre><code>...\n# It is possible to fall back to a static IP if DHCP fails:\n# define static profile\n#profile static_eth0\n#static ip_address=192.168.1.23/24\n#static routers=192.168.1.1\n#static domain_name_servers=192.168.1.1\n\n# fallback to static profile on eth0\n#interface eth0\n#fallback static_eth0\ninterface uap0\n        static ip_address=10.10.50.1/24\n        nohook wpa_supplicant\n</code></pre> <p>If your office Wi-Fi uses the same address range you should use another one by, for example, using 51 instead of 50. But you need to remember that address to connect to your OTCamera later on.</p> <p>Finally, let's configure dnsmasq's config (<code>/etc/dnsmasq.conf</code>). It's a quite long file with a lot of explaining comments. We will backup this template and afterwards create a new, empty config:</p> <pre><code>sudo mv /etc/dnsmasq.conf /etc/dnsmasq.conf.backup\nsudo nano /etc/dnsmasq.conf\n</code></pre> <p>Now add the necessary config:</p> /etc/dnsmasq.conf<pre><code>interface=lo,uap0\nno-dhcp-interface=lo,wlan0\nbind-interfaces\nserver=10.10.50.1\ndomain-needed\nbogus-priv\ndhcp-range=10.10.50.10,10.10.50.250,2h\n</code></pre> <p>If you chose a different ip address range in <code>/etc/dhcpcd.conf</code> you'll need to adjust ip addresses here as well.</p> <p>Puh, this was quite a lot configuration edit stuff... But we are almost ready :)</p> <p>We don't want the services to start uncoordinated at bootup. So let's unmask hostapd first and then disable all three services to not start up at boot:</p> <pre><code>systemctl unmask hostapd.service\nsystemctl disable hostapd.service\nsystemctl disable dhcpcd.service\nsystemctl disable dnsmasq.service\n</code></pre> <p>We'll use a script instead to start up services. Make sure you are still inside the OTCamera directory.</p> <pre><code>sudo cp ./raspi-files/usr/local/bin/wifistart /usr/local/bin/wifistart\n</code></pre> <p>Last but not least, let's add this script to <code>/etc/rc.local</code> to start it at boot.</p> /etc/rc.local<pre><code>#!/bin/sh -e\n#\n# rc.local\n#\n# This script is executed at the end of each multiuser runlevel.\n# Make sure that the script will \"exit 0\" on success or any other\n# value on error.\n#\n# In order to enable or disable this script just change the execution\n# bits.\n#\n# By default this script does nothing.\n\n# Print the IP address\n_IP=$(hostname -I) || true\nif [ \"$_IP\" ]; then\n  printf \"My IP address is %s\\n\" \"$_IP\"\nfi\n\n/usr/bin/tvservice -o\n/sbin/iw dev wlan0 set power_save off\n\n/bin/bash /usr/local/bin/wifistart\"\n\nexit 0\n</code></pre> <p>Yeah, we're done! If you carfully followed this steps and we maintained this manual (...) the Raspberry should still connect to your Wi-Fi and will also create it's own Wi-Fi. It could take a minute or two, so don't worry to early. Let's try:</p> <pre><code>sudo reboot\n</code></pre> <p>All good? We hope so!</p>"},{"location":"OTCamera/usage/buttons_leds/","title":"Buttons and LEDs","text":"<p>OTCamera can be controlled using four switches. Its operating status is indicated by 3 LEDs.</p> <p>The switches from left to right are:</p> <ul> <li>Reset</li> <li>Boot</li> <li>Wi-Fi</li> <li>24/7</li> </ul> <p>The three LEDs correspond to the rightmost three switches.</p>"},{"location":"OTCamera/usage/buttons_leds/#reset-switch","title":"Reset Switch","text":"<p>The Reset switch functions as a constant power supply for the Raspberry Pi. By switching it off the power supply to the Raspberry Pi is interrupted. Therefore, it is recommended to keep the Reset switch always turned on unless there is a problem with the Raspberry Pi.</p> <p>Potential Data Loss</p> <p>When the Reset switch is pressed, the Raspberry Pi turns off immediately but is not shut down properly. This can potentially affect the file system, and the SD card may become unreadable.</p> <p>The other three switches are used for normal operation.</p>"},{"location":"OTCamera/usage/buttons_leds/#boot-switch-and-led","title":"Boot Switch and LED","text":"<p>The Boot switch is used to turn the system on and off. When it is turned on, the green LED below the switch remains continuously lit.</p> <p>Once the Raspberry Pi has finished booting and OTCamera has been launched, the green LED starts blinking. Blinking twice means that an external power source is connected via USB. If it blinks only once, the system is running on internal battery power.</p> <p>After OTCamera has finished booting, the other two LEDs also start blinking (if the Wi-Fi and 24/7 switches are turned on).</p>"},{"location":"OTCamera/usage/buttons_leds/#wi-fi-switch-and-led","title":"Wi-Fi Switch and LED","text":"<p>The Wi-Fi switch controls the Wi-Fi module of the Raspberry Pi. This includes both the connection to a Wi-Fi network and the Wi-Fi access point of the OTCamera itself.</p> <p>When the switch is turned off, the Wi-Fi is turned off after a configurable time period (default is 15 minutes). This allows to turn off the switch before mounting OTCamera in the field. Accessing the web interface is still possible for a few minutes. This way, the camera can be safely mounted and aligned.</p> <p>When the Wi-Fi is permanently enabled, the Wi-Fi LED blinks once every 5 seconds. The Wi-Fi LED blinks once per second when the timer is active. Once the Wi-Fi is turned off, the Wi-Fi LED stops blinking and turns off completely.</p> <p>If the Wi-Fi switch is already turned off when OTCamera starts up, the Wi-Fi is immediately turned off after the start (without a timer).</p>"},{"location":"OTCamera/usage/buttons_leds/#247-switch-and-record-led","title":"24/7 Switch and Record LED","text":"<p>The 24/7 switch controls when OTCamera records videos. If the switch is turned on, recording is continuous. If the switch is turned off, recording only occurs between a configured start and end time (default is from 06:00 AM to 10:00 PM).</p> <p>The LED below the switch indicates whether the camera is currently recording. It should blink every 5 seconds. When the LED blinks, a new preview for the web interface is also generated.</p>"},{"location":"OTCamera/usage/buttons_leds/#led-blink-patterns","title":"LED Blink Patterns","text":"LED Blink Pattern Description Boot (green) <code>__________</code> OTCamera powered off. Boot (green) <code>**********</code> OTCamera is booting. Boot (green) <code>*_*_______</code> OTCamera is running and connected to external power (USB). Boot (green) <code>*_________</code> OTCamera is running and not connected to external power (USB). Wi-Fi (yellow) <code>__________</code> Wi-Fi and Access Point (AP) are turned off. Wi-Fi (yellow) <code>*_________</code> Wi-Fi and AP are turned on. Wi-Fi (yellow) <code>*_*_*_*_*_</code> Wi-Fi will turn off after a specific timer period (default is 15 minutes). Record (red) <code>*_________</code> OTCamera is currently recording videos. Record (red) <code>__________</code> OTCamera is not recording videos, probably because the current time is outside the recording timer. <code>_</code>: off  <code>*</code>: on"},{"location":"OTCamera/usage/getvideos/","title":"Get the Videos","text":"<p>Coming soon</p> <p>Unfortunately, there is no content here yet. But we are currently working on completing this site.</p>"},{"location":"OTCamera/usage/mounting/","title":"Mounting","text":"<p>Coming soon</p> <p>Unfortunately, there is no content here yet. But we are currently working on completing this site.</p>"},{"location":"OTCamera/usage/preparation_safety/","title":"Preparation &amp; Safety","text":"<p>Coming soon</p> <p>Unfortunately, there is no content here yet. But we are currently working on completing this site.</p>"},{"location":"OTLabels/","title":"Features","text":"<p>The backbone of OpenTrafficCam.</p> <p>OTLabels is a workflow for retraining AI models to detect road users in videos based on custom video frames and existing open source software packages (CVAT, YOLO v8).</p>"},{"location":"OTLabels/#key-features","title":"Key features","text":"<ul> <li>Annotation of custom video frames using existing open source software (CVAT)</li> <li>Retrain existing neural network models with YOLO v8 for individual optimization of the detection ability</li> <li>Runs on any Windows laptop. However, a decent GPU with CUDA support is strongly recommended.</li> </ul>"},{"location":"OTLabels/#content-of-documentation","title":"Content of documentation","text":"<ul> <li>Training</li> <li>Validation</li> <li>Models</li> </ul> <p>Most of the code describing the workflow will be developed by the OpenTrafficCam contributors (based on python libraries like NumPy, pandas, tkinter and OpenCV).</p>"},{"location":"OTLabels/gettingstarted/data/","title":"Data","text":"<p>Coming soon</p> <p>Unfortunately, there is no content here yet. But we are currently working on completing this site.</p>"},{"location":"OTLabels/gettingstarted/installation/","title":"Installation","text":"<p>Coming soon</p> <p>Unfortunately, there is no content here yet. But we are currently working on completing this site.</p>"},{"location":"OTLabels/gettingstarted/requirements/","title":"Requirements","text":"<p>Coming soon</p> <p>Unfortunately, there is no content here yet. But we are currently working on completing this site.</p>"},{"location":"OTLabels/models/coco6/","title":"COCO 6-class","text":"<p>Coming soon</p> <p>Unfortunately, there is no content here yet. But we are currently working on completing this site.</p>"},{"location":"OTLabels/training/cvat/","title":"CVAT","text":"<p>CVAT is a free video and image annotation tool for computer vision. We recommend the developers guides for Installation as well as the user's guide.</p> <p>If you want to label your own dataset to retrain models, keep in mind that the format of your new labels need to be the same as the format of the original dataset. On this page we define our intended organizational and technical annotation workflow using CVAT.</p> <p>Since YOLOv5 is based on the the MS COCO dataset of 2D bounding boxes, you need to apply your own labels in the frames from your sample videos in the same format. We therefore recommend the following steps:</p> <ol> <li>Install and set up an instance of CVAT either on your local computer or on a server (recommended when working in a team).</li> <li>Import the videos in CVAT and select the frames you want to use for labelling.</li> <li>Download the dataset from CVAT using the YOLO v1.1 format.</li> <li>Pre-annotate your frames with the standard YOLOv5x model. The results do probably not meet the desired quality.</li> <li>However, the pre-annotated labels will save you some time since it is not necessary to draw all labels from scratch.</li> <li>Upload the pre-annotated frames to CVAT and revise the detected labels.</li> <li>Download the final labels from CVAT and convert them to COCO format (using our cvat_to_coco.py script).</li> </ol> <p>The COCO dataset and consequently the YOLOv5 models can detect 80 classes. However, for the detection of traffic, only six classes are relevant.</p>"},{"location":"OTLabels/training/cvat/#target-classes","title":"Target classes","text":"<p>Target classes are all road user classes from the initial COCO data set that are relevant for our sample videos:</p> <ul> <li>Person</li> <li>Bicycle</li> <li>Motorcycle</li> <li>Car</li> <li>Bus</li> <li>Truck</li> </ul> <p>All vehicles or combinations of vehicles are labeled as \"truck\" if they have</p> <ul> <li>twin tires (except for regular service buses or coaches),</li> <li>a superstructure or a loading area or</li> <li>a trailer (also cars with trailer, the 2D box includes vehicle and trailer).</li> </ul> <p>Here, as in the COCO dataset, both the \"person\" and the \"bicycle\" are labeled separately for cyclists.</p>"},{"location":"OTLabels/training/cvat/#object-dimensions","title":"Object dimensions","text":"<p>The 2D box boundaries are defined by the outermost parts of the objects visible in that frame of the video.</p> <p>Even if only a part of an object is visible and the class of the object is recognizable for the annotator, it should be labeled.</p>"},{"location":"OTLabels/training/cvat/#projects-tasks-and-jobs","title":"Projects, Tasks and Jobs","text":"<p>We define certain set of videos as a project. Every project has multiple tasks (each consists of one video to annotate). Each task has a status (\"Pending\", \"In Progress\" and \"Completed\") and a progress bar showing the number of completed jobs. Open the task with click on \"Open\". For each task (or video) multiple jobs with a certain amounts of frames from the video are defined. Each job has a status (\"annotation\", \"validation\" and \"completed\"), an \"assignee\" (responsible for annotation) and a \"reviewer\" (responsible for reviewing the assignees annotation). Open the job with click on \"Job #...\".</p>"},{"location":"OTLabels/training/cvat/#import-datafix","title":"Import datafix","text":"<p>Coming soon</p> <p>Unfortunately, there is no content here yet. But we are currently working on completing this site.</p>"},{"location":"OTLabels/training/cvat/#workflow","title":"Workflow","text":"<p>For each job, the project coordinator assigns the roles of the \"assignee\" (= annotator) and the \"reviewer\". The reviewer then starts working on a job by initial labeling of the video frames. The assignee should save his labeling work from time to time and can also stop and later resume working on a job. After finishing the initial labeling, the assignee requests a review (menu - request review, the assigned reviewer should already be named there) and thus sets status of the job to \"validation\". Then the reviewer checks the initial labeling done by the assignee and raises issues when there is a problem or question. After reviewing all video frames of the job, the reviewer submits the review by either accepting and completing the job or rejecting it (which sets the status of the job bach to \"Annotation\"). Now the assignee has to check the issues, update the labels, resolve the issues and again request a review. The review process starts again and if necessary also next annotations, until the reviewer accepts the job and it is marked as \"Completed\". </p> <p>We recommend the following procedure for annotation in CVAT:</p> <ul> <li>Go through all pre-labeled objects on the right</li> <li>Check and delete false positive labels (if there is no object)</li> <li>Check and delete duplicate labels for the same object</li> <li>Check and correct object class for target classes</li> <li>Check and delete objects from other classes if they are classified wrong</li> <li>Zoom in on one quadrant of the image at a time:</li> <li>Check and correct position of object\u00b4s 2D boxes</li> <li>Check if there are objects from the target classes not already pre-labeled, draw new 2D box and annotate object class</li> </ul>"},{"location":"OTLabels/training/cvat/#download-data","title":"Download data","text":"<p>Coming soon</p> <p>Unfortunately, there is no content here yet. But we are currently working on completing this site.</p>"},{"location":"OTLabels/training/preprocessingdata/","title":"Preprocessing Data","text":"<p>Before retraining the YOLOv5 model, you might want to preprocess your labeled data to ensure that your frames are in the right format.</p>"},{"location":"OTLabels/training/preprocessingdata/#convert-cvat-output-to-yolov5","title":"Convert CVAT output to YOLOv5","text":"<p>OTLabels provides the <code>cvat_to_yolo.py</code> script to convert the annotated data to the appropriate format YOLOv5 needs for retraining the model. Before executing the script, you have to setup the configurations for your needs:</p> <ul> <li>Within the script:</li> <li>Set destPath: path where your data is stored (typically ./data/*).</li> <li>Set annFile: path to the COCO-annotation file (if you do not have it yet, see below how to get it).</li> <li>Set catFile: path to a text file containing your CVAT labels (standard: labels_CVAT.txt).</li> <li>Set cvatFile: path to a CVAT output file (containing images and labels, see also section CVAT: Download data) or to a folder containing multiple CVAT output files.</li> <li>Set name: the name of subfolder of destPath/images and destPath/labels to store the data in.</li> <li>labels_CVAT.txt: a text file with two columns separated by actual commas with headers named Cat and CatId containing the name and the ID of your CVAT labels.   For a example, see the labels_CVAT.txt file in the OTLabels repository.   Please note that labels not provided in this file will not be converted and consequently be deleted.</li> </ul> <p>The script performs the following steps:</p> <ol> <li>Unzip the CVAT output file</li> <li>Copy the images to the directory destPath/images/name.</li> <li>Import the label files and convert the CVAT labels to the COCO labels used by YOLOv5.</li> <li>Export the converted label files to the directory destPath/labels/name.</li> </ol>"},{"location":"OTLabels/training/preprocessingdata/#filter-the-labels","title":"Filter the labels","text":"<p>If for some reason you want to filter the labels that are already stored in your data directory, you can use the filter_labels.py script. Before executing the script, you have to setup the configurations for your needs:</p> <ul> <li>Within the script:</li> <li>Set path: path where the data is stored (typically ./data/*).</li> <li>Set name: the name of one or more subfolder of destPath/images and destPath/labels to store the data in.     More than one name must be provided as list.</li> <li>Set annFile: path to the COCO-annotation file (if you do not have it yet, see below how to get it).</li> <li>Set labelsFilter: path to a text file (standard: label_filter.txt) containing the labels you want to keep.</li> <li>label_filter.txt: a text file containing the category names (e.g., \"car\") of the labels you want to keep (without quotes and one category name per row).</li> </ul> <p>Please note that the images will not be copied to the corresponding folder in destPath/images in order to save file space. Instead, a text file with all images still containing the filtered labels is created. This file of filenames can also be referred to by YOLOv5.</p> <p>The script performs the following steps:</p> <ol> <li>Fetch the category IDs to the corresponding category name.</li> <li>Import the label files.</li> <li>Filter the labels by the provided category names.</li> <li>Export the labels to the directory destPath/labels/name_filtered.</li> <li>Create a text file with all image files in the directory path. Please note that image and label files not include any label after filtering are not exported.</li> </ol>"},{"location":"OTLabels/training/preprocessingdata/#get-coco-annotation-file","title":"Get COCO annotation file","text":"<p>This file is necessary to ensure your labels match the labels of the pretrained YOLOv5 models. Use the get_coco_annotation_files.py script to download one or more COCO annotation files. Configurations to be made:</p> <ul> <li>Within the script:</li> <li>Set path: path where the data is stored (typically ./data/*).</li> <li>Set URLFile: path to the config file that stores the URLs of the annotation files</li> <li>coco_annotation_json_URLs.txt: a text file containing the URLs of the annotation files (without quotes and one URL per row)</li> </ul>"},{"location":"OTLabels/training/preprocessingdata/#get-the-original-coco-dataset","title":"Get the original COCO dataset","text":"<p>In some cases you might want to get the original COCO dataset that was used to train the original YOLOv5 models. Therefore, OTLabels provides the get_coco_data.py script to download the data. Please note that you could also use the get_coco.sh script from the YOLOv5 repository. Before executing the script, you have to set up the configurations for your needs:</p> <ul> <li>Within the script:</li> <li>Set imageURLs: path of a text file (standard: coco_image_URLs.txt) containing the URLs of the image data sets.</li> <li>Set annURL: URL to the labels.</li> <li>Set destPath: path where your data is stored (typically ./data/*).</li> <li>coco_image_URLs.txt: a text file containing the URLs of the images for training, validation and testing (without quotes and one URL per row).</li> </ul>"},{"location":"OTLabels/training/retrainmodel/","title":"Retraining a Model","text":"<p>Although the pretrained YOLOv5 models (especially the bigger ones like YOLOv5l or YOLOv5xl) return acceptable detection results, we experienced some shortcomings for certain object classes which are not well represented and/or distributed in the COCO dataset used for training the YOLOv5 models.</p> <ol> <li>Especially county specific differences were identified, since the COCO dataset contains mainly pictures taken in North-American areas (US trucks vs. European trucks).</li> <li>Also, less represented object classes (e.g., bicycles) might cause worse detection rate or even false detections and hence, to an uneven detection accuracy between the object classes.</li> <li>This might lead to biases in the final counts (e.g., every car is detected, but only every second motorcycle).</li> </ol> <p>Further, one might condense the 80 classes of the pretrained YOLOv5 models to the relevant six classes (pedestrian, bicycle, car, bus, truck, motorcycle) for detecting moving traffic to reduce noise from non-relevant classes.</p>"},{"location":"OTLabels/training/retrainmodel/#first-step-setting-up-the-config-files","title":"First step: setting up the config files","text":"<p>For retraining, there are two relevant config files that needs to be set up.</p>"},{"location":"OTLabels/training/retrainmodel/#data-structure-and-labels","title":"Data structure and labels","text":"<p>YOLOv5 needs a config file in yaml format that contains information about file locations and the labels. This config file is usually stored in the data folder in the OTLabels directory.</p>"},{"location":"OTLabels/training/retrainmodel/#data_structure_labelsyaml","title":"data_structure_labels.yaml","text":"<pre><code># COCO 2017 dataset http://cocodataset.org\n# Train command: python train.py --data coco.yaml\n# Default dataset location is next to /yolov5:\n#   /parent_folder\n#     /coco\n#     /yolov5\n\n# download command/URL (optional)\n# download: bash data/scripts/get_coco.sh\n\n# train and val data as \n# 1) directory: path/images/, \n# 2) file: path/images.txt, or \n# 3) list: [path1/images/, path2/images/]\n\ntrain: ../OTLabels/data/path_to_structure_file_training.txt\nval: ../OTLabels/data/coco/path_to_structure_file_validation.txt\ntest: ../OTLabels/data/coco/path_to_structure_file_test.txt\n\n# number of classes\nnc: 6\n\n# class names\nnames: [\"person\", \"bicycle\", \"car\", \"motorcycle\", \"bus\", \"truck\"]\n</code></pre>"},{"location":"OTLabels/training/retrainmodel/#model-structure","title":"Model structure","text":"<p>Further, the configuration file containing the model structure (also in yaml format) needs to be set up or adapted. These configuration files are usually stored in the models folder within the yolov5 directory. Natively, YOLOv5 comes with one configuration file for each model (e.g., yolov5s.yaml). We strongly recommend to keep the model structure itself as it is and only adapt the number of classes, since the retraining process is based on the trained standard weights, which rely on the original structure.</p>"},{"location":"OTLabels/training/retrainmodel/#model_structureyaml","title":"model_structure.yaml","text":"<pre><code># parameters\nnc: 6  # number of classes\n...\n</code></pre>"},{"location":"OTLabels/training/retrainmodel/#second-step-connecting-to-wandb","title":"Second step: connecting to wandb","text":"<p>If you want to have your training process logged and visualized, YOLOv5 comes with the option to connect to weights and biases (wandb). For further instructions, please visit the wandb issue on GitHub. Otherwise you can chose the option not to use wandb when asked.</p>"},{"location":"OTLabels/training/retrainmodel/#third-step-retraining-the-model","title":"Third step: retraining the model","text":"<p>Now, you are ready to start with the retraining of your models. To start the process, simply execute the train.py with the desired options.</p> <p><code>python yolov5/train.py --img **img_size** --batch **batch_size** --epochs **n_epochs** --data path_to/**data_structure_labels**.yaml --weights yolov5/weights/**yolov5_weights** --cfg path_to/**model_structure**.yaml</code></p> <p>For further information about the whole retraining process with YOLOv5, please see the original documentation on GitHub.</p>"},{"location":"OTLabels/validation/gettingstarted/","title":"Getting Started","text":"<p>This section provides a guide on how to install and use OTValidate.</p>"},{"location":"OTLabels/validation/gettingstarted/#installation","title":"Installation","text":"WindowsLinux/ macOS IntelApple M1 <p>Note</p> <p>Installation instructions for Windows are following soon.</p> <p>Install OTValidate by cloning the repository  with git:</p> <pre><code>git clone https://github.com/OpenTrafficCam/OTValidate.git\ncd OTValidate\npip install -r requirements.txt\npip install .\n</code></pre> <p>Or install by using the <code>Makefile</code>:</p> <pre><code>git clone https://github.com/OpenTrafficCam/OTValidate.git\ncd OTValidate\nmake install\n</code></pre> <p>The installation for machines running on Apple's M1 chip is not as straightforward. There are two ways to install <code>OTValidate</code> on an M1 Mac. As a prerequisite the package manager Homebrew is required.</p> <ol> <li> <p>By executing these commands in the following order:</p> <pre><code>git clone https://github.com/OpenTrafficCam/OTValidate.git\ncd OTValidate\nbrew install openblas\nOPENBLAS=$(brew --prefix openblas) CFLAGS=\"-falign-functions=8 ${CFLAGS}\" pip install scipy==1.7.2\npip install -r requirements.txt\npip install .\n</code></pre> </li> <li> <p>By using the <code>Makefile</code></p> <pre><code>git clone https://github.com/OpenTrafficCam/OTValidate.git\ncd OTValidate\nmake install_m1\n</code></pre> </li> </ol>"},{"location":"OTLabels/validation/gettingstarted/#prerequisites","title":"Prerequisites","text":""},{"location":"OTLabels/validation/gettingstarted/#image-annotation-data","title":"Image Annotation Data","text":"<p>The folder containing the ground truth annotations of the images need to be in the YOLO format:</p> <pre><code>annotation_data\n\u2502   obj.data\n\u2502   obj.names\n\u2502   train.txt\n\u2502\n\u2514\u2500\u2500\u2500obj_train_data\n    \u2502   frame_01.png\n    \u2502   frame_01.txt\n    \u2502   frame_02.png\n    \u2502   frame_02.txt\n    \u2502   ...\n    \u2502\n</code></pre>"},{"location":"OTLabels/validation/gettingstarted/#usage","title":"Usage","text":""},{"location":"OTLabels/validation/gettingstarted/#analyse-object-detection-performance","title":"Analyse Object Detection Performance","text":""},{"location":"OTLabels/validation/gettingstarted/#quickstart-guide","title":"Quickstart Guide","text":"<pre><code>from OTValidate  import evaluate_detection_performance\n\n# path to the directory containing the annotated dataset in otdet format\ngt_data = \"path/to/data1\"\n\n# model weights\nmodel1 = \"path/to/model_weights1.pt\"\nmodel2 = \"path/to/model_weights2.pt\"\nmodel3 = \"path/to/model_weights3.pt\"\n</code></pre> <p>Use the <code>evaluate_detection_performance</code> function to calculate a set of object detection metrics of the respective models:</p> <pre><code>evaluate_detection_performance(\n    path_to_model_weights=[model1, model2, model3],\n    yolo_path=yolo_path,\n    otdet_gt_dir=gt_data,\n    is_gt_xyxy_format=False, # whether the ground truth's bounding box is in xyxy or xywh format\n    normalized=True,\n)\n</code></pre>"},{"location":"OTLabels/validation/gettingstarted/#results","title":"Results","text":"<p>The evaluation results of the models will be saved in the directories containing the annotation data. An <code>out</code> directory containing all the results will be created there.</p>"},{"location":"OTLabels/validation/metrics/","title":"Performance Metrics","text":"<p>In this section we briefly go over the different metrics to evaluate our models on how they fare in the tasks of object detection and object tracking. Furthermore, we will look into the different metrics used to evaluate the results in terms of traffic analysis.</p>"},{"location":"OTLabels/validation/metrics/#object-detection","title":"Object Detection","text":"<p>In the task of object detection a model is considered to be good if it is able to detect and classify an object correctly. In this section we are going to have a look in the different object detection performance metrics.</p>"},{"location":"OTLabels/validation/metrics/#intersection-over-union-iou","title":"Intersection Over Union (IOU)","text":"<p>We can tell if a predicted bounding box matches a ground truth bounding box by calculating and looking at the IOU of the two bounding boxes. As Padilla et al. explained in the paper<sup>1</sup>, \"a perfect match is considered when the area and location of the predicted and ground-truth boxes are the same\". Therefore, the IOU is calculated by determining the area of the intersection of the two bounding boxes and dividing it by the area of the union of the two bounding boxes as shown in:</p> <p></p> <p>Illustration adapted from the paper \"Analysis of Object Detection Metrics with a Companion Open-Source Toolkit\"</p> <p>Thus, two bounding boxes are considered a perfect match if the IOU = 1. Meaning the predicted and ground truth bounding boxes share the same location and the same size.</p> <p>On the other hand, the IOU = 0 when there is no intersection between the predicted and the ground truth bounding box.</p> <p>Usually an IOU threshold is defined in order to decide whether a predicted and ground truth bounding box are considered a match.</p>"},{"location":"OTLabels/validation/metrics/#true-positives-false-positives-false-negatives","title":"True Positives, False Positives, False Negatives","text":"<p>This section will explain what true positives, false positives, false negatives and true negatives are in the task of object detection. Thus, we will look at their definitions as defined by Padilla et al. <sup>1</sup>:</p> <p>A True Positive is a correct detection of a ground-truth bounding box. An incorrect detection of a non-existing object or a misplaced detection of an existing object is a False Positive. An undetected ground-truth bounding box is named False Negative.</p>"},{"location":"OTLabels/validation/metrics/#precision","title":"Precision","text":"<p>Padilla et al. <sup>1</sup> explain precision as \"the ability of a model to identify only relevant objects. It is the percentage of correct positive predictions.\"</p> <p>Precision is calculated as:</p> \\[ Precision = \\frac{TP}{TP + FP} \\]"},{"location":"OTLabels/validation/metrics/#recall","title":"Recall","text":"<p>Padilla et al. <sup>1</sup> explain recall as \"the ability of a model to find all relevant cases (all ground-truth bounding boxes). It is the percentage of correct positive predictions among all given ground truths.\"</p> <p>Recall is calculated as:</p> \\[ Recall = \\frac{TP}{TP + FN} \\]"},{"location":"OTLabels/validation/metrics/#average-precision-ap","title":"Average Precision (AP)","text":"<p>As Padilla et al. explained that an object detection model \"can be considered good if, when the confidence threshold decreases, its precision remains high as its recall increases\"[ <sup>1</sup> ]. Taking this into account a model with a large area under a precision-recall curve indicates a high precision and a high recall. Therefore, the average precision \"is a metric based on the area under a [precision-recall curve]\" <sup>1</sup> with a selected IOU threshold. Thus the following notation for example, AP@50 denotes the average precision with IOU threshold at 50%.</p>"},{"location":"OTLabels/validation/metrics/#mean-average-precision-map","title":"Mean Average Precision (mAP)","text":"<p>We need to keep in mind that the average precision needs to be calculated for each class individually. Hence, the mean average precision \"is a metric that is able to represent the exactness of the detections among all classes\" <sup>1</sup>.</p> <p>The mAP is calculated as follows:</p> \\[ mAP = \\frac{1}{C} \\sum_{i=1}^{C} AP_i \\] <p>where C is the total number of classes and \\(AP_i\\) is the average precision of class \\(i\\) <sup>1</sup>.</p>"},{"location":"OTLabels/validation/metrics/#tide-metrics","title":"TIDE Metrics","text":"<p>Bolya et al. created TIDE a General Toolbox for Identifying Object Detection Errors<sup>2</sup>. As Bolya et al. explain in their paper<sup>2</sup> \"mAP succinctly summarizes the performance of a model in one number\". Thus, the mAP performance metric does not give us any insight on what and how the different error types influence its score, that is the mAP score. The aim of TIDE is exactly that, to give us this insight on how the different error types affect the mAP score and as Bolya et al. <sup>2</sup> stated giving us \"a comprehensive analysis of each model's strengths and weaknesses\".</p> <p>TIDE defines six main error types as follows:</p> <p>Info</p> <p>The following descriptions of the error types are directly taken from the TIDE source code</p> <ol> <li> <p>Classification Error: Error caused when a prediction would have been marked positive if it had the correct class.</p> </li> <li> <p>Localization Error: Error caused when a prediction would have been marked positive if it was localized better.</p> </li> <li> <p>Both Cls and Loc Error: This detection didn't fall into any of the other error categories.</p> </li> <li> <p>Duplicate Detection Error: Error caused when a prediction would have been marked positive if the GT wasn't already in use by another detection.</p> </li> <li> <p>Background Error: Error caused when this detection should have been classified as background (IoU &lt; 0.1).</p> </li> <li> <p>Missed Ground Truth Error: Represents GT missed by the model. Doesn't include GT corrected elsewhere in the model.</p> </li> </ol>"},{"location":"OTLabels/validation/metrics/#confusion-matrix","title":"Confusion Matrix","text":"<p>The confusion matrix gives us a visual insight on how our object detection model performs in the classification task.</p> <p>Let us have a look first at the confusion matrix of a binary classification problem:</p> <p></p> <p>Illustration adapted from the paper Evaluation: from precision, recall and F-measure to ROC, informedness, markedness and correlation\" by David M. W. Powers</p> <p>The rows of the above confusion matrix represent the predicted class whereas the columns represent the ground truth class. Thus a prediction can be categorized as follows:</p> <ol> <li> <p>A prediction that has been predicted as positive class and that is found to be an actual/real positive class in the ground truth, is counted as a true positive.</p> </li> <li> <p>A prediction that has been predicted as negative class and is found to be an actual/real negative class is counted as a true negative.</p> </li> <li> <p>A prediction that has been predicted as positive and is found to be not an actual/real positive class is counted as a false positive.</p> </li> <li> <p>A prediction that has been predicted as negative and is found to be not an actual/real negative class is counted as a false negative.</p> </li> </ol> <p>Hence, the confusion matrix gives us a clear visualization of how many of our predictions were classified correctly or incorrectly.</p> <p>The confusion matrix of multi classification problem looks a little bit different:</p> <p></p> <p>As the image above implies, we now have multiple classes. For this example we want to classify the class <code>car</code>, <code>person</code> and <code>truck</code>. The green color coded tiles denote the true positive predictions.</p> <p>Let's take for example the row denoted with the class <code>car</code>. Here is how it is to be interpreted: Out of the 8 cars that have been predicted:</p> <ul> <li>three were correctly classified as a <code>car</code>,</li> <li>none were incorrectly classified as a <code>person</code> and</li> <li>5 were incorrectly classified as <code>truck</code></li> </ul>"},{"location":"OTLabels/validation/metrics/#traffic-measures","title":"Traffic Measures","text":"<p>To see, how well OpenTrafficCam performs see OTAnalytics.</p>"},{"location":"OTLabels/validation/metrics/#references","title":"References","text":"<ol> <li> <p>Padilla, R., Passos, W. L., Dias, T. L., Netto, S. L., &amp; da Silva, E. A. (2021). A comparative analysis of object detection metrics with a companion open-source toolkit. Electronics, 10(3), 279. https://doi.org/10.3390/electronics10030279 \u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Bolya, D., Foley, S., Hays, J., &amp; Hoffman, J. (2020). Tide: A general toolbox for identifying object detection errors. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part III 16 (pp. 558-573). Springer International Publishing. https://dbolya.github.io/tide/paper.pdf \u21a9\u21a9\u21a9</p> </li> </ol>"},{"location":"OTLabels/validation/modelvalidation/","title":"Model Validation","text":"<p>In this section you will find a comparison of the different YOLO models on how well they perform in the object detection. The YOLO models to be evaluated are YOLOv5s, YOLOv5m, YOLOv5l and YOLOv5x. The models are evaluated on a custom dataset consisting of custom video recordings. Thus, we want to investigate which of the four YOLO models might be best suited in the detection of traffic objects.</p>"},{"location":"OTLabels/validation/modelvalidation/#dataset","title":"Dataset","text":"<p>As mentioned above, the dataset consists of different video recordings capturing different traffic scenes. Thus, the video recordings are turned into datasets that will be used to evaluate the object detection models. It is important to note that not every single frame is selected, but only every \\(n\\)-th frame where \\(n \\in \\mathbb{N}\\) and \\(n\\) is arbitrarily chosen by the user.</p> <p>Here is how the scenes look like:</p> Scenes <p>As seen above, there are two instances where scenes were also recorded at night. This makes it interesting to see how the YOLO models perform in detecting traffic objects in low light conditions.</p> <p>The class labels to be considered for our evaluation of the YOLO models are:</p> <ul> <li>person</li> <li>bicycle</li> <li>car</li> <li>motorcycle</li> <li>bus</li> <li>truck</li> </ul>"},{"location":"OTLabels/validation/modelvalidation/#evaluation-procedure","title":"Evaluation Procedure","text":"<p>The YOLO models are evaluated with the help of OTValidate. As a prerequisite, what OTValidate needs in order to start the evaluation are the YOLO models and the labeled ground truth data. In our case the ground truth labeled data is our custom dataset, which needs to be in the CVAT YOLO format. As for the YOLO model, a custom trained or an existing model can be loaded into OTValidate.</p> <p>OTValidate then uses each model to predict the ground truth images. Afterwards, the prediction results and ground truth data are used to calculate the different object detection metrics for model comparison. In our case, we will use the mAP and the TIDE metrics.</p> <p>Info</p> <p>OTValidate uses a list of class labels as a parameter. All predictions and ground truth data are then filtered according to that list of class labels. Meaning, all predictions or ground truth data, whose predicted or labeled class are not contained in the list, are discarded and therefore not regarded in the evaluation process. This is especially useful if the ground truth data contains class labels that the model can't predict. </p>"},{"location":"OTLabels/validation/modelvalidation/#evaluation","title":"Evaluation","text":"<p>In this section you will find the evaluation results of each model as well as the comparison. Specifically, the models are evaluated based on each dataset depicted by the scenes as shown above in the table and on all the ground truth data. The models confidence and IOU threshold are set to 0.25 and 0.5 respectively. Meaning, all detections that have an IOU lower than 0.5 are not regarded as possible detections and all detections with a confidence lower than 0.25 are discarded.</p>"},{"location":"OTLabels/validation/modelvalidation/#evaluation-on-all-data","title":"Evaluation on all data","text":"<p>This diagram depicts each models mAP at different IOU thresholds:</p> <p></p> <p>We can clearly see that the YOLOv5s model's mAP is the lowest compared to the other models. Meaning, by only looking at the mAP metric the YOLOv5s, YOLOv5m, and the YOLOv5x models are to be considered.</p> <p>Nevertheless, let us also have a look at the TIDE metrics to get an insight on the types of errors made by the models:</p> TIDE Metrics <p>What immediately stands out are the Missed Ground Truth Error (Miss) and the Classification Error (Cls). Meaning, the models were not able to detect many ground truth bounding boxes or be able to classify them correctly. Thus, the models were able to detect the majority of the bounding boxes, but had problems predicting the correct classes.</p> <p>The YOLOv5s model had the highest classification error rate out of the four models. Although, the missed ground truth error rate is also the highest, it does not differ much from the other three models.</p> <p>It becomes apparent that the bigger models perform better at detecting bounding boxes than the smaller ones. But there is also a point where the models' performance, namely the YOLOv5m, YOLOv5l and YOLOv5x, don't differ much at all. Implying that there is not much of a trade-off in choosing the YOLOv5m or YOLOv5l over the YOLOv5x.</p> <p>Still, let us try to find out why the  Missed Ground Truth Error appears to be a problem for all four models by evaluating them on the data of each scene.</p>"},{"location":"OTLabels/validation/modelvalidation/#evaluation-on-each-dataset-of-a-scene","title":"Evaluation on each dataset of a scene","text":""},{"location":"OTLabels/validation/modelvalidation/#scene-1-2","title":"Scene 1 &amp; 2","text":"Scene 1 Scene 2 <p>Scene 1 captures a three way junction. Compared to the combined dataset, the models' mAP value doesn't differ that much. The missed ground truth error of all four models is above 20%. Our assumption as to why the missed ground truth error is so high might be due to moving objects appearing in the distance. The models might not be able to detect these objects as they appear to be very small in the picture, as marked in red bounding boxes in the following image:</p> <p></p> <p>It is also possible that the timestamp covers up parts of objects making the models not be able to detect them.</p> <p>Scene 2 is similar to scene 1 in respect to the metrics calculated on all four models. The models' mAP evaluated on the dataset of scene 2 is higher than those of scene 1 when looking at the YOLOv5's <code>m</code>, <code>l</code> and <code>x</code> models. Scene 2's TIDE errors behave also very similar to those of scene 1. Also, small objects that have been annotated with small bounding boxes appear in images of the dataset of Scene 2. This could also be related to the high Missed Ground Truth Error.</p> <p></p>"},{"location":"OTLabels/validation/modelvalidation/#scene-3","title":"Scene 3","text":"<p>Scene 3 captures a four-way junction. There are two datasets capturing scene 2 at daylight and nighttime respectively.</p> Scene 3 At Night <p>Comparing these two datasets, it is not surprising that the models' mAP evaluated on the dataset taken at daylight fares much better to the one evaluated at nighttime. The low light condition makes it really hard for the models to detect any of the objects. The biggest drop in performance in terms of the mAP metric is seen from the YOLOv5s model.</p> <p>But then, why is the Missed Ground Truth Error (Miss) much lower and the Classification Error (Cls) that much higher of the nighttime dataset of Scene 3 compared to the one taken at daylight? Let's have a closer look at the two datasets first:</p> Scene 3 at daylight Scene 3 at nighttime <p>Overall, the amount of ground truth bounding boxes in scene 3 taken at daylight is more than twice as big as the one taken at nighttime. This implies that there is less traffic at nighttime, which makes sense. Thus, there is not much to detect at night resulting in a lower Missed Ground Truth Error. On the other hand, the classification error for all models is above 40% with the exception of the YOLOv5l model whose classification error is at approximately 24%. In addition to that the Localization Error (Loc) is very low. Meaning, the model is able to detect objects very well, but has trouble assigning them to the correct class in badly lit areas. Here is an example where a car is barely recognizable:</p> <p></p> <p>Speaking of scene 3 at daylight, the MissedGround Truth Error is relatively high across all models which is due to the same reason as explained in Scene 1. Traffic objects whose bounding boxes are really small might be detected by the models resulting in misses.</p> <p>But it is also important to take into account that there is the possibility of not moving objects appearing in every image at the same spot in the dataset. Such case can be seen in scene 3 at nighttime where cars have been parked on the sidewalk in the following image:</p> <p></p> <p>Thus, such case could influence the values of the calculated metrics for better or worse.</p>"},{"location":"OTLabels/validation/modelvalidation/#scene-4","title":"Scene 4","text":"<p>Scene 4 captures a rural road.</p> Scene 4 <p>The Missed Ground Truth Error (Miss) is pretty high for the YOLOv5s and the YOLOv5x models with an error value over 50% compared to the YOLOv5m and YOLOv5l models. At this moment, we could not come up with an explanation of why the YOLOv5x model's missed ground truth error is that much higher than its <code>m</code> and <code>l</code> counterpart. But there is an explanation why the missed ground truth error is high across all models. The reason lies in the dataset of scene 4 itself:</p> Scene 4 - images with cut off objects <p>The images above contain objects, which are surrounded by red bounding boxes, that are only partly in their respective image. Therefore, a model might not have been able to detect these objects due to a low confidence score and thus resulting in misses. These type of images appear often in the dataset. Another thing to keep in mind is that there is not much traffic on this rural road. Most images in the dataset do not contain bounding box annotations.</p> <p></p> <p>There are a total of 110 labeled objects in a dataset consisting of 401 images. As a result, even a small number of cut off objects could have a great impact on the metrics.</p>"},{"location":"OTLabels/validation/modelvalidation/#scene-5","title":"Scene 5","text":"<p>Scene 5 captures a rural road.</p> Scene 5 At Night <p>Scene 5 is captured at two different times, namely at day and night. The only difference to scene 3 is that scene 5 captures a rural road instead of a four way junction. Unsurprisingly, the models performed tremendously better on the dataset taken during the day compared to the one taken at night when looking at the mAP diagram.</p> <p>It is not surprising that all models evaluated on the nighttime dataset have a high Missed Ground Truth Error (Miss) which is due to the worse light conditions. What is standing out is the fact that the models evaluated on the day dataset also have a high miss error rate. To get behind the possible reason for this we should have quick peek in the ground truth dataset:</p> <p>Objects or vehicles emerging from the road in the upper left corner of the image make it harder for the models to detect and assign them to the correct class. This is reflected in the missed ground truth error and the localization error. Thus the further the objects move to the right area of the image, the better they are detected by the models.</p>"},{"location":"OTLabels/validation/modelvalidation/#conclusion-and-future-work","title":"Conclusion and Future Work","text":"<p>We have evaluated the <code>YOLOv5s</code>, <code>YOLOv5m</code>, <code>YOLOv5x</code>, and the <code>YOLOv5l</code> object detection models on a custom dataset consisting of videos capturing different scenes of traffic junctions and roads. Looking at mAP and TIDE metrics gave us insight to how the YOLOv5 models performed on our custom dataset and we can come to the conclusion that a <code>YOLOv5m</code>, <code>YOLOv5l</code> and <code>YOLOv5x</code> models performed far better than the <code>YOLOv5s</code>. But there is not a big increase in performance in terms of object detection upon choosing the <code>YOLOv5x</code> over the <code>YOLOv5m</code> or <code>YOLOv5l</code> model.</p> <p>Another important aspect to look into in the future is how much time the models under discussion take to detect the images. Depending on the use case and resources at hand, choosing the <code>YOLOv5x</code> model which might need much more time to finish the detection might not be suitable and thus taking the <code>YOLOv5m</code> or <code>YOLOv5l</code> model might be the better choice.</p> <p>What we are also currently working on is to train our own models using the YOLOv5 models as our foundation on a custom dataset. Thus it would be interesting to see if there would be a significant increase in performance by using custom trained models. If there is a significant increase in performance, the next question to ask if it makes sense to invest time in training a custom model.</p>"},{"location":"OTLabels/validation/overview/","title":"Overview","text":"<p>Validation gives insight on how well a model or software performs. In the case of OpenTrafficCam, we want to evaluate and compare different object detection models on how well they perform in object detection and object tracking. Furthermore, we want to analyse the results of OTAnalytics on a set of traffic performance metrics.</p> <p>For this purpose we are currently developing OTValidate which allows the user to analyse and compare the models' performances on object detection and tracking by calculating metrics corresponding to the specific task at hand. As mentioned above OTValidate will also provide tools to analyse the results of OTAnalytics in regards to traffic performance.</p>"},{"location":"OTLabels/validation/overview/#otvalidate","title":"OTValidate","text":"<p>The image below gives a good overview on the structure of <code>OTValidate</code>:</p> <p></p> <p>For the object detection task <code>OTValidate</code> needs two input files namely an <code>.otdet</code> file and the ground truth label data for the object detection task. Alternatively, a custom or an existing model can be given over as an input instead of an <code>.otdet</code> file.</p>"},{"location":"OTVision/","title":"Features","text":"<p>The heart of OpenTrafficCam.</p> <p>OTVision is a collection of algorithms to generate trajectories of road users from traffic videos. Most of the algorithms have been developed in previous open source projects. We modify them and link them with our own developments into a working pipeline.</p> <p>The current pipeline consists of three core functionalities of OTVision: convert, detect and track.</p> <pre><code>graph LR\n    subgraph OTVision[\"OTVision:\"]\n        direction LR\n        conv(&lt;b&gt;convert&lt;/b&gt;\n        ...raw video\n        files to mp4)\n        det(&lt;b&gt;detect&lt;/b&gt;\n        ...road users\n        in single\n        frames)\n        tr(&lt;b&gt;track&lt;/b&gt;\n        ...detected road\n        users over\n        multiple frames)\n        conv --&gt; det --&gt; tr\n    end\n    tr .-&gt; traj[/.ottrk\n    Trajectories/]\n    vf[/.mp4\n    Video Files/] .-&gt; det\n    rvf[/.h264\n    Video Files/] .-&gt; conv</code></pre>"},{"location":"OTVision/#key-features","title":"Key features","text":"<ul> <li>Can be used without programming knowledge</li> <li>Conversion of .h264 video files to other formats     (using ffmpeg)</li> <li>Detection (joint localization and classification) of road users using     state-of-the-art AI object detection models in single video frames     (currently using YOLOv8 by Ultralytics)</li> <li>Tracking of detected road users over multiple frames     (currently using the     IOU-Tracker by Bochinski et al.)     and over multiple videos.</li> <li>Result: Trajectories in pixel coordinates</li> </ul>"},{"location":"OTVision/advanced_usage/configuration/","title":"Configuration File","text":"<p>If you\u00b4re getting tired of providing all those parameters to the CLI, you can also specify them in a configuration <code>yaml</code> file and pass it\u00b4s path to the CLI instead.</p> <p>By default, OTVision refers to the <code>user_config.otvision.yaml</code> in the root directory.</p> <p>You can either</p> <ul> <li>modify this file (then you don\u00b4t have to specify it\u00b4s path in the CLI) or</li> <li>build your own file and save it somewhere else (then you have to specify it\u00b4s path     in the CLI).</li> </ul> <p>You can specify parameters for the sub-tasks that are provided by OTVision (<code>convert</code>, <code>detect</code>, <code>track</code>) in separate configuration files or in a single file. The scripts for the respective sub-tasks only read the parameters they need.</p> <p>Warning</p> <p>Any parameter passed to the CLI will overwrite the respective parameter from the config file.</p> <p>In case of any problems, we suggest looking at yaml.info first when you customize your configuration files.</p>"},{"location":"OTVision/gettingstarted/firstuse/","title":"First Use","text":"<p>OTVision can be executed on a command-line (e.g., a Terminal on macOS and Linux or the Command Prompt on Windows).</p>"},{"location":"OTVision/gettingstarted/firstuse/#the-command-line-interface","title":"The Command-Line Interface","text":"<p>We provide a command-line interface (CLI) to run the OTVision pipeline steps. This documentation is written for using the Command Prompt on Windows or the integrated terminals in Linux or macOS.</p>"},{"location":"OTVision/gettingstarted/firstuse/#navigate-to-the-otvision-root-directory","title":"Navigate to the OTVision root directory","text":"<p>Open a Terminal and navigate to the OTVision root folder.</p> <pre><code>cd \"path/to/OTVision\"\n</code></pre> <p>Where is the OTVision root directory?</p> <p>It's the folder you downloaded und unzipped.</p> <p>Maybe your OTVision root directory is called <code>OTVision-main</code> after unzipping, if you downloaded it from GitHub. This is the correct directory.</p> <p>Inside the OTVision root directory, there is another directory called <code>OTVision</code> (this child directory is the wrong directory).</p>"},{"location":"OTVision/gettingstarted/firstuse/#activate-virtual-environment","title":"Activate virtual environment","text":"<p>Before using OTVision, you have to activate the virtual environment that was created by running the installation scripts:</p> Windows command promptLinux / macOS terminal <p>Open a Command Prompt an run:</p> <pre><code>.venv\\Scripts\\activate\n</code></pre> <pre><code>source .venv/bin/activate\n</code></pre> <p>The virtual environment should be activated, indicated by the <code>(venv)</code> in braces in front of your current working directory in the terminal.</p>"},{"location":"OTVision/gettingstarted/firstuse/#build-your-command","title":"Build your command","text":"<p>Every command consists of three parts:</p> <ol> <li>Invoke the Python interpreter with <code>python</code>.</li> <li>Specify the pipeline step you want to run (<code>convert.py</code>, <code>detect.py</code> or <code>track.py</code>).</li> <li> <p>Specify parameters for your script:</p> <p>For basic use, you only have to specify one parameter: The path(s) to the data you want to process. You can specify a file or folder path (in quotation marks) after the <code>-p</code> (or <code>--paths</code>) argument.</p> Some hints about specifying the paths <p>You can just drag a file or folder and drop them into the terminal.</p> <p>If you provide a path to a folder, every file within the folder will be processed.</p> <p>You can also provide multiple paths straight after one another (each in quotation marks).</p> <p>All other parameters are optional. They can also be set via arguments in the CLI (see \"Usage\" section) or via a separate configuration file.</p> </li> </ol> <p>Have a look at the basic examples provided below.</p>"},{"location":"OTVision/gettingstarted/firstuse/#convert","title":"Convert","text":"<p>In case you have raw <code>.h264</code> videos (e.g. from OTCamera), you need to convert them to a supported video format (see convert.py) first. Therefore, we provide the <code>convert.py</code> script.</p> <p>To convert <code>.h264</code> videos, run the following command after activating the venv:</p> <pre><code>python convert.py -p \"path/to/your/h264 files\"\n</code></pre> <p>where <code>path/to/h264 files</code> is either the path to a single h264 video file or a folder containing multiple h264 video files.</p> <p>Each converted video will by default be saved as a <code>.mp4</code> file in the same folder with the same name as the input <code>.h264</code> file.</p>"},{"location":"OTVision/gettingstarted/firstuse/#detect","title":"Detect","text":"<p>If you have converted your video files to one of the accepted file formats (<code>.avi</code>,<code>.mkv</code>,<code>.mov</code>,<code>.mp4</code>) or you already have your video files in one of those formats, you are ready to detect road users in each single frame of the video(s). Therefore, we provide the <code>detect.py</code> script.</p> <p>To detect video files, run the following command after activating your venv:</p> <pre><code>python detect.py -p \"path/to/your/video files\" --expected-duration &lt;video duration [sec]&gt;\n</code></pre> <p>where</p> <ul> <li><code>path/to/video files</code> is either the path to a single video file or a folder containing multiple video files and</li> <li><code>video duration [sec]</code> is the duration of the individual videos.</li> </ul> <p>Naming convention for your video files</p> <p>The filenames must contain the date and time of the start of the video in the following pattern:</p> <p><code>YYYY-MM-DD_HH-MM-SS</code></p> <p>Example: <code>path/to/my/video_2023-04-26_12-15-00.mp4</code></p> <p>By specifying this, OTVision can link the detections and tracks in successive videos so that there are no 'cracks' through many individual videos.</p> <p>If you use OTCamera to record your videos, the video files are automatically named according to this pattern.</p> <p>If you do not know the start date and time, you can simply use a dummy date (e.g. <code>1970-01-01_00-00-00</code>).</p> <p>For each video file, the detected objects will be written to a separate <code>.otdet</code> file in the same folder with the same name as the input video file.</p>"},{"location":"OTVision/gettingstarted/firstuse/#track","title":"Track","text":"<p>With the detected road users in one or more <code>.otdet</code> files, you are ready to track the road users over multiple frames of the video (or even over multiple consecutive videos). Therefore, we provide the <code>track.py</code> script.</p> <p>To track <code>.otdet</code> files, run the following command after activating your venv:</p> <pre><code>python track.py -p \"path/to/your/otdet files\"\n</code></pre> <p>where <code>path/to/otdet files</code> is either the path to a single <code>.otdet</code> file or a folder containing your <code>.otdet</code> files. If you provide a folder, every <code>.otdet</code> file within the folder will be tracked.</p> <p>For each video file, the tracks will be written to a separate <code>.ottrk</code> file in the same folder as the input <code>.otdet</code> file.</p>"},{"location":"OTVision/gettingstarted/installation/","title":"Installation","text":"<p>In this section, we provide instructions how to install OTVision on the most common Operating Systems.</p> <p>Before installing OTVision, make sure your system meets all requirements.</p>"},{"location":"OTVision/gettingstarted/installation/#install-otvision","title":"Install OTVision","text":"<p>We provide install scripts for the most common operating systems.</p> <p>Download and unzip the latest version of OTVision from GitHub or clone the OTVision repository.</p> WindowsLinux / macOS <p>Inside the unzipped folder open the <code>install.cmd</code> and wait until the installation of the dependencies is complete.</p> <p>In a terminal, navigate to the OTVision folder and run the installer.</p> <pre><code>./install.sh\n</code></pre> <p>The installation of the dependencies could take a moment.</p> What is installed here? <p>The <code>install</code> script will create and activate a virtual environment (venv) and install the Python packages specified in the requirements.txt via pip from the Python Package Index.</p>"},{"location":"OTVision/gettingstarted/installation/#nvidia-cuda-optional","title":"Nvidia CUDA (optional)","text":"<p>If you have a Windows or Linux PC with a Nvidia graphics card and already installed CUDA, you chose the release with the suffix <code>-cuda</code>. It contains the requirements to use CUDA. If you want to contribute to OTVision and use CUDA, you have to perform additional steps in your Terminal/Command Prompt:</p>"},{"location":"OTVision/gettingstarted/installation/#check-cuda-version","title":"Check CUDA version","text":"<p>Check if CUDA is recognized and available.</p> <pre><code>nvcc --version\n</code></pre> <p>Navigate to the OTVision root directory.</p> <pre><code>cd \"path/to/OTVision\"\n</code></pre> <p>Where is the OTVision root directory?</p> <p>It's the folder you downloaded und unzipped.</p> <p>Maybe your OTVision root directory is called <code>OTVision-main</code> after unzipping, if you downloaded it from Github. This is the correct directory.</p> <p>Inside the OTVision root directory, there is another directory called <code>OTVision</code> (this child directory is the wrong directory).</p>"},{"location":"OTVision/gettingstarted/installation/#activate-virtual-environment","title":"Activate virtual environment","text":"<p>Activate the virtual environment that was created by running the installation scripts.</p> Windows command promptLinux / macOS terminal <p>Open a Command Prompt an run:</p> <pre><code>.venv\\Scripts\\activate\n</code></pre> <pre><code>source .venv/bin/activate\n</code></pre> <p>The virtual environment should be activated, indicated by the <code>(venv)</code> in braces in front of your current working directory in the terminal.</p>"},{"location":"OTVision/gettingstarted/installation/#install-torch-and-torchvision-for-cuda","title":"Install torch and torchvision for CUDA","text":"<p>If you downloaded a <code>-cuda</code> release, you are good to go, if the CUDA version in the <code>requirements.txt</code> matches your system.</p> <p>To install another version you can do so follwing the PyTorch documentation:</p> <p>Depending on your operating system (Windows or Linux) and your CUDA version you can select, copy and run the install command from the PyTorch site under \"INSTALL PYTORCH\" (choose Build=\"Stable\", Package=\"pip\" and Language=\"Python\").</p> <p>E.g., for CUDA 11.6 and the latest stable PyTorch Build, the command is:</p> <pre><code>pip3 install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu116\n</code></pre>"},{"location":"OTVision/gettingstarted/installation/#if-you-encounter-problems","title":"If you encounter problems","text":"<p>Maybe you also have to install Microsoft Visual C++ 14.0 or greater from the Visual Studio Build Tools.</p> <p>In case of further problems please open an issue in the OTVision repository on GitHub or contact us. We are happy to know about your experience.</p>"},{"location":"OTVision/gettingstarted/installation/#contribute","title":"Contribute","text":"<p>We welcome code contributions (e.g., fixing bugs or adding features) from others by forking the repository and creating a pull request. Please check the contribute section of this documentation first.</p> <p>If you want to contribute code, additional requirements should be installed in the virtual environment. Clone the OTVision repository from GitHub. Run the <code>install_dev.sh</code> in your OTVision folder and wait until the installation of the dependencies is complete.</p>"},{"location":"OTVision/gettingstarted/requirements/","title":"Requirements","text":"<p>OTVision is designed to run on most modern operating systems (Windows, Linux and Mac). However, a few prerequisites are required before installing OTVision.</p>"},{"location":"OTVision/gettingstarted/requirements/#hardware-prerequisites","title":"Hardware prerequisites","text":"<p>OTVision runs on modern desktops and laptops (e.g. Intel i5+ of the last few generations, AMD Zen chips or Apple Silicon processors and 8 GB RAM).</p> <p>If you want to detect road users in videos with OTVision on a regular basis, we strongly recommend purchasing a powerful desktop computer or workstation (&gt;= 8+ cores, &gt;= 16 GB RAM) with a Nvidia graphics card (&gt;= GeForce 10XX Series, better is usually faster). Make sure that the Nvidia drivers as well as the NVidia CUDA Toolkit are installed and up-to-date to get the best performance.</p> <p>Apple Neural Engine</p> <p>Accelerated detection using the Apple Neural Engine is not yet supported. Detection on Apple silicon chips is currently performed using CPU only.</p>"},{"location":"OTVision/gettingstarted/requirements/#software-prerequisites","title":"Software prerequisites","text":""},{"location":"OTVision/gettingstarted/requirements/#python-310","title":"Python 3.10","text":"<p>OTVision is based on Open Source Python packages. Thus, it runs on Windows, macOS and Linux systems after setting up the environment correctly. Consequently, Python needs to be installed on your system. Currently OTVision runs best on Python 3.10. An instruction on how to do that on a Windows system can be found below.</p> What if I want to use another Python version? <p>Python 3.10 is the version we are currently testing against and providing installation scripts for. Other Python versions might work as well, but are not currently tested. In any case, you will have to install the requirements manually or adapt the installation scripts accordingly.</p> WindowsLinuxmacOS <p>If not done yet, install the latest 64-bit version of Python 3.10 via Windows installer (www.python.org/downloads).</p> <p>Please make sure to check the following boxes during installation:</p> <ul> <li> Add Python to PATH</li> <li> Install pip</li> <li> All advanced options</li> </ul> What if I already have another Python version installed? <p>In addition, also install Python 3.10. On most operating systems you can choose the python version to use by using <code>python-3.10</code>oder <code>python3.10</code> styled commands.</p> <p>On Windows, it is also possible to change the default Python interpreter by changing the order of the system-wide environment variables (move Python310 and Python310\\Scripts to the top, see animation below).</p> <p>This is necessary e.g. if you have already installed Python 3.10, but another Python version is your default because you installed it (e.g. 3.11).</p> <p></p> <p>To check your Python installation, run the the follwing commands in a command terminal:</p> <pre><code>python --version\n</code></pre> <p>If <code>Python 3.10.x</code> is returned, you are good to go!</p> <p>Depending on the Linux distribution you are using, Python 3.10 might not be the pre-installed version. You can check if (and which sub-version of) Python 3.10 is installed by running the following command in the terminal:</p> <pre><code>python3.10 --version\n</code></pre> <p>If necessary, install Python 3.10 manually using additional sources.  Since the sources might differ depending on the Linux distribution you are using, we kindly ask you to use a search engine for a detailed instruction on how to install Python 3.10 manually for your distribution. </p> <p>In any case, please make sure that you also have the python virtual environment package <code>python3.10-venv</code> installed.</p> <p>Depending on the macOS version you are using, Python 3.10 might not be the pre-installed version. You can check if (and which sub-version of) Python 3.10 is installed by running the following command in the terminal:</p> <pre><code>python3.10 --version\n</code></pre> <p>If necessary, install Python 3.10 as described below.</p>"},{"location":"OTVision/gettingstarted/requirements/#homebrew","title":"Homebrew","text":"<p>We recommend installing Python using Homebrew: </p> <pre><code>brew install python@3.10\n</code></pre>"},{"location":"OTVision/gettingstarted/requirements/#manual-installation","title":"Manual installation","text":"<p>Alternatively, you can download a macOS installer for python 3.10 from python.org.</p>"},{"location":"OTVision/gettingstarted/requirements/#check-installation","title":"Check installation","text":"<p>Again, run this command in the terminal:</p> <pre><code>python3.10 --version\n</code></pre>"},{"location":"OTVision/gettingstarted/requirements/#ffmpeg","title":"ffmpeg","text":"<p>If you want to use videos recorded by OTCamera with OTVision, you need to convert the videos to .mp4 files first. For the conversion, we use the Open Source software ffmpeg.</p> <p>Before using the <code>convert.py</code> script, make sure that ffmpeg is installed and available on the whole system:</p> WindowsLinuxmacOS <p>To install ffmpeg on your Windows system, please perform the following steps:</p> <ol> <li>Download the file <code>ffmpeg-git-full.7z</code> from     gyan.dev.  </li> <li>Unzip this file by using any file archiver such as 7zip in a folder of     your choice (e.g., <code>C:\\ffmpeg</code>).</li> <li> <p>Now, open a Command Prompt with administrator privileges and set the environment path variable for ffmpeg:</p> <pre><code>setx /m PATH \"path_to_your_ffmpeg_folder\\bin;%PATH%\"\n</code></pre> <p>where <code>path_to_your_ffmpeg_folder</code> represents the folder that you have ffmpeg     unzipped in.</p> <p>If you unzipped to <code>C:\\ffmpeg</code>, for example: </p> <pre><code>setx /m PATH \"C:\\ffmpeg\\bin;%PATH%\"\n</code></pre> </li> <li> <p>Restart your computer and verify the installation by running </p> <pre><code>ffmpeg -version\n</code></pre> </li> </ol>"},{"location":"OTVision/gettingstarted/requirements/#ubuntu-repositories","title":"Ubuntu repositories","text":"<p>If you use Ubuntu, you can install ffmpeg using the official Ubuntu repositories.</p> <pre><code>sudo apt install ffmpeg\n</code></pre>"},{"location":"OTVision/gettingstarted/requirements/#manual-installation_1","title":"Manual installation","text":"<p>For the manual installation of ffmpeg on Linux or if you use another  distribution, please refer to the instructions on the developers website.</p>"},{"location":"OTVision/gettingstarted/requirements/#homebrew_1","title":"Homebrew","text":"<p>For the installation using Homebrew, use the following command in the terminal:</p> <pre><code>brew install ffmpeg\n</code></pre>"},{"location":"OTVision/gettingstarted/requirements/#manual-install","title":"Manual install","text":"<p>For the manual installation of ffmpeg on macOS, please refer to the instructions on the developers website.</p>"},{"location":"OTVision/gettingstarted/requirements/#nvidia-cuda-optional","title":"Nvidia CUDA (optional)","text":"<p>If you intend to use OTVision on a Windows or Linux PC with a modern Nvidia graphics card, download and install version 11.6 of the NVIDIA Cuda Toolkit.</p>"},{"location":"OTVision/usage/convert/","title":"Convert","text":""},{"location":"OTVision/usage/convert/#synopsis","title":"Synopsis","text":"<pre><code>python  convert.py  [-p paths] [-c config]\n                    [--fps_from_filename] [--input_fps]\n                    [--delete_input] [--overwrite]\n</code></pre>"},{"location":"OTVision/usage/convert/#description","title":"Description","text":"<p>In case you use raw <code>.h264</code> videos (e.g. from OTCamera), you need to convert the videos to a supported video format.</p> <p>As <code>.h264</code> does not include metadata, the frame rate of the video has to be either read from the filename or specified by the user.</p> <p>We suggest converting to <code>.mp4</code> (the default output file type).</p>"},{"location":"OTVision/usage/convert/#parameters","title":"Parameters","text":""},{"location":"OTVision/usage/convert/#paths-required","title":"paths (required)","text":"<p><code>-p \"path/to/h264 files\" \"path/to/other h264 files\"</code></p> <p>or</p> <p><code>--paths \"path/to/h264 files\" \"path/to/other h264 files\"</code></p> <p>One or multiple paths to <code>.h264</code> files or folders containing <code>.h264</code> video files.</p> <p>This parameter is required to run the <code>convert.py</code> script. It has to be specified either using the CLI or in the configuration yaml file.</p>"},{"location":"OTVision/usage/convert/#config","title":"config","text":"<p><code>-c \"path/to/config file\"</code></p> <p>or</p> <p><code>--config \"path/to/config file\"</code></p> <p>Path to a custom user configuration yaml file. Other parameters (including <code>paths</code>) are parsed from this configuration file.</p> <p>This parameter is optional. By default, the <code>user_config.otvision.yaml</code> in the root directory is read. This also overrides the default values of all parameters. If the <code>user_config.otvision.yaml</code> in the root directory is missing and no other configuration file is specified, the default values of the parameters as described below are used.</p> <p>Warning</p> <p>Any parameter passed to the CLI will overwrite the respective parameter from the config file.</p>"},{"location":"OTVision/usage/convert/#fps_from_filename","title":"fps_from_filename","text":"<p><code>--fps_from_filename</code> to parse the video frame rates from the input <code>.h264</code> filenames.</p> <ul> <li>In this case the frame rate of each input <code>.h264</code> file has to be specified in the filename using the following pattern: <code>_FR&lt;fps&gt;_</code> (where <code>fps</code> is the frame rate)</li> <li>An example would be <code>_FR20_</code> in the following filename:     <code>OTCamera01_FR20_2023-01-01_12-15-00.h264</code></li> <li>In this case, <code>input_fps</code> will be ignored.</li> </ul> <p>This parameter is used by default if <code>--no-fps_from_filename</code> and <code>--input_fps</code> are not specified.</p>"},{"location":"OTVision/usage/convert/#no_fps_from_filename","title":"no_fps_from_filename","text":"<p>Use <code>--no-fps_from_filename</code> to prevent parsing the video frame rates from the input <code>.h264</code> filenames.</p> <ul> <li>In this case, an <code>input_fps</code> has to be specified.</li> </ul>"},{"location":"OTVision/usage/convert/#input_fps","title":"input_fps","text":"<p><code>--input_fps &lt;int&gt;</code> to set the frame rate for all <code>.h264</code> files.</p> <ul> <li><code>input_fps</code> should be an integer value above zero.</li> <li>E.g. if the input <code>.h264</code> have been recorded at 20 frames per second, specify this parameter as follows: <code>--input_fps 20</code></li> </ul> <p>If <code>--fps_from_filename</code> is used, <code>input_fps</code> will be ignored.</p> <p>This parameter is optional and defaults to <code>20</code>.</p>"},{"location":"OTVision/usage/convert/#overwrite","title":"overwrite","text":"<p><code>--overwrite</code> to overwrite existing <code>.mp4</code> files.</p> <p><code>--no-overwrite</code> to prevent overwriting existing <code>.mp4</code> files.</p> <p>This parameter is optional and defaults to <code>--overwrite</code>.</p>"},{"location":"OTVision/usage/convert/#delete-input","title":"delete input","text":"<p><code>--delete_input</code> to delete input <code>.h264</code> files after conversion to <code>.mp4</code> to save disk space.</p> <p><code>--no-delete_input</code> to keep input <code>.h264</code> files after conversion to <code>.mp4</code>.</p> <p>This parameter is optional and defaults to <code>--no-delete_input</code>.</p>"},{"location":"OTVision/usage/detect/","title":"Detect","text":""},{"location":"OTVision/usage/detect/#synopsis","title":"Synopsis","text":"<pre><code>python  detect.py   [-p paths] [--expected-duration] [-c config]\n                    [-w weights] [--conf] [--iou] [--half] [--force]\n                    [--overwrite] [--detect_start] [--detect_end]\n</code></pre>"},{"location":"OTVision/usage/detect/#description","title":"Description","text":"<p>If you have converted your video files to one of the supported file formats (<code>.avi</code>,<code>.mkv</code>,<code>.mov</code>,<code>.mp4</code>) or you already have such video files, you are ready to start the detection of the road users in each video frame.</p>"},{"location":"OTVision/usage/detect/#parameters","title":"Parameters","text":""},{"location":"OTVision/usage/detect/#paths-required","title":"paths (required)","text":"<p>Filename convention</p> <p>To prevent to set the start date and time in the config file for each individual video file, the current version of OTVision reads the start date and time from the video filenames.</p> <p>Video files recorded by OTCamera already contain the start date and time in the filename.</p> <p>In case you use OTVision to process video files recorded by other camera systems, please make sure that the filename of these files contain the start date and time in the following format: <code>YYYY-MM-DD_hh-mm-ss</code></p> <p><code>-p \"path/to/video files\" \"path/to/other video files\"</code></p> <p>or</p> <p><code>--paths \"path/to/video files\" \"path/to/other video files\"</code></p> <p>One or multiple paths to video files or folders containing video files.</p> <p>This parameter is required to run <code>detect.py</code>. It has to be specified either using the CLI or in the configuration YAML file.</p>"},{"location":"OTVision/usage/detect/#expected-duration","title":"expected-duration","text":"<p><code>--expected-duration &lt;video duration [sec]&gt;</code></p> <p>Expected duration of each video in seconds (must be all the same). This parameter helps to avoid errors if some images are missing in a video.</p> <p>This parameter is optional. By default, the length of the video is read from the video file.</p>"},{"location":"OTVision/usage/detect/#config","title":"config","text":"<p><code>-c \"path/to/config file\"</code></p> <p>or</p> <p><code>--config \"path/to/config file\"</code></p> <p>Path to a custom user configuration YAML file. Other parameters (including <code>paths</code>) are parsed from this configuration file.</p> <p>This parameter is optional. By default, the <code>user_config.otvision.yaml</code> in the root directory is read. This also overrides the default values of all parameters. If the <code>user_config.otvision.yaml</code> in the root directory is missing and no other configuration file is specified, the default values of the parameters as described below are used.</p> <p>Warning</p> <p>Any parameter passed to the CLI will overwrite the respective parameter from the config file.</p>"},{"location":"OTVision/usage/detect/#weights","title":"weights","text":"<p><code>-w &lt;weights&gt;</code> or <code>--weights &lt;weights&gt;</code></p> <p>Name of weights from PyTorch hub or path to weights file.</p> <p>This parameter is optional and defaults to <code>yolov8s.pt</code>.</p>"},{"location":"OTVision/usage/detect/#conf","title":"conf","text":"<p><code>--conf &lt;float&gt;</code></p> <p>The YOLOv8 model confidence threshold. Should be a float value between zero and one.</p> <p>The confidence threshold is the minimum confidence for a detection to be considered a true detection (otherwise this detection will be ignored entirely).</p> <p>This parameter is optional and defaults to <code>0.25</code>.</p>"},{"location":"OTVision/usage/detect/#iou","title":"iou","text":"<p><code>--iou &lt;float&gt;</code></p> <p>The YOLOv8 model IOU threshold. Should be a float value between zero and one.</p> <p>The IOU threshold is the overlap threshold for areas of bounding boxes used in non-maximum suppression to avoid duplicate detections.</p> <p>This parameter is optional and defaults to <code>0.45</code>.</p>"},{"location":"OTVision/usage/detect/#half","title":"half","text":"<p><code>--half</code> to use half precision (FP16) to speed up detection.</p> <p><code>--no-half</code> to not use half precision.</p> <p>This parameter is optional and defaults to <code>--no-half</code>.</p> <p>Half precision only works for running detection on a GPU!</p>"},{"location":"OTVision/usage/detect/#force","title":"force","text":"<p><code>--force</code> to force a reload of a YOLOv8 standard model from PyTorch hub instead of using a cached model from previous detection runs.</p> <p><code>--no-force</code> to prevent forcing this reload.</p> <p>This parameter is optional and defaults to <code>--no-force</code>.</p>"},{"location":"OTVision/usage/detect/#overwrite","title":"overwrite","text":"<p><code>--overwrite</code> to overwrite existing <code>.otdet</code> files.</p> <p><code>--no-overwrite</code> to prevent overwriting existing <code>.otdet</code> files.</p> <p>This parameter is optional and defaults to <code>--overwrite</code>.</p>"},{"location":"OTVision/usage/detect/#start-time","title":"start-time","text":"<p><code>--start-time</code> to specify the start time of the detection in `%Y-%m-%d_%H-%M-%S format. If this parameter is not set, the start time will be parsed from the filename in the given format. The start time needs to be in the filename or given via this parameter.</p> <p>This parameter is optional and defaults to <code>None</code>.</p>"},{"location":"OTVision/usage/detect/#detect-start","title":"detect-start","text":"<p><code>--detect-start</code> to specify the start time of the detection in seconds. Frames occurring before the specified start time will be excluded from the detection process.</p> <p>This parameter is optional and defaults to <code>None</code>.</p>"},{"location":"OTVision/usage/detect/#detect-end","title":"detect-end","text":"<p><code>--detect-end</code> to specify the end time of the detection in seconds. Frames occurring at or after the specified end time will be excluded from the detection process.</p> <p>This parameter is optional and defaults to <code>None</code>.</p>"},{"location":"OTVision/usage/track/","title":"Track","text":""},{"location":"OTVision/usage/track/#synopsis","title":"Synopsis","text":"<pre><code>python  track.py    [-p paths] [-c config]\n                    [--sigma_l] [--sigma_h] [--sigma_iou] [--t_min] [--t_miss_max]\n                    [--overwrite]\n</code></pre>"},{"location":"OTVision/usage/track/#description","title":"Description","text":"<p>With the detected road users in one or more <code>.otdet</code> files, you are ready to start tracking road users over consecutive frames and even consecutive videos.</p>"},{"location":"OTVision/usage/track/#parameters","title":"Parameters","text":""},{"location":"OTVision/usage/track/#paths","title":"paths","text":"<p><code>-p \"path/to/otdet files\" \"path/to/other otdet files\"</code></p> <p>or</p> <p><code>--paths \"path/to/otdet files\" \"path/to/other otdet files\"</code></p> <p>One or multiple paths to <code>.otdet</code> files or folders containing <code>.otdet</code> files.</p> <p>This parameter is required to run the <code>track.py</code> script. It has to be specified either using the CLI or in the configuration yaml file.</p>"},{"location":"OTVision/usage/track/#config","title":"config","text":"<p><code>-c \"path/to/config file\"</code></p> <p>or</p> <p><code>--config \"path/to/config file\"</code></p> <p>Path to a custom user configuration yaml file. Other parameters (including <code>paths</code>) are parsed from this configuration file.</p> <p>This parameter is optional. By default, the <code>user_config.otvision.yaml</code> in the root directory is read. This also overrides the default values of all parameters. If the <code>user_config.otvision.yaml</code> in the root directory is missing and no other configuration file is specified, the default values of the parameters as described below are used.</p> <p>Warning</p> <p>Any parameter passed to the CLI will overwrite the respective parameter from the config file.</p>"},{"location":"OTVision/usage/track/#sigma_l","title":"sigma_l","text":"<p><code>--sigma_l &lt;float&gt;</code></p> <p>The lower confidence threshold for the IOU tracker. Detections with confidences below <code>sigma_l</code> are not considered for tracking at all.</p> <p>This parameter is optional and defaults to <code>0.27</code>.</p>"},{"location":"OTVision/usage/track/#sigma_h","title":"sigma_h","text":"<p><code>--sigma_h &lt;float&gt;</code></p> <p>The upper confidence threshold for the IOU tracker. Tracks are only considered as valid if they contain at least one detection with a confidence above sigma_h.</p> <p>This parameter is optional and defaults to <code>0.42</code>.</p>"},{"location":"OTVision/usage/track/#sigma_iou","title":"sigma_iou","text":"<p><code>--sigma_iou &lt;float&gt;</code></p> <p>Intersection-Over-Union threshold for the IOU tracker. Two detections in subsequent frames are considered to belong to the same track if their IOU value exceeds sigma_iou and this is the highest IOU of all possible combinations of detections.</p> <p>This parameter is optional and defaults to <code>0.38</code>.</p>"},{"location":"OTVision/usage/track/#t_min","title":"t_min","text":"<p><code>--t_min &lt;int&gt;</code></p> <p>Minimum number of detections to count as a valid track. All tracks with less detections will be dismissed.</p> <p>This parameter is optional and defaults to <code>5</code>.</p>"},{"location":"OTVision/usage/track/#t_miss_max","title":"t_miss_max","text":"<p><code>--t_miss_max &lt;int&gt;</code></p> <p>Maximum number of missed detections before continuing a track. If more detections are missing, the track will not be continued.</p> <p>This parameter is optional and defaults to <code>51</code>.</p>"},{"location":"OTVision/usage/track/#overwrite","title":"overwrite","text":"<p><code>--overwrite</code> to overwrite existing <code>.ottrk</code> files.</p> <p><code>--no-overwrite</code> to prevent overwriting existing <code>.ottrk</code> files.</p> <p>This parameter is optional and defaults to <code>--overwrite</code>.</p>"},{"location":"contribute/","title":"Getting Started","text":"<p>Become a part of OpenTrafficCam</p> <p>We are happy if you contribute your own code to OpenTrafficCam. This can be bugfixes in the current code, or even completely new stuff. It is best to start at GitHub to get a first overview.</p> <p>We are also happy to receive reports about the experience using OpenTrafficCam. If you find a bug in the software or the documentation, please report it as an issue in the corresponding repository on GitHub or via email.</p> <p>From time to time we are looking to expand the core team behind OpenTrafficCam. So if you are an enthusiastic developer or engineer who has programming experience and would like to work at the intersection of mobility analysis and planning, software and hardware, feel free to get in touch :)</p> <p>If you have any questions, feel free to send us an email.</p>"},{"location":"contribute/coding/","title":"Coding (Python)","text":"<p>Coding standards are not about right and wrong or good and bad. They are about uniformity, so people can easily read and contribute to the code regardless who worked on the software before.</p> <p>For now, nearly all code is Python so we just agreed on coding conventions for the Python programming language. We basically follow the PEP 8 Style Guide for Python Code. A good summary and some additions can be found in the Hitchhikers Guide or at RealPython.com. Below we defined what we consider the most important standards.</p>"},{"location":"contribute/coding/#naming-conventions","title":"Naming Conventions","text":"<p>All names should be as short as possible but as long as necessary to understand them.</p>"},{"location":"contribute/coding/#general","title":"General","text":"<p>The following table from RealPython.com summarizes the PEP8 naming conventions:</p> Type Naming Convention Examples Function Use a lowercase word or words. Separate words by underscores to improve readability. <code>function, my_function</code> Variable Use a lowercase single letter, word, or words. Separate words with underscores to improve readability. <code>x, var, my_variable</code> Class Start each word with a capital letter. Do not separate words with underscores. This style is called camel case. <code>Model, MyClass</code> Method Use a lowercase word or words. Separate words with underscores to improve readability. <code>class_method, method</code> Constant Use an uppercase single letter, word, or words. Separate words with underscores to improve readability. <code>CONSTANT, MY_CONSTANT, MY_LONG_CONSTANT</code> Module Use a short, lowercase word or words. Separate words with underscores to improve readability. <code>module.py, my_module.py</code> Package Use a short, lowercase word or words. Do not separate words with underscores. <code>package, mypackage</code>"},{"location":"contribute/coding/#files-folder-dirs","title":"Files, Folder, Dirs","text":"Naming Convention Examples dir D:\\tmp\\ filename testfile suffix _detections file type .csv file D:\\tmp\\testfile_detections.csv path \"D:\\tmp\\\" or \"D:\\tmp\\testfile_detections.csv\""},{"location":"contribute/coding/#file-extensions","title":"File extensions","text":"<p>Files get an extension according to their content.</p> Extension Description .otdet detections as bounding boxes .ottrk trajectories in pixel (and UTM) coordinates .otrfpts reference points to convert pixel to UTM coordinates"},{"location":"contribute/coding/#code-documentation","title":"Code documentation","text":"<p>Each module, function, class or method should be described in a docstring (Google style)</p>"},{"location":"contribute/coding/#docstrings-for-modules","title":"Docstrings for Modules","text":"<p>Each file should start with the license snippet followed by a docstring describing the contents and usage of the module:</p> <pre><code>\"\"\"A one line summary of the module or program, terminated by a period.\n\nLeave one blank line.  The rest of this docstring should contain an\noverall description of the module or program.  Optionally, it may also\ncontain a brief description of exported classes and functions and/or usage\nexamples.\n\n  Typical usage example:\n\n  foo = ClassFoo()\n  bar = foo.FunctionBar()\n\"\"\"\n</code></pre>"},{"location":"contribute/coding/#docstrings-for-functions","title":"Docstrings for Functions","text":"<p>Each function must be described by a docstring:</p> <pre><code>def hello_world(message: str = \"Hello World\", author: str = \"Santa Claus\"):\n    \"\"\"Deliver a message from an author to the world.\n\n    \"author says message\"\n\n    Args:\n        message (str, optional): Message to deliver. Defaults to \"Hello World\".\n        author (str, optional): Author name. Defaults to \"Santa Claus\".\n\n    Returns:\n        str: the message said\n    \"\"\"\n    msg = author + \" says \" + message\n    print(msg)\n\n    return msg\n</code></pre> <p>Note</p> <p>If you are using VS Code, you may want to use the Python Docstring Generator extension.</p>"},{"location":"contribute/coding/#comments","title":"Comments","text":"<p>If it is necessary to explain your code in between, use single line or block comments for some (or all) code that follows:</p> <pre><code># This is an example for a single line comment\n\n# This is an \n# example for a\n# block comment\n</code></pre> <p>Try to avoid inline comments.</p>"},{"location":"contribute/coding/#dependencies","title":"Dependencies","text":"<p>We try to develop OpenTrafficCam using as few dependencies as possible. For basic functionality we prefer packages that come with the Python standard distribution (like pathlib or Tkinter). However, OpenTrafficCam would not be possible without code from third party libraries, especially when it comes to videos, images, data analysis, neural networks or web development. These are the libraries we intend to use for specific functionalities throughout the whole framework (we are grateful to the authors):</p> Functionality Library Paths pathlib (distributed with Python since 3.4) Videos, Images OpenCV Neural networks PyTorch, torchvision Data handling pandas, NumPy Shape handling GeoPandas, Shapely Plotting, graphing Plotly User interface Tkinter Web Dash (dashboards)"},{"location":"contribute/coding/#lintingautoformatting","title":"Linting/Autoformatting","text":"<p>To match PEP8 specs and write pretty code we use flake8 to check against structural and style errors and black for autoformatting. Additionally the package flake8-docstrings can be used to lint the docstrings.</p> <p>The following settings are set in the <code>.flake8</code> file:</p> <pre><code>[flake8]\nmax-line-length = 88\ndocstring-convention=google\n</code></pre>"},{"location":"contribute/documentation/","title":"Documentation","text":"<p>This documentation of OpenTrafficCam is generated by MkDocs using the awesome material theme. The documentation is fully written in Markdown.</p> <p>You can help improving and extending this documentation creating a fork and a pull request on GitHub. After review and discussion (if necessary) we will merge your pull request into the documentation repository and then it will be also visible on the site.</p> <p>Have a look at the README file of the documentation repository for more information. There you will also find a description of how this site is rendered and how inserting a new page into the navigation works.</p> <p>If you are not willing to contribute directly, you can also write us about your suggestions: team@opentrafficcam.org</p>"},{"location":"contribute/github/","title":"GitHub","text":"<p>All code of OpenTrafficCam is hosted on GitHub. For each core module a separate repository exists:</p> <ul> <li>OTCamera</li> <li>OTVision</li> <li>OTAnalytics</li> <li>OTLabels</li> </ul> <p>Following the Feature Branch Workflow each repository has a master branch, which at all times should contain a stable version of the software. All coding should be committed to separate feature branches. Once you completed coding on a feature, issue or bugfix, you create a pull request for the respective feature branch (preferably labeled using the GitHub labeling system). Your contributed code will then be reviewed by the main developers, discussed (if necessary) and merged into the master branch.</p> <p>Even without coding you can contribute to the project: If you encounter any bug or got any idea for enhancement, feel free to open an issue in the corresponding repository. Every issue should at least have two labels assigned: One label indicating the maturaty of the issue (question, bug, feature or idea) and one assigning the repository-specific subpackage or section (black colored labels).</p> <p>All pull request will be checked by GitHub's super-linter.</p>"},{"location":"contribute/gui/","title":"GUI","text":"<p>For now we decided on using PysimpleGUI to build the OpenTrafficCam user interface, as it is lightweight and easy to start with. Whenever possible, we try to separate the gui code from all other code. All the gui modules have a place in the \"gui\" subpackage of the respective OT repository:</p> <ul> <li>OTCamera/OTCamera/gui</li> <li>OTVision/OTVision/gui</li> <li>OTAnalytics/OTAnalytics/gui</li> </ul> <p>For the future we can also imagine using a web framework for user interaction. This would also help to enable remote access to local GPU machines or high performance computing centers.</p>"},{"location":"contribute/otcamera/","title":"OTCamera Dev Version","text":"<p>The Raspberry Pi Zero is wonderful to use outside (it consumes little power, is cheap, ...). But the development of OTCamera is more fun on a big Pi. Everything is much faster.</p> <p>You will need</p> <ul> <li>a prepared Raspberry Pi 2B/3B(+)/4 (the 2 GB works well)</li> <li>Camera module (no USB webcam)</li> </ul>"},{"location":"contribute/otcamera/#setup-vs-code-remote-development-extension","title":"Setup VS Code Remote Development Extension","text":"<p>Install the Remote-SSH extension on your desktop using the marketplace.</p> <p>Add the Pi as remote host. Connect to the Pi using the Remote-SSH extension (rightclick on the ne host - \"Connect to Host in New Window\"). When asked for the operating system of the host, choose \"Linux\". VS code will download and install the necessary dependencies on the Pi.</p> <p>Open the extension manager in the new windows an install all necessary extensions.</p> <p>Install a virtual environment and all requirements.</p> <pre><code>sudo apt install python3-venv -y\npython -m venv venv --upgrade-deps\npip install -r requirements.txt -r requirements-dev.txt -U\n</code></pre>"},{"location":"contribute/otcamera/#setup-git-and-github","title":"Setup Git and GitHub","text":"<p>If not already done install git using apt.</p> <pre><code>sudo apt install git -y\n</code></pre> <p>To setup your git commit name and email, login to your GitHub account and copy your private commit email.</p> <p>On the pi run</p> <pre><code>git config --global user.name \"Your Name\"\ngit config --global user.email \"123456+username@users.noreply.github.com\"\n</code></pre> <p>The easiest way to setup your GitHub credentials is to use vs code. In the file explorer panel click \"Clone Repository\" and choose \"Clone from GitHub\". Depending on your desktop computer settings, a browser tab will open to login into your GitHub account. Afterwards, you can search for \"OpenTrafficCam/OTCamera\" inside the vs code command prompt and select a folder to pull (choose /home/pi by default).</p>"},{"location":"contribute/otcamera/#install-screen-to-run-otcamera-in-background","title":"Install Screen to Run OTCamera in Background","text":"<p>Quote</p> <p>Screen is a full-screen window manager that multiplexes a physical terminal between several processes, typically interactive shells.</p> <p>To install screen on the developer pi just run</p> <pre><code>sudo apt install screen -y\n</code></pre> <p>You can now start <code>screen</code> and use it as an usual terminal. You can run the <code>python OTCamera</code> to start the camera in an active screen session. Hit Ctrl+A and then D to detach the current session. Screen will still run in background including the OTCamera. If you start screen including the <code>-r</code> option you can reconnect to this terminal session:</p> <pre><code>screen -r\n</code></pre> <p>You can safely disconnect you ssh connection to the pi, while the screen session is still running.</p>"},{"location":"contribute/otcamera/#ready-to-develop","title":"Ready to Develop","text":"<p>You should now be ready to pull the OTCamera repository and start developing.</p>"},{"location":"contribute/vscode/","title":"VS Code","text":"<p>We are developing OpenTrafficCam using Visual Studio Code using the following extensions:</p> <ul> <li>GitHub Pull Requests and Issues</li> <li>markdownlint</li> <li>Pylance</li> <li>Python</li> <li>Python Docstring Generator</li> <li>yaml Language Support</li> </ul>"},{"location":"contribute/vscode/#pylance-settings","title":"Pylance Settings","text":"<p>To solve import errors in the repository, we need to add an extra path to the pylance config.</p> <p>For example in the workspace settings of OTCamera add:</p> <pre><code>\"python.analysis.extraPaths\": [\n  \"./OTCamera\"\n]\n</code></pre> <p>Or set it using the Settings-UI (Workspace -&gt; Pylance -&gt; Python - Analysis: Extra Path -&gt; add './OTCamera')</p>"},{"location":"contribute/vscode/#snippets","title":"Snippets","text":"<p>To comfortably insert code use snippets. You can open the configuration file for a specific language to add or modify the user snippets: <code>Ctrl</code> + <code>Shift</code> + <code>p</code> and <code>Preferences: Configure User Snippets</code>.</p> <p>You can insert the user snippets by start typing the \"prefix\" value. For example start typing <code>gpl_</code> and autocompletection should recognize the user snippets stated below.</p>"},{"location":"contribute/vscode/#gpl-license-information","title":"GPL License Information","text":"<p>Add the following snippet to <code>vscodedata/user/snippets/python.json</code></p> <pre><code>\"gpl_license_header\": {\n        \"prefix\": \"gpl_add_head\",\n        \"body\":[\n            \"$LINE_COMMENT ${1:Program Name and Function}\",\n            \"$LINE_COMMENT Copyright (C) $CURRENT_YEAR OpenTrafficCam Contributors\",\n            \"$LINE_COMMENT &lt;https://github.com/OpenTrafficCam\",\n            \"$LINE_COMMENT &lt;team@opentrafficcam.org&gt;\",\n            \"$LINE_COMMENT\",\n            \"$LINE_COMMENT This program is free software: you can redistribute it and/or modify\",\n            \"$LINE_COMMENT it under the terms of the GNU General Public License as published by\",\n            \"$LINE_COMMENT the Free Software Foundation, either version 3 of the License, or\",\n            \"$LINE_COMMENT (at your option) any later version.\",\n            \"$LINE_COMMENT\",\n            \"$LINE_COMMENT This program is distributed in the hope that it will be useful,\",\n            \"$LINE_COMMENT but WITHOUT ANY WARRANTY; without even the implied warranty of\",\n            \"$LINE_COMMENT MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\",\n            \"$LINE_COMMENT GNU General Public License for more details.\",\n            \"$LINE_COMMENT\",\n            \"$LINE_COMMENT You should have received a copy of the GNU General Public License\",\n            \"$LINE_COMMENT along with this program.  If not, see &lt;https://www.gnu.org/licenses/&gt;.\",\n            \"\",\n        ],\n        \"description\": \"Add GPLv3 license information in source code.\"\n    }\n</code></pre> <p>Add a short description and use tab to jump to the end of the snippet.</p>"},{"location":"contribute/vscode/#mkdocs-configuration-support","title":"MKDocs Configuration Support","text":"<p>In order to minimize friction and maximize productivity, Material for MkDocs provides its own schema.json1 for mkdocs.yml. If your editor supports YAML schema validation, it's definitely recommended to set it up.</p> <p>Source</p>"},{"location":"overview/","title":"Welcome to OpenTrafficCam","text":"<p>You are looking for a tool that makes analyzing traffic easier, faster and more efficient?</p> <p>Welcome to the documentation of OpenTrafficCam - the only fully integrated open source workflow for video-based recording and automated analysis of road traffic.</p> <p>On this site you will find all the information to set up OpenTrafficCam.</p> <p>You want to know more? Check out our GitHub page for downloads and codes.</p>"},{"location":"overview/#motivation","title":"Motivation","text":"<p>Thanks to the rapid development of computer hardware and machine learning in the last decade, automatic object detection is state of the art in many business sectors. However, most of the time road traffic is still surveyed manually to a great expense. There are tools that optimize some of the processing steps, but they can hardly be used without programming skills, or they cause significant costs. This is why research and planning still have to be conducted with minimal or sometimes without any data at all. This lack of data primarily affects sustainable modes of transportation such as walking and cycling.</p> <p>OpenTrafficCam aims to address this gap for a stronger data-base for transportation planning, policy and research. In doing so, we ourselves benefit substantially from other open source projects. Consequently, we publish large parts of OpenTrafficCam as open source hardware and software under the GNU General Public License v3.0. This also helps in efficiently using limited public resources for road infrastructure planning and research and in avoiding substantial vendor locks. So all transport professionals and anyone else interested are very welcome to try OpenTrafficCam.</p> <p>You want to support?</p> <p>Developing, maintaining and organizing open source requires quite a lot of time and money. But fortunately, there are several ways to support us:</p> <ul> <li>Traffic engineers can help by spreading the word and submitting questions, bugs or feature ideas as issues     in the corresponding GitHub repositories of the modules     OTCamera,     OTVision,     OTAnalytics or     OTLabels.</li> <li>Researchers and developers can support by contributing code     or contacting us for scientific collaboration.</li> <li>Users, such as municipalities or engineering companies, can contact us to fund specific enhancements to     meet their use case and thus help all other users.</li> </ul>"},{"location":"overview/#how-it-works","title":"How it works","text":"<p>OpenTrafficCam consists of multiple modules. The core is composed of three:</p> <ol> <li>The OTCamera hardware to record videos,</li> <li>OTVision, a collection of algorithms to generate trajectories of objects (road users) based on the videos and</li> <li>OTAnalytics to gather traffic measures based on these trajectories.</li> </ol> <p>Each of the three modules comes with (easy) installation, a basic user interface and a documentation. The Open Source version covers the most common use cases of video-based traffic surveying, such as traffic counts.</p> <p>In addition to the three main modules, with OTLabels we will provide a set of labelled images of German road users and vehicles along with algorithms to train object detection models on custom data sets.</p> <p></p> <p>As described on the following pages of this documentation, some manual work, patience and basic skills are required to assemble OTCamera. And in order to regularly extract trajectories from videos using OTVision or train your own custom object detection models using OTLabels, you will need a powerful machine with a strong graphics card.</p> <p>Don\u00b4t want to deal with it? We got your back!</p> <p>We offer various services related to OpenTrafficCam:</p> <p> Selling and renting OTCameras</p> <p> Processing and analyzing your videos</p> <p> Setting up the OpenTrafficCam pipeline in your environment</p> <p> Onboarding and support to get the most out of OpenTrafficCam</p> <p> Conducting whole traffic surveys for you</p> <p> Developing features beyond the current open source version</p> <p> Consulting on traffic analysis, modeling and optimization</p> <p> Contact us for more information</p>"},{"location":"overview/dataprivacy/","title":"Data Privacy (Germany)","text":"<p>German law only</p> <p>The following FAQ on data protection are intentionally only available in German, as they refer exclusively to German law. They also do not apply to other countries with German as an official language (e.g. Austria, Switzerland).</p> <p>Die nachfolgenden FAQ zum Datenschutz sind mit Absicht nur in deutscher Sprache verf\u00fcgbar, da sie sich ausschlie\u00dflich auf deutsches Recht beziehen.  Sie gelten daher auch nicht f\u00fcr andere L\u00e4nder mit Deutsch als Amtssprache (z. B. \u00d6sterreich, Schweiz).</p>"},{"location":"overview/dataprivacy/#vorwort","title":"Vorwort","text":"<p>In einem Workshop im Rahmen der durch das BMVI gef\u00f6rderten mFund-Machbarkeitsstudie zu OpenTrafficCam haben wir gemeinsam mit anderen Verkehrsfachleuten h\u00e4ufige Fragen zum Datenschutz bei der Videoerhebung des Stra\u00dfenverkehrs gesammelt. Diese Fragen sind nachfolgend aufgelistet.</p> <p>Die Beantwortung der folgenden Fragen erfolgte in mehreren Interviews mit dem Datenschutzbeauftragten der TU Dresden. Sie dienen als initiale Informationsquelle zur Einhaltung des Datenschutzes bei der Videoaufzeichnung in Forschungsprojekten. Sie ersetzen keine Rechtsberatung und k\u00f6nnen verst\u00e4ndlicherweise nicht jedes einzelne Verfahren sowie die rechtlichen Spezifika der Bundesl\u00e4nder ber\u00fccksichtigen.</p> <p>Deshalb ist es auf jeden Fall erforderlich, die/den jeweils zust\u00e4ndigen Datenschutzbeauftragten fr\u00fchzeitig in das konkrete Projekt einzubeziehen. Als ein Ergebnis der Gespr\u00e4che hat sich auch gezeigt, dass zur Kl\u00e4rung detaillierter und \u00fcber den Forschungsbereich hinausgehender Fragen des Datenschutzes bei der automatisierten, videobasierten Verkehrserfassung eine dezidierte Besch\u00e4ftigung einer spezialisierten Anwaltskanzlei zielf\u00fchrend sein kann. Zuk\u00fcnftig ist eine Erweiterung der FAQ um Inhalte zum Datenschutz bei der Videoerhebung in nichtwissenschaftlichen Projekten geplant.</p>"},{"location":"overview/dataprivacy/#faq-zum-datenschutz-bei-videobasierter-verkehrserfassung-in-forschungsprojekten","title":"FAQ zum Datenschutz bei videobasierter Verkehrserfassung in Forschungsprojekten","text":""},{"location":"overview/dataprivacy/#grundsatzliches","title":"Grunds\u00e4tzliches","text":"Wer ist in einem Projekt zust\u00e4ndig f\u00fcr den Datenschutz und tr\u00e4gt die Verantwortung? Die Auftragnehmenden oder die Auftraggebenden? <p>Bei Forschungsf\u00f6rderung: Die erhebende Institution ist verantwortlich.  Hier ist der Forschungsparagraph des jeweiligen Landesdatenschutzgesetzes relevant, der relativ viele Freiheitsgrade beinhaltet. Dementsprechend ist die Freiheit von Forschung und Lehre besonders sch\u00fctzenswert und ist mit dem Recht auf Schutz personenbezogener Daten abzuw\u00e4gen. Grundsatz dabei ist immer die weitgehende Minimierung der Eingriffe in das Pers\u00f6nlichkeitsrecht. </p> <p>Bei Forschungsauftr\u00e4gen: Hier kommt es darauf an, in wessen Interesse die Datenerhebung liegt.</p> <p>Wenn die Forschungsmethodik durch die Auftraggebenden vorgegeben wird oder die Forschungsfrage nicht mit alternativen Erhebungsdesigns beantwortet werden kann (z.B. falls Videobeobachtungen in geringer Aufl\u00f6sung ohne die Erfassung personenbezogener Daten daf\u00fcr nicht ausreichen), liegt die Verantwortung aus rechtlicher Sicht allein beim Auftraggeber. Dies gilt auch, wenn die Auftragnehmenden die (zu diesem Zeitpunkt bereits anonymisierten) Daten \u00fcber den Projektgegenstand hinaus publizieren (es z\u00e4hlt der initiale Zweck der Datenerfassung). In diesem Fall sind die Auftraggebenden nicht nur verpflichtet, nach einem Datenschutzkonzept zu fragen. Sie sind auch dazu verpflichtet, Vorgaben f\u00fcr den Datenschutz in der Ausschreibung zu formulieren und Angebote darauf zu pr\u00fcfen, dass diese eigenen Vorgaben des Datenschutzes eingehalten werden.</p> <p>Wenn die Ausschreibung und die zu beantwortenden Forschungsfragen es zulassen, dass die Auftragnehmenden das Erhebungsdesign frei w\u00e4hlen, gilt die gemeinsame Verantwortlichkeit der Auftraggebenden und Auftragnehmenden gem\u00e4\u00df Art. 26 DSGVO (\u201cgemeinsames Interesse an der Datenauswertung\u201d / Gemeinsame Verantwortlichkeit).</p> Ist dar\u00fcber hinaus eine Instanz (z. B. Stra\u00dfenbaulasttr\u00e4ger) befugt, eine Messung aus Gr\u00fcnden des Datenschutzes zu verbieten? <p>Nein, weder der Stra\u00dfenbaulasttr\u00e4ger, noch eine Gebietsk\u00f6rperschaft oder der entsprechende Landesdatenschutzbeauftragte d\u00fcrfen die Datenerhebung auf \u00f6ffentlichen Fl\u00e4chen aus Gr\u00fcnden des Datenschutzes erlauben oder verbieten. Die Einhaltung des Datenschutzes liegt wie bereits beschrieben entweder in der Verantwortung der Forschungseinrichtung oder des Auftraggebers.</p> <p>Achtung!</p> <p>Neben dem Datenschutz k\u00f6nnten jedoch der Stra\u00dfenbaulasttr\u00e4ger bzw. die Gebietsk\u00f6rperschaft aufgrund ihrer Verkehrssicherungspflicht Einw\u00e4nde haben oder aufgrund der Montage von Kameras an Einbauten oder B\u00e4umen in deren Zust\u00e4ndigkeit.</p> Welche sind dabei die relevanten Rechtsvorschriften und wo/f\u00fcr wen gelten sie? Gibt es bereichsspezifische Regelungen? <p>Grunds\u00e4tzlich gilt zun\u00e4chst immer die europ\u00e4ische DSGVO, die jedoch gewisse Punkte offenl\u00e4sst, die wiederum von den Gesetzen nachgeordneter Gebietsk\u00f6rperschaften geregelt werden.</p> <p>F\u00fcr die Privatwirtschaft und Institutionen des Bundes gilt dabei spezialgesetzlich das BDSG.</p> <p>F\u00fcr \u00f6ffentliche Einrichtungen der Bundesl\u00e4nder (also die meisten Hochschulen), der Landkreise und kreisfreien St\u00e4dte gilt das jeweilige Landesdatenschutzgesetz, welches sich in der Regel bzgl. Forschung nicht stark vom BDSG unterscheidet.</p> <p>Der Geltungsbereich bereichsspezifischer Regelungen wird also durch die rechtliche Zuordnung der Institution zu einer Gebietsk\u00f6rperschaft bestimmt und nicht durch den Ort der Datenerhebung. Wenn mehrere Institutionen gemeinsam f\u00fcr den Datenschutz verantwortlich sind, gilt f\u00fcr jede das f\u00fcr sie relevante Gesetz bzw. die Gesetze.</p> Gibt es Unterschiede je nach Erhebungszweck bzw. Zweck des Projekts? <p>Es gibt Unterschiede zwischen Datenerhebungen zum Zweck der Forschung und anderen Zwecken wie Wirtschaft oder Verwaltung (z.B. Wahrung der \u00f6ffentlichen Sicherheit) sowie in der Verantwortung der Beteiligten. Ansonsten sind keine weiteren Unterschiede nach dem Erhebungszweck bekannt.</p> Wie \"dehnbar\" ist das Datenschutzrecht, wie unterschiedlich kann es ausgelegt werden? <p>Es existieren tats\u00e4chlich viele Grauzonen aufgrund der Nichtregulierung spezifischer Einzelf\u00e4lle (zu denen auch die videobasierte, automatisierte Verkehrserfassung f\u00fcr die Forschung und die Planung z\u00e4hlt), die im Falle rechtlicher Streitigkeiten separat bewertet werden m\u00fcssen. </p> <p>Wenn f\u00fcr rechtliche Fragestellungen keine dezidierten Rechtsnormen vorliegen, kann sich ein Gericht einerseits auf Pr\u00e4zedenzurteile berufen. Zur videobasierten Verkehrserfassung sind bisher jedoch nur wenige Urteile bekannt (siehe Frage \"Gibt es Pr\u00e4zedenz-Urteile zur videobasierten Verkehrserfassung?\").</p> <p>Andererseits ist eine rechtliche Anerkennung niedergeschriebener Branchenstandards m\u00f6glich, die in Zusammenarbeit zwischen den datenerhebenden Institutionen und den Aufsichtsbeh\u00f6rden erstellt wurden. Ein Beispiel daf\u00fcr ist der Arbeitskreis \"Datenschutz\" f\u00fcr Gesundheitsdaten, der in Abstimmung mit den Aufsichtsbeh\u00f6rden ein Rahmenkonzept entwickelt hat, auf das sich im Falle rechtlicher Streitigkeiten berufen werden kann. Zur Schaffung einer gewissen Rechtssicherheit bei den Details der videobasierten Verkehrserhebung f\u00fcr die Forschung und Planung kann ein \u00e4quivalentes Vorgehen sinnvoll sein.</p> Gibt es Pr\u00e4zedenz-Urteile zur videobasierten Verkehrserfassung? <p>Urteile zur Verkehrserfassung per Video zum Zweck der allgemeinen Verkehrsdatenerhebung f\u00fcr oder zur Unterst\u00fctzung von Forschung oder Verwaltungst\u00e4tigkeiten sind uns nicht bekannt. Folgende Urteile zum Datenschutz bei der Bilderfassung im Stra\u00dfenverkehr sind dar\u00fcber hinaus bekannt:</p> OLG Frankfurt, 06.11.2019 - 2 Ss - OWi 942/19 <p>Das OLG Frankfurt hat 2019 entschieden, dass private Dienstleister keine Verkehrs\u00fcberwachungen durf\u00fchren d\u00fcrfen und entsprechende Bu\u00dfgeldbescheide gesetzeswidrig sind.</p> BGH 15.05.2018 \u2013 VI ZR 233/17 <p>Der Bundesgerichtshof hat 2018 das sogenannte Dashcam-Urteil gef\u00e4llt, nachdem es f\u00fcr private Personen datenschutzrechtlich unzul\u00e4ssig ist, personenbezogene Merkmale mit einer Kamera aus einem fahrenden Auto \u201cpermanent und anlasslos\u201d aufzuzeichnen. Kameras mit Ringspeicher, die das Geschehen nur im Falle eines Unfalls permanent speichern, w\u00e4ren demnach zul\u00e4ssig. Im entsprechenden Fall hat das Gericht die permanente Videoaufzeichnung dennoch als Beweismittel zugelassen.</p> Muss eine Verkehrskamera als solche erkennbar und ausgewiesen sein oder darf man \"verdeckt\" filmen? <p>Falls keine personenbezogenen Daten erfasst werden, ist keine Information der Verkehrsbeteiligten notwendig.</p> <p>Falls personenbezogene Daten erfasst werden, ist grunds\u00e4tzlich eine Einverst\u00e4ndniserkl\u00e4rung der jeweiligen Personen einzuholen. Da dies bei Videobeobachtungen im Verkehrsraum meist nicht m\u00f6glich ist, soll stattdessen eine gute erkennbare Information der Verkehrsteilnehmenden erfolgen (z.B. \u00fcber ein auch f\u00fcr Kfz-F\u00fchrende gut lesbares Schild \u201cVerkehrserhebung\u201d) und Detailinformationen zur Erhebung an bzw. in der N\u00e4he der Kamera (mind. erhebende Institution und Kontaktinformationen, weitere Details z.B. auf einer Website, die per QR-Code erreichbar ist).</p> <p>Falls personenbezogene Daten erfasst werden und im Rahmen von Forschungsprojekten eine Information der Verkehrsteilnehmenden \u00fcber die Verkehrserhebung die Verl\u00e4sslichkeit der angestrebten Ergebnisse beeintr\u00e4chtigen w\u00fcrde (z.B. bei der Untersuchung regelkonformen oder sicherheitsrelevanten Verkehrsverhaltens), kann in Einzelf\u00e4llen darauf verzichtet werden. Grundlage hierf\u00fcr ist der Forschungsparagraph des jeweiligen Landesdatenschutzgesetzes, nach dem die Freiheit von Forschung und Lehre besonders sch\u00fctzenswert ist und das Pers\u00f6nlichkeitsrecht \u00fcberwiegen kann. Dieses Vorgehen sollte jedoch ausf\u00fchrlich begr\u00fcndet und von Beginn an dokumentiert werden, um etwaigen Klagen durch Verkehrsteilnehmende vor Gericht standzuhalten. Eine Abstimmung mit der zust\u00e4ndigen Stelle f\u00fcr Informationssicherheit wird dringend empfohlen.</p>"},{"location":"overview/dataprivacy/#vermeidung-der-erfassung-personenbezogener-daten","title":"Vermeidung der Erfassung personenbezogener Daten","text":"Wie sind personenbezogene Daten definiert? Z\u00e4hlen dazu neben Gesicht und Nummernschild auch die Gr\u00f6\u00dfe der Person, die Art ihres Gangs, die Art und Farbe ihrer Kleidung oder auff\u00e4llige Frisuren? <p>Laut Art. 4 Abs. 1 DSGVO sind alle Namen, Kennnummern, Standortdaten und Merkmale von nat\u00fcrlichen Personen als \u201cpersonenbezogene Daten\u201d definiert, die R\u00fcckschl\u00fcsse auf deren Identit\u00e4t erlauben. Es gilt als grunds\u00e4tzlich anerkannt, dass im Bereich des Stra\u00dfenverkehrs das Kfz-Kennzeichen und das Gesicht einer Person als personenbezogene Daten gelten. In Einzelf\u00e4llen k\u00f6nnte auch die Kombination anderer Merkmale wie Gr\u00f6\u00dfe, Kleidung und Frisur mit weiteren Informationen (z.B. dem Standort) die Identifikation einer Person erm\u00f6glichen. Hier liegt die Beweispflicht jedoch bei potentiellen Klagenden und die Identifikation der Person im Bild/Video muss beispielhaft durch unabh\u00e4ngige Personen nachgewiesen werden (d.h. es reicht nicht aus, wenn die Person selbst oder eine ihr bekannte Person sie im Bild/Video identifizieren kann). Es sind Klagen und Urteile zur Identifikation von Personen auf Fotographien des \u00f6ffentlichen Raums bekannt, in denen f\u00fcr die Kl\u00e4ger entschieden wurde \u2013 diese sind jedoch nicht einfach auf Videoerhebungen im Stra\u00dfenverkehr zum Zwecke der allgemeinen Verkehrsdatenerfassung \u00fcbertragbar.</p> Welche Entfernung zur Kamera z\u00e4hlt f\u00fcr die Datenschutzkonformit\u00e4t? <p>Die Kamera ist grunds\u00e4tzlich so zu konfigurieren, dass nur der relevante Untersuchungsbereich erfasst wird. Wenn dies technisch nicht m\u00f6glich ist, dann muss im Nachgang eine Sichtung und L\u00f6schung erfolgen. Dies gilt z. B. auch f\u00fcr F\u00e4lle, in denen eine Person sich aktiv auf H\u00f6he der Kamera begibt, um diese aus der N\u00e4he zu betrachten und dadurch selbst deutlich erkennbar wird. Sichtung und L\u00f6schung k\u00f6nnen auch automatisiert im Nachgang erfolgen (technische Ma\u00dfnahme, falls z.B. Objekte automatisiert detektiert werden und ein Grenzwert f\u00fcr die Gr\u00f6\u00dfe des erfassten Objekts \u00fcberschritten wird).</p> Reicht es aus, in den Videos sichtbare Gesichter und Kennzeichen zu verpixeln? Wann und wie muss das geschehen? <p>Grunds\u00e4tzlich sollte wenn m\u00f6glich bereits durch niedrige Aufl\u00f6sung und De-Fokussierung die Erfassung von Kennzeichen und Gesichtern vermieden werden.</p> <p>Wenn dies f\u00fcr bestimmte, nicht interessierende Bildbereiche nicht m\u00f6glich ist, kann als erste Alternative eine Nicht-Erfassung (z. B. Schw\u00e4rzung) dieser Bildbereiche bereits im Kamerasystem gepr\u00fcft und technisch umgesetzt werden.</p> <p>Wenn auch dies nicht m\u00f6glich ist, k\u00f6nnen die entsprechenden Bildbereiche direkt auf der Kamera live und permanent verpixelt und somit anonymisiert werden. Diese Methode setzt eine automatisierte Detektion des Kennzeichens bzw. Gesichts voraus. Wenn daraus keine weiteren Daten abgeleitet und gespeichert oder \u00fcbermittelt werden (z.B. das Kennzeichen als Klartext), gilt diese Methode als anerkannt, da sie dem Stand der Technik entspricht. Vor dem Einsatz dieser Methode wird jedoch dringend empfohlen, die Konformit\u00e4t mit der entsprechend geltenden Rechtsgrundlage zu pr\u00fcfen.</p> <p>Das zeitversetzte, nachtr\u00e4gliche (manuelle oder automatische) Verpixeln von Kennzeichen oder Gesichtern bei der Sichtung/Auswertung des Videomaterials sollte auf Einzelf\u00e4lle beschr\u00e4nkt bleiben (siehe Frage \"Welche Entfernung zur Kamera z\u00e4hlt f\u00fcr die Datenschutzkonformit\u00e4t?\"). Sollten die zuvor genannten Ma\u00dfnahmen nicht zielf\u00fchrend sein und sind dennoch geh\u00e4uft Kennzeichen oder Gesichter in den Videos zu vermuten, kann nicht mehr von einer anonymen Erhebung (Vermeidung der Erhebung personenbezogener Daten) gesprochen werden. In diesem Fall m\u00fcssen die nachfolgend beschriebenen Rahmenbedingungen f\u00fcr die Erhebung personenbezogener Daten eingehalten werden.</p>"},{"location":"overview/dataprivacy/#ablauf-in-fallen-in-denen-die-erfassung-personenbezogener-daten-nicht-vermieden-werden-kann","title":"Ablauf in F\u00e4llen, in denen die Erfassung personenbezogener Daten nicht vermieden werden kann","text":"Unter welchen Umst\u00e4nden d\u00fcrfen personenbezogene Daten erhoben werden? <p>Grunds\u00e4tzlich ist zu pr\u00fcfen, ob f\u00fcr den Erhebungszweck auch eine Videobeobachtung ohne die Erfassung personenbezogener Daten (bzw. eine Methode g\u00e4nzlich ohne Videobeobachtung) m\u00f6glich ist. Wenn dies nicht m\u00f6glich ist und das Interesse der Allgemeinheit an den Ergebnissen der Videobeobachtung die Interessen der einzelnen Personen (deren Datenschutz) \u00fcberwiegt, ist eine videobasierte Erfassung personenbezogener Daten grunds\u00e4tzlich denkbar.</p> <p>Dabei muss die Zustimmung der betroffenen Personen eingeholt werden oder zumindest eine Information der betroffenen Personen erfolgen (siehe Frage \u201cMuss eine Verkehrskamera als solche erkennbar und ausgewiesen sein oder darf man \"verdeckt\" filmen?\u201d).</p> Was beinhaltet ein Datenschutzkonzept und in welchen F\u00e4llen braucht man es? Wer muss dieses \"absegnen\"? <p>Die Erstellung eines Datenschutzkonzepts ist nicht in allen F\u00e4llen n\u00f6tig, in denen die Erhebung personenbezogener Daten geplant ist. Es ist jedoch sinnvoll, ein solches Konzept zu erstellen, da eine Rechenschaftspflicht besteht, d.h. eine Dokumentation zu f\u00fchren ist. In der DSGVO und in den Landesdatenschutzgesetzen ist f\u00fcr viele Aspekte auch grunds\u00e4tzlich geregelt, ob und wie personenbezogene Daten erhoben werden d\u00fcrfen. Wenn die Konformit\u00e4t mit diesen Rechtsgrundlagen eingehalten wird, reicht es aus, dies im Nachgang belegen zu k\u00f6nnen. Falls diesbez\u00fcglich Unklarheiten existieren, empfiehlt sich die R\u00fccksprache mit Datenschutzfachleuten (z.B. den zust\u00e4ndigen Beauftragten f\u00fcr Informationssicherheit und dem/der Datenschutzbeauftragten). In Zusammenarbeit mit diesen kann au\u00dferdem in F\u00e4llen, in denen vereinzelte Grunds\u00e4tze der Verarbeitung personenbezogener Daten nicht eingehalten werden k\u00f6nnen, eine sogenannte \u201cDatenschutzfolgenabsch\u00e4tzung\u201d erarbeitet werden. In dieser werden Erhebungszweck und -design, der Umgang mit den Daten sowie Gr\u00fcnde f\u00fcr die vereinzelten Abweichungen von den Rechtsgrundlagen detailliert erl\u00e4utert.</p> Sind die Daten bereits w\u00e4hrend der Messung vor Diebstahl zu sch\u00fctzen? Ist es datenschutzkonform m\u00f6glich, einen externen Zugang auf die Kamera w\u00e4hrend der Messung herzustellen? <p>Wenn personenbezogenen Daten erfasst werden, muss bereits w\u00e4hrend der Erfassung technisch sichergestellt werden, dass unbefugte Personen keinen Zugriff auf diese Daten haben. Daher sollten die Daten auf Kameras im Verkehrsraum nach dem aktuellen Stand der Technik gesch\u00fctzt werden. Momentan k\u00f6nnte dies durch ein sicheres Passwort erfolgen. Falls personenbezogene Daten \u00fcbermittelt werden, sind verschl\u00fcsselte Verbindungen zu empfehlen (z. B. per SSH, HTTPS, VPN).</p> Unter welchen Umst\u00e4nden darf man die Videos speichern und aufheben? D\u00fcrfen die Videos f\u00fcr einen anderen als f\u00fcr den urspr\u00fcnglichen Zweck verwendet werden? <p>Personenbezogene Daten m\u00fcssen so gespeichert werden, dass ausschlie\u00dflich befugte Personen Zugriff zu diesen haben. Au\u00dferdem d\u00fcrfen sie grunds\u00e4tzlich nur so lange gespeichert werden bis deren spezifischer Erhebungszweck erf\u00fcllt wurde (also z.B. die spezifische Forschungsfrage innerhalb eines Projekts beantwortet wurde). Anschlie\u00dfend m\u00fcssen die Daten gel\u00f6scht oder um die Eigenschaften reduziert werden, die einen Personenbezug erm\u00f6glichen (bei Videos z.B. Verringerung der Aufl\u00f6sung). Personenbezogene Daten d\u00fcrfen nur dann auch nach Erf\u00fcllung des spezifischen Erhebungszwecks vorgehalten werden, wenn die betroffenen Personen dem ausdr\u00fccklich zugestimmt haben. Vor der Verwendung gespeicherter personenbezogener Daten f\u00fcr weitere Zwecke ist jedoch erneut die Konformit\u00e4t des Zwecks sowohl mit den Rechtsgrundlagen als auch mit dem Wortlaut der Einwilligung der betroffenen Personen zu pr\u00fcfen.</p> Darf ich die Verkehrsvideos weitergeben? Was muss ich dabei beachten? (Stichworte \"Nachnutzbarkeit\" und \"Datensparsamkeit\") <p>Wenn es sich um personenbezogene Daten handelt und der \u00f6ffentliche F\u00f6rder- oder Auftraggeber f\u00fcr den Datenschutz verantwortlich ist, liegt diese Entscheidung bei ihm (siehe Frage \"Wer ist in einem Projekt zust\u00e4ndig f\u00fcr den Datenschutz und tr\u00e4gt die Verantwortung? Die Auftragnehmenden oder die Auftraggebenden?\").</p> <p>Wenn der F\u00f6rder- oder Auftragnehmer selbst f\u00fcr den Datenschutz verantwortlich ist und die Anfrage von \u00f6ffentlichen Stellen (z.B. Gebietsk\u00f6rperschaft, Beh\u00f6rde, Universit\u00e4t) kommt, ist die anfragende \u00f6ffentliche  Stelle auch f\u00fcr die \u00dcbermittlung verantwortlich. Das bedeutet, dass die anfragende Stelle sicherstellen muss, dass die \u00dcbermittlung der Daten gerechtfertigt ist.</p> <p>Einer \u00dcbermittlung an eine \u00f6ffentliche Stelle steht also nichts im Wege. Dies gilt auch, falls die \u00f6ffentliche Stelle die Daten f\u00fcr einen anderen als den urspr\u00fcnglichen Erhebungszweck verarbeiten m\u00f6chte (diese \u00f6ffentliche Stelle muss dann selbst pr\u00fcfen, ob dies rechtskonform ist). Vor einer \u00dcbermittlung an eine nicht\u00f6ffentliche Stelle (z.B. an ein Ingenieurb\u00fcro) empfehlen wir dringend die Konsultation von Fachleuten f\u00fcr Datenschutz. Die \u00dcbermittlung anonymisierter Verkehrsvideos stellt aus datenschutzrechtlicher Sicht nat\u00fcrlich kein Problem dar und kann aus dieser Hinsicht an alle Stellen erfolgen.</p>"},{"location":"overview/usecases/trafficcounts/","title":"Traffic Counts","text":"<p>Coming soon</p> <p>Unfortunately, there is no content here yet. But we are currently working on completing this site.</p>"},{"location":"overview/usecases/trafficsafety/","title":"Traffic Safety","text":""},{"location":"overview/usecases/trafficsafety/#the-rest-of-the-iceberg","title":"The rest of the iceberg","text":"<p>Traditionally, accident data has primarily been used to analyse the road safety of a traffic facility. However, this is a reactive approach, which means that accident occurrences must first be observed over many years before measures can be taken to improve road safety. In addition, the accident statistics show a certain number of unreported cases and there is only limited information on the sequence of events that led to the accident.</p> <p>Assuming that accidents are only the tip of the iceberg, traffic flow can also be observed and analysed in terms of conflicts or near misses in a complementary or supplementary manner. Near misses or traffic conflicts are defined by Amundsen and Hyd\u00e9n (1977) as \"observable situations in which two or more road users approach each other in space and time to such an extent that there is a risk of collision if their movements remain unchanged\". In a simplified way, the traffic flow can be divided into the following hierarchy of severity: Undisturbed passages - Interactions - Near misses/Conflicts - Accidents. The near misses (similar to accidents) can be divided into further severity levels with regard to the proximity to a collision, but also with regard to the severity of the resulting collision (similar to accident severity).</p> <p></p>"},{"location":"overview/usecases/trafficsafety/#pet-easy-peasy","title":"PET - easy peasy","text":"<p>A number of Surrogate Measures of Safety have been developed as objective indicators to classify an interaction as a near miss and to assess its severity (primarily in terms of proximity to a collision). One commonly used indicator is PET (Post Encroachment Time), which is a rather simple safety indicator. It describes the time period between the first road user leaving the common conflict area and the second one entering it.</p> <p></p>"},{"location":"overview/usecases/trafficsafety/#ttc-complex-but-meaningful","title":"TTC - complex, but meaningful","text":"<p>Another widely applied safety indicator in surrogate analysis is Time To Collision (TTC). It describes the remaining time to a collision between two road users at a certain point in time of their interaction and under the assumption that neither of the parties involved will take an evasive action. The lower the TTC, the more severe the conflict between road users. In case of a collision the TTC equals zero.</p> <p>A TTC can only be calculated if the two interacting road users are on a collision course. If the collision course is left due to an evasive action, the TTC for the time from the start of the evasive action is calculated on the basis of the hypotethic trajectories without the evasive action. Such hypotethic trajectories can be approximated by various methods of trajectory prediction. The simplest (but in some cases also the most unrealistic) method for trajectory prediction is to assume continued motion with constant velocity vectors.</p> <p>The TTC is calculated for each moment of interaction. This results in a TTC curve. Interactions and also periods of interactions are often categorised according to whether the TTC is below a threshold value (e.g. 1.5 sec). The Minimum TTC, the Time Exposed TTC and the Time Integrated TTC are used as aggregated indicators.</p> <p>In addition to road safety analysis, TTC is also used for active accident avoidance, especially for decision-making in algorithms of autonomous vehicles.</p> <p></p>"},{"location":"overview/usecases/trafficsafety/#best-practices","title":"Best practices","text":"<p>In order to enable comparisons, e.g. with other traffic facilities, safety indicators like PET and TTC are related to suitable exposure units, such as traffic volume or the total number of interactions in the observation period. For comparisons one also needs statistcal significance of the frequency of conflicts at every individual location. To achieve this one must capture trajectories over a sufficient period of time. In most constellations, conflicts do occur more frequently than accidents that require observation periods of several years. However, recent studis show, that capturing videos over weeks or at least multiple days is the minimum required duration for conflict analyses. One mistake that also should be avoided is to raise the threshold for categorising encounters as conflicts, as this means measuring exposure, but not road safety.</p>"},{"location":"overview/usecases/trafficsafety/#it-s-all-about-validity","title":"It\u00b4s all about validity","text":"<p>Exposure based conflict frequencies can then be compared over different road sections or intersections or to known mean values. In addition, in the past, fixed factors for estimating the number of accidents were derived from the number of conflicts, which, however, could often not be confirmed by other researchers. To overcome fixed coefficients, a new approach taking into account variability of conflict severity based on Extreme Value Theory (EVT) is now state of the art, that has also been applied in stock market and flood protection earlier.</p> <p>Conflicts are even more valid with crash data when it comes to relative safety asssessment, e.g. ranking a number of road sites by traffic safety, which should deliver nearly the same results as using crash data from several years. This relative validity is sufficient for many applications in road safety work.</p> <p>Another advantage of the safety analysis of trajectories is that the entire movements of the road users leading to the conflict are known. For example, differences in movement patterns between conflicts and other encounters can be analyzed and suitable measures for increasing road safety can be derived from them. In order to avoid accidents, however, a certain process validity between conflicts and accidents is also required here.</p>"},{"location":"overview/usecases/trafficsafety/#how-widespread-is-the-method","title":"How widespread is the method?","text":"<p>Several other European countries developed their own dedicated traffic conflict techniques many years ago and conflict analysis already plays an important role in their road safety assessment tools. In some other countries there appear to be concerns about the conflict technique from earlier days, as the severity of the conflicts was assessed by local human observers and was therefore largely subjective. Against the background of Vision Zero (zero traffic fatalities) and the developments in computer technology and AI, we advocate the use of proactive safety assessments based on the trajectories of road users. This can save lives and prevent many accidents. Nonetheless, there is still a need for further research into data collection, indicators, accident prediction and validation. In addition, clarification is required about the strengths and weaknesses of the conflict technology compared to the accident analysis and the need for further proactive safety tools.</p>"},{"location":"overview/usecases/trafficsafety/#what-s-next-for-us","title":"What\u00b4s next for us?","text":"<p>Forthe next years it is planned to integrate the conflict analysis functionalities in OpenTrafficCam, to automate the processes as best as possible and still make them available to all non-programmers in order to enable both basic research and application studies for traffic experts.</p>"},{"location":"overview/usecases/vehiclespeeds/","title":"Vehicle Speeds","text":"<p>Coming soon</p> <p>Unfortunately, there is no content here yet. But we are currently working on completing this site.</p>"}]}